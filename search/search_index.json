{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Tools of Big Data \ud83d\udd17 The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data. The objective of this class is that students gain: First hand experience with and detailed knowledge of computing models, notably cloud computing An understanding of distributed programming models and data distribution Broad knowledge of many databases and their respective strengths As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes. Class structure \ud83d\udd17 The class is structured in three parts: Data computation \ud83d\udd17 20 hours on the computing platforms used in the data ecosystem. We will briefly cover cluster computing and then go in depth on cloud computing, using Google Cloud Platform as an example. Finally, a class on GPU computing will be given in coordination with the deep learning section of the AML class. Data distribution \ud83d\udd17 20 hours on the distribution of data, with a focus on distributed programming models. We will introduce functional programming and MapReduce, then use these concepts in a practical session on Spark. Finally, students will do a graded exercise with Dask. Databases \ud83d\udd17 In the final 10 hours of the course, state-of-the-art databases will be presented. Students will install and demonstrate the advantages of different databases to their peers as a graded project. Class schedule \ud83d\udd17 Introduction Introduction to tools of Big Data 2h 29/09/2020 Global Datasphere Data Computation Cloud Computing & Google Cloud Platform 2h 2h 07/10/2020 14/10/2020 Readings Containers 2h 14/10/2020 Readings Orchestration 1h 20/10/2020 Readings Cloud Compute BE 3h 20/10/2020 GPU computing, part 1 3h 01/12/2020 GPGPU TP GPU computing, part 2 3h 02/12/2020 GPGPU TP Data Distribution Readings Data distribution 1h 06/01/2021 Spanner Functional programming 4h 06/01/2021 Julia Hadoop and MapReduce 3h 19/01/2021 MapReduce Spark 3h 19/01/2021 Spark , PySpark Dask on Kubernetes 3h 20/01/2021 Dask documentation Dask project 6h 27/01/2021 Dask Databases Databases overview 2h 03/02/2021 Databases and SQL PostgeSQL TP 3h 08/02/2021 PostgeSQL Project work day 2h 10/02/2021 Project presentations 2h 08/03/2021","title":"Overview"},{"location":"index.html#tools-of-big-data","text":"The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data. The objective of this class is that students gain: First hand experience with and detailed knowledge of computing models, notably cloud computing An understanding of distributed programming models and data distribution Broad knowledge of many databases and their respective strengths As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes.","title":"Tools of Big Data"},{"location":"index.html#class-structure","text":"The class is structured in three parts:","title":"Class structure"},{"location":"index.html#data-computation","text":"20 hours on the computing platforms used in the data ecosystem. We will briefly cover cluster computing and then go in depth on cloud computing, using Google Cloud Platform as an example. Finally, a class on GPU computing will be given in coordination with the deep learning section of the AML class.","title":"Data computation"},{"location":"index.html#data-distribution","text":"20 hours on the distribution of data, with a focus on distributed programming models. We will introduce functional programming and MapReduce, then use these concepts in a practical session on Spark. Finally, students will do a graded exercise with Dask.","title":"Data distribution"},{"location":"index.html#databases","text":"In the final 10 hours of the course, state-of-the-art databases will be presented. Students will install and demonstrate the advantages of different databases to their peers as a graded project.","title":"Databases"},{"location":"index.html#class-schedule","text":"Introduction Introduction to tools of Big Data 2h 29/09/2020 Global Datasphere Data Computation Cloud Computing & Google Cloud Platform 2h 2h 07/10/2020 14/10/2020 Readings Containers 2h 14/10/2020 Readings Orchestration 1h 20/10/2020 Readings Cloud Compute BE 3h 20/10/2020 GPU computing, part 1 3h 01/12/2020 GPGPU TP GPU computing, part 2 3h 02/12/2020 GPGPU TP Data Distribution Readings Data distribution 1h 06/01/2021 Spanner Functional programming 4h 06/01/2021 Julia Hadoop and MapReduce 3h 19/01/2021 MapReduce Spark 3h 19/01/2021 Spark , PySpark Dask on Kubernetes 3h 20/01/2021 Dask documentation Dask project 6h 27/01/2021 Dask Databases Databases overview 2h 03/02/2021 Databases and SQL PostgeSQL TP 3h 08/02/2021 PostgeSQL Project work day 2h 10/02/2021 Project presentations 2h 08/03/2021","title":"Class schedule"},{"location":"1_1_overview.html","text":"Data Computation Part 1: Cloud Computing, Containers & Orchestration \ud83d\udd17 Back to home All slides Syllabus \ud83d\udd17 Introduction \ud83d\udd17 Introduction to data computation module Cloud Computing (4h) \ud83d\udd17 Intro to cloud computing & google cloud platform Date Type Link Description 07/10 Lecture Intro to Cloud Computing A lecture about an introduction to \"what is the cloud\" 07/10 Lecture Using Cloud Computing in your daily job What does it mean to \"use the cloud\" ? 07/10 Lecture Intro to Google Cloud Platform A quick intro to GCP 07/10 14/10 Hands-on GCP Part 1: The Basics My first steps with GCP, Google Cloud Shell 14/10 Hands-on GCP Part 2: My first VMs, Storage, IAAS & PAAS Here I will create a GCE instance, interact with GCS, discover managed products 20/10 Recap Important notions about Cloud Computing A recap of important notions Containers (2h) \ud83d\udd17 Intro to containers & docker Date Type Link Description 14/10 Lecture From virtualisation to Containerisation, Docker What are containers and why do we need them ? What is Docker ? 14/10 Hands-on Docker Discover Docker 20/10 Recap Important notions about Docker A recap of important notions Orchestration & Deployment (1h) \ud83d\udd17 Intro to container orchestra features Date Type Link Description 20/10 Lecture Microservices, Orchestration, Kubernetes Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes 20/10 Interactive A development env on Kubernetes and Deploying ML models into production Interactive demos of deployment & scalability with Docker and Kubernetes At Home Bonus Hands-on My first kubernetes cluster for a development environment Where we deploy a development environment on Kubernetes 20/10 Recap Important notions about Orchestration A recap of important notions Final BE (3h) \ud83d\udd17 A small workshop that puts everything together: Docker, Kubernetes, to deploy an image classifier in a scalable fashion Date Type Link Description 20/10 BE Explanations Walkthrough A final Bureau d'\u00e9tudes to wrap everything together Concluding Slides \ud83d\udd17 Finally, it's over ! What's next \ud83d\udd17 Date Duration Title 18/11 2h Cluster Compute 01/12 3h GPU 02/12 3h GPU","title":"Introduction"},{"location":"1_1_overview.html#data-computation-part-1-cloud-computing-containers-orchestration","text":"Back to home All slides","title":"Data Computation Part 1: Cloud Computing, Containers &amp; Orchestration"},{"location":"1_1_overview.html#syllabus","text":"","title":"Syllabus"},{"location":"1_1_overview.html#introduction","text":"Introduction to data computation module","title":"Introduction"},{"location":"1_1_overview.html#cloud-computing-4h","text":"Intro to cloud computing & google cloud platform Date Type Link Description 07/10 Lecture Intro to Cloud Computing A lecture about an introduction to \"what is the cloud\" 07/10 Lecture Using Cloud Computing in your daily job What does it mean to \"use the cloud\" ? 07/10 Lecture Intro to Google Cloud Platform A quick intro to GCP 07/10 14/10 Hands-on GCP Part 1: The Basics My first steps with GCP, Google Cloud Shell 14/10 Hands-on GCP Part 2: My first VMs, Storage, IAAS & PAAS Here I will create a GCE instance, interact with GCS, discover managed products 20/10 Recap Important notions about Cloud Computing A recap of important notions","title":"Cloud Computing (4h)"},{"location":"1_1_overview.html#containers-2h","text":"Intro to containers & docker Date Type Link Description 14/10 Lecture From virtualisation to Containerisation, Docker What are containers and why do we need them ? What is Docker ? 14/10 Hands-on Docker Discover Docker 20/10 Recap Important notions about Docker A recap of important notions","title":"Containers (2h)"},{"location":"1_1_overview.html#orchestration-deployment-1h","text":"Intro to container orchestra features Date Type Link Description 20/10 Lecture Microservices, Orchestration, Kubernetes Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes 20/10 Interactive A development env on Kubernetes and Deploying ML models into production Interactive demos of deployment & scalability with Docker and Kubernetes At Home Bonus Hands-on My first kubernetes cluster for a development environment Where we deploy a development environment on Kubernetes 20/10 Recap Important notions about Orchestration A recap of important notions","title":"Orchestration &amp; Deployment (1h)"},{"location":"1_1_overview.html#final-be-3h","text":"A small workshop that puts everything together: Docker, Kubernetes, to deploy an image classifier in a scalable fashion Date Type Link Description 20/10 BE Explanations Walkthrough A final Bureau d'\u00e9tudes to wrap everything together","title":"Final BE (3h)"},{"location":"1_1_overview.html#concluding-slides","text":"Finally, it's over !","title":"Concluding Slides"},{"location":"1_1_overview.html#whats-next","text":"Date Duration Title 18/11 2h Cluster Compute 01/12 3h GPU 02/12 3h GPU","title":"What's next"},{"location":"1_1_presentation.html","text":"Introduction \ud83d\udd17 Link to slides","title":"Presentation"},{"location":"1_1_presentation.html#introduction","text":"Link to slides","title":"Introduction"},{"location":"1_2_cloud.html","text":"Cloud Computing \ud83d\udd17 Intro to cloud computing \ud83d\udd17 Link to slides Using Cloud Computing \ud83d\udd17 Link to slides Google Cloud Platform \ud83d\udd17 Link to slides","title":"Lectures"},{"location":"1_2_cloud.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"1_2_cloud.html#intro-to-cloud-computing","text":"Link to slides","title":"Intro to cloud computing"},{"location":"1_2_cloud.html#using-cloud-computing","text":"Link to slides","title":"Using Cloud Computing"},{"location":"1_2_cloud.html#google-cloud-platform","text":"Link to slides","title":"Google Cloud Platform"},{"location":"1_2_gcp_handson.html","text":"GCP Hands On, first VMs, Google Cloud Storage \ud83d\udd17 How to run it ? \ud83d\udd17 The easiest way to run this workshop is to have google cloud sdk on your machine, However, like demonstrated earlier, you can use google cloud shell as the \"front end\" for GCP to run all gcloud commands from inside this VM However you have to have google chrome without too much privacy tools and a good wifi without firewall for it to work perfectly :) SSH from the cloud shell should also be possible from google cloud shell, and you will be able to use the web preview from your cloud shell to display a port on another machine (double ssh tunnel !) Otherwise for SSHing you will be able to use the web based ssh tools and port forwarding tools of google cloud platform: https://cloud.google.com/compute/docs/ssh-in-browser 1. My first Google Compute Engine Instance \ud83d\udd17 First, we will make our first steps by creating a compute engine instance (a vm) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it What is google cloud compute engine ? try to describe it with your own words Creating my VM using the console \ud83d\udd17 Follow this codelab to create your VM Create the following instance type: n1-standard-2 zone: europe-west4-a (the netherlands) os: ubuntu 20.04 boot disk size: 10 Gb Name it however you see fit DO NOT SHUT IT DOWN for now Connecting to SSH \ud83d\udd17 Connect to ssh from google cloud shell or your terminal to the machine Solution `gcloud compute ssh {USER}@{MACHINE NAME}` Check available disk space Bash command to run `df -h` Check the OS name Bash command to run `cat /etc/os-release` Check the CPU model Solution `cat /proc/cpuinfo` Check instance google cloud properties Solution `cat /proc/cpuinfo` The magic of redimensioning VMs \ud83d\udd17 Shutdown the VM (from the web browser), check the previous codelab to see how to do it Select it and click on EDIT Change the machine type to n1-standard-4 ( link to documentation ) Relaunch it, reconnect to it and try to list the CPUs & RAM amount again Magic isn't it ? Note: If you had any files and specific configuration, they would still be here ! Transfering files from the computer to this machine \ud83d\udd17 We will use the terminal to transfer some files from * your computer to** this machine, Follow this link to learn how to use the gcloud cli tool to transfer files to your instance TOC For experts, it's possible to do it manually using rsync from ssh or scp Transfer some files to your /home/${USER} directory List them from your instance ( ls ) How do we do the opposite ? See below, 2. Interacting with Google Cloud Storage \ud83d\udd17 Here we will discover google cloud storage, upload some files from your computer and download them from your instance in the cloud What is Google Cloud Storage ? Try to describe it with your own words This codelab will guide you through the basics of google cloud storage Use this codelab something from your computer to google cloud storage from the web browser. DO NOT DELETE THE FILES YET Now we will download it using the google cloud CLI tool, Here's the documentation List the content of the bucket you just created (if you deleted it previously, create a new one) Upload a file to a bucket Download a file from a bucket What if we want to do the same from the VM ? Now go back to your machine Try to list bucket, download and upload files Is it possible ? If not, it's because you have to allow the instance to access google cloud storage Shutdown the VM and edit it (like we did when we resized the instance) Check \"access scopes\", select \"set access for each api\", and select \"storage / admin\" Now restart you machine, connect back to it. You should be able to upload to google cloud storage now files now Now you can delete the bucket you just created You can delete the VM as well, we will not use it 3. Google Compute Engine from the CLI and \"deep learning VMs\" \ud83d\udd17 Here we will use the google cloud sdk to create a more complex VM with a pre-installed image and connect to its jupyter server This will be useful for the next part of our workshop because both git and docker are already installed ! Google Cloud Platform comes with a set of services targeted at data scientists called AI Platform , among them are Deep Learning VMs which are essentially preinstalled VMs (more or less the same configuration as google colab) with some bonuses. What are \"Deep Learning VMs\" ? Try to use your own words What would be the alternative if you wanted to get a machine with the same installation ? Create a google compute engine instance with a GPU using the command line \ud83d\udd17 This is very important for the next workshop Instead of using the browser to create this machine, we will be using the CLI to create instances export INSTANCE_NAME = \"fch-dlvm-1\" # RENAME THIS !!!!!!!!!! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --image-family = \"common-cu100\" \\ --image-project = deeplearning-platform-release \\ --maintenance-policy = TERMINATE \\ --accelerator = \"type=nvidia-tesla-p100,count=1\" \\ --scopes = \"storage-rw\" \\ --machine-type = \"n1-standard-8\" \\ --boot-disk-size = 120GB \\ --metadata = \"install-nvidia-driver=True\" If you can't access GPU, remove the line regarding gpu for now, and go and request a quota increase for your project so that you will be able to access gpus later (request the corresponding GPU) In that case, change common-cu-100 for common-cpu and remove the metadata line Notice the similarities between the first VM you created and this one, What changed ? If you want to learn more about compute images, image families etc... go here Metadata command is a bit of magic provided by google to automatically install GPU drivers (very useful !) Connect to ssh to this machine and query the gpu \ud83d\udd17 Connect to your instance using the gcloud cli & ssh To query if there is a GPU recognized, you have to run nvidia-smi Shut it down, remove the gpu and restart it \ud83d\udd17 This time you have to EDIT the VM and remove the GPU instead of resizing the number of CPUs Connect back to the VM \ud83d\udd17 Relaunch the previous VM and connect to it using ssh This time, you will forward some ports as well Solution TOC {:toc} `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 Go to your local browser and type http://localhost:8080 , you should be in a jupyter notebook under the user jupyter You can try to play with the jupyter lab (that has a code editor and terminal capabilities) to get a feel of manipulating a remote instance Try to pip3 list to check all dependencies installed ! DO NOT DELETE THE INSTANCE FOR NOW YOU WILL NEED IT LATER 4. Introduction to infrastructure as code \ud83d\udd17 This tutorial will guide you through google cloud deployment manager, which is a way to deploy google compute engine instances using configuration files Don't forget to adapt machine configurations and zone to your use case (see above) 5. Persistent sessions with TMUX \ud83d\udd17 https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ Connect to your instance using SSH Question: What happens if you start a long computation and disconnect ? Check that tmux is installed on the remote instance (run tmux ). if not install it Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ To check you have understood you should be able to: Connect to your remote instance with ssh Start a tmux session Launch a process (for example top ) inside it Detach from the session ( CTRL+B :detach ) Kill the ssh connection Connect again tmux attach to your session Your process should still be here ! Congratulations :)","title":"TP 2 GCP Hands On"},{"location":"1_2_gcp_handson.html#gcp-hands-on-first-vms-google-cloud-storage","text":"","title":"GCP Hands On, first VMs, Google Cloud Storage"},{"location":"1_2_gcp_handson.html#how-to-run-it","text":"The easiest way to run this workshop is to have google cloud sdk on your machine, However, like demonstrated earlier, you can use google cloud shell as the \"front end\" for GCP to run all gcloud commands from inside this VM However you have to have google chrome without too much privacy tools and a good wifi without firewall for it to work perfectly :) SSH from the cloud shell should also be possible from google cloud shell, and you will be able to use the web preview from your cloud shell to display a port on another machine (double ssh tunnel !) Otherwise for SSHing you will be able to use the web based ssh tools and port forwarding tools of google cloud platform: https://cloud.google.com/compute/docs/ssh-in-browser","title":"How to run it ?"},{"location":"1_2_gcp_handson.html#1-my-first-google-compute-engine-instance","text":"First, we will make our first steps by creating a compute engine instance (a vm) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it What is google cloud compute engine ? try to describe it with your own words","title":"1. My first Google Compute Engine Instance"},{"location":"1_2_gcp_handson.html#creating-my-vm-using-the-console","text":"Follow this codelab to create your VM Create the following instance type: n1-standard-2 zone: europe-west4-a (the netherlands) os: ubuntu 20.04 boot disk size: 10 Gb Name it however you see fit DO NOT SHUT IT DOWN for now","title":"Creating my VM using the console"},{"location":"1_2_gcp_handson.html#connecting-to-ssh","text":"Connect to ssh from google cloud shell or your terminal to the machine Solution `gcloud compute ssh {USER}@{MACHINE NAME}` Check available disk space Bash command to run `df -h` Check the OS name Bash command to run `cat /etc/os-release` Check the CPU model Solution `cat /proc/cpuinfo` Check instance google cloud properties Solution `cat /proc/cpuinfo`","title":"Connecting to SSH"},{"location":"1_2_gcp_handson.html#the-magic-of-redimensioning-vms","text":"Shutdown the VM (from the web browser), check the previous codelab to see how to do it Select it and click on EDIT Change the machine type to n1-standard-4 ( link to documentation ) Relaunch it, reconnect to it and try to list the CPUs & RAM amount again Magic isn't it ? Note: If you had any files and specific configuration, they would still be here !","title":"The magic of redimensioning VMs"},{"location":"1_2_gcp_handson.html#transfering-files-from-the-computer-to-this-machine","text":"We will use the terminal to transfer some files from * your computer to** this machine, Follow this link to learn how to use the gcloud cli tool to transfer files to your instance TOC For experts, it's possible to do it manually using rsync from ssh or scp Transfer some files to your /home/${USER} directory List them from your instance ( ls ) How do we do the opposite ? See below,","title":"Transfering files from the computer to this machine"},{"location":"1_2_gcp_handson.html#2-interacting-with-google-cloud-storage","text":"Here we will discover google cloud storage, upload some files from your computer and download them from your instance in the cloud What is Google Cloud Storage ? Try to describe it with your own words This codelab will guide you through the basics of google cloud storage Use this codelab something from your computer to google cloud storage from the web browser. DO NOT DELETE THE FILES YET Now we will download it using the google cloud CLI tool, Here's the documentation List the content of the bucket you just created (if you deleted it previously, create a new one) Upload a file to a bucket Download a file from a bucket What if we want to do the same from the VM ? Now go back to your machine Try to list bucket, download and upload files Is it possible ? If not, it's because you have to allow the instance to access google cloud storage Shutdown the VM and edit it (like we did when we resized the instance) Check \"access scopes\", select \"set access for each api\", and select \"storage / admin\" Now restart you machine, connect back to it. You should be able to upload to google cloud storage now files now Now you can delete the bucket you just created You can delete the VM as well, we will not use it","title":"2. Interacting with Google Cloud Storage"},{"location":"1_2_gcp_handson.html#3-google-compute-engine-from-the-cli-and-deep-learning-vms","text":"Here we will use the google cloud sdk to create a more complex VM with a pre-installed image and connect to its jupyter server This will be useful for the next part of our workshop because both git and docker are already installed ! Google Cloud Platform comes with a set of services targeted at data scientists called AI Platform , among them are Deep Learning VMs which are essentially preinstalled VMs (more or less the same configuration as google colab) with some bonuses. What are \"Deep Learning VMs\" ? Try to use your own words What would be the alternative if you wanted to get a machine with the same installation ?","title":"3. Google Compute Engine from the CLI and \"deep learning VMs\""},{"location":"1_2_gcp_handson.html#create-a-google-compute-engine-instance-with-a-gpu-using-the-command-line","text":"This is very important for the next workshop Instead of using the browser to create this machine, we will be using the CLI to create instances export INSTANCE_NAME = \"fch-dlvm-1\" # RENAME THIS !!!!!!!!!! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --image-family = \"common-cu100\" \\ --image-project = deeplearning-platform-release \\ --maintenance-policy = TERMINATE \\ --accelerator = \"type=nvidia-tesla-p100,count=1\" \\ --scopes = \"storage-rw\" \\ --machine-type = \"n1-standard-8\" \\ --boot-disk-size = 120GB \\ --metadata = \"install-nvidia-driver=True\" If you can't access GPU, remove the line regarding gpu for now, and go and request a quota increase for your project so that you will be able to access gpus later (request the corresponding GPU) In that case, change common-cu-100 for common-cpu and remove the metadata line Notice the similarities between the first VM you created and this one, What changed ? If you want to learn more about compute images, image families etc... go here Metadata command is a bit of magic provided by google to automatically install GPU drivers (very useful !)","title":"Create a google compute engine instance with a GPU using the command line"},{"location":"1_2_gcp_handson.html#connect-to-ssh-to-this-machine-and-query-the-gpu","text":"Connect to your instance using the gcloud cli & ssh To query if there is a GPU recognized, you have to run nvidia-smi","title":"Connect to ssh to this machine and query the gpu"},{"location":"1_2_gcp_handson.html#shut-it-down-remove-the-gpu-and-restart-it","text":"This time you have to EDIT the VM and remove the GPU instead of resizing the number of CPUs","title":"Shut it down, remove the gpu and restart it"},{"location":"1_2_gcp_handson.html#connect-back-to-the-vm","text":"Relaunch the previous VM and connect to it using ssh This time, you will forward some ports as well Solution TOC {:toc} `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 Go to your local browser and type http://localhost:8080 , you should be in a jupyter notebook under the user jupyter You can try to play with the jupyter lab (that has a code editor and terminal capabilities) to get a feel of manipulating a remote instance Try to pip3 list to check all dependencies installed ! DO NOT DELETE THE INSTANCE FOR NOW YOU WILL NEED IT LATER","title":"Connect back to the VM"},{"location":"1_2_gcp_handson.html#4-introduction-to-infrastructure-as-code","text":"This tutorial will guide you through google cloud deployment manager, which is a way to deploy google compute engine instances using configuration files Don't forget to adapt machine configurations and zone to your use case (see above)","title":"4. Introduction to infrastructure as code"},{"location":"1_2_gcp_handson.html#5-persistent-sessions-with-tmux","text":"https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ Connect to your instance using SSH Question: What happens if you start a long computation and disconnect ? Check that tmux is installed on the remote instance (run tmux ). if not install it Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ To check you have understood you should be able to: Connect to your remote instance with ssh Start a tmux session Launch a process (for example top ) inside it Detach from the session ( CTRL+B :detach ) Kill the ssh connection Connect again tmux attach to your session Your process should still be here ! Congratulations :)","title":"5. Persistent sessions with TMUX"},{"location":"1_2_gcp_setup.html","text":"GCP Initial Setup & First Steps \ud83d\udd17 Abstract In this hands on you will configure your GCP account, the google cloud SDK and access the cloud console using Google Cloud Shell, You will also discover a very useful tool, a managed jupyter notebook service from google named Google Colab which may be very important for your future developments this year 1. Create your GCP Account \ud83d\udd17 Overview link Create an account within Google cloud Platform using your ISAE e-mail Use the code given by Dennis to get your free credits You should have 300$ + a \"free tier\" available to you From the interface you should create a project with a name of your choice 2. Install Google Cloud SDK & Configure the shell \ud83d\udd17 If you want to interact with GCP from your computer, you will need to install the Google Cloud SDK , which will also install a shell if you are on windows If you don't, you will have to do everything from google cloud shell (it's not as easy), so I recommend installing the SDK. The best ways to interact with google cloud SDK is with a terminal so in that order: Linux (either VM or native): https://cloud.google.com/sdk/docs/install#linux MacOS: https://cloud.google.com/sdk/docs/install#mac Windows Subsystem for Linux: see Linux Windows: https://cloud.google.com/sdk/docs/install#windows If you are on windows, you should launch the google cloud sdk shell now, Then you can configure the google cloud sdk with your account 3. My first \"VM\", Google Cloud Shell \ud83d\udd17 Intro to Google Cloud Shell \ud83d\udd17 Google Cloud Shell is a \"managed VM\" made available to interact with the GCP platform without needing to configure locally the google cloud sdk. It is useful if you only have a web browser, but it may not work and it's not as easy as using a local terminal Compared to configured a VM by yourself, this one comes loaded with developer tools and gcp authentification correctly set up, and thus is faster to use, However the main drawback to using it as a development machine is the available disk space limited to 5 Gb (not enough to build docker images for example) Here is the description of Google Cloud Shell Look at the documentation Question Can you describe it with your own words ? What would be the closest service that you can find on GCP that is similar to cloud shell ? Connect to google cloud shell \ud83d\udd17 Follow this guide for connecting to google cloud shell using the browser If this doesn't work on your machine for whichever reason, there is a workaround which requires having installed the google-cloud-sdOther references:k Explore google cloud shell \ud83d\udd17 Check available disk space Bash command to run df -h Check the OS name Bash command to run cat /etc/os-release Check the CPU model Bash command to run cat /proc/cpuinfo This is the hardware model... how many cores do you have available ? Which amount of RAM ? Help htop will give you your current usage and available cores, or you can do nproc A demo of cloud shell web preview \ud83d\udd17 We will install Visual Studio Code Server , which is a cloud-based text editor, on Cloud Shell and preview it from your browser. There is already a code editor in Google Cloud Shell (based on Theia) but we want to showcase the web preview as well, so we will do it manually, You may enable boost mode Run curl -fsSL https://code-server.dev/install.sh | sh in your terminal to download & install code server Run code-server --port=8080 to start code server Shut it down ( CTRL+C ) then Fetch your password using cat ~/.config/code-server/config.yaml Re-run it Open web preview on port 8080 and log in You should be able to open files, get a terminal from inside a vscode inside your browser inside a VM ... Magic isn't it ? Warning It is possible that the browser-based method does not work on your machine, there is a troubleshooting guide on this (mainly it doesn't like too much privacy on your browser) The alternative solution would be to connect to it from your terminal / local shell using the google cloud sdk, Here is the documentation for this Command to run in this case: gcloud alpha cloud-shell ssh -- -L 8080:localhost:8080 4. Google Colaboratory \ud83d\udd17 Here, you will look at Google Colaboratory, which is a very handy tool for doing data science work (based on jupyter notebooks) on the cloud, using a preconfigured instance (which can access a GPU). You will be able to store data on Google Drive and to share I highly recommend using this for Jupyter based AML BE , but I invite you to discover google colab at home, or during AML BE because it's a useful tool but mastering it is not relevant for our cloud class Intro & Description of Google Colaboratory \ud83d\udd17 Open Google Colab Some intro , another one Question Can you describe what it is ? Is it IaaS ? PaaS ? SaaS ? why exactly ? Info Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with Zero configuration required Free access to GPUs Easy sharing It offers a \"jupyter notebook - like\" interface, and allows to install your own dependencies by running bash commands inside the VM, with connection to google drive, google sheets You can manipulate the notebooks from your Google Drive and share it like it was a GDoc document It's essentially between SaaS and PaaS, it offers you a development platform without you having to manage anything except your code and your data (which are both data from the cloud provider point of view) Loading jupyter notebooks, interacting with google drive \ud83d\udd17 Open a notebook you previously ran on your computer (from AML class), you can run a notebook on github directly in google colab Try to run it inside google colab Link google colab and google drive and upload something on google drive (like an image) and display in on google colab Other nice usages of Google Colab \ud83d\udd17 Writing markdown to generate reports Installing custom dependencies","title":"TP 1 GCP Initial Setup & First Steps"},{"location":"1_2_gcp_setup.html#gcp-initial-setup-first-steps","text":"Abstract In this hands on you will configure your GCP account, the google cloud SDK and access the cloud console using Google Cloud Shell, You will also discover a very useful tool, a managed jupyter notebook service from google named Google Colab which may be very important for your future developments this year","title":"GCP Initial Setup &amp; First Steps"},{"location":"1_2_gcp_setup.html#1-create-your-gcp-account","text":"Overview link Create an account within Google cloud Platform using your ISAE e-mail Use the code given by Dennis to get your free credits You should have 300$ + a \"free tier\" available to you From the interface you should create a project with a name of your choice","title":"1. Create your GCP Account"},{"location":"1_2_gcp_setup.html#2-install-google-cloud-sdk-configure-the-shell","text":"If you want to interact with GCP from your computer, you will need to install the Google Cloud SDK , which will also install a shell if you are on windows If you don't, you will have to do everything from google cloud shell (it's not as easy), so I recommend installing the SDK. The best ways to interact with google cloud SDK is with a terminal so in that order: Linux (either VM or native): https://cloud.google.com/sdk/docs/install#linux MacOS: https://cloud.google.com/sdk/docs/install#mac Windows Subsystem for Linux: see Linux Windows: https://cloud.google.com/sdk/docs/install#windows If you are on windows, you should launch the google cloud sdk shell now, Then you can configure the google cloud sdk with your account","title":"2. Install Google Cloud SDK &amp; Configure the shell"},{"location":"1_2_gcp_setup.html#3-my-first-vm-google-cloud-shell","text":"","title":"3. My first \"VM\", Google Cloud Shell"},{"location":"1_2_gcp_setup.html#intro-to-google-cloud-shell","text":"Google Cloud Shell is a \"managed VM\" made available to interact with the GCP platform without needing to configure locally the google cloud sdk. It is useful if you only have a web browser, but it may not work and it's not as easy as using a local terminal Compared to configured a VM by yourself, this one comes loaded with developer tools and gcp authentification correctly set up, and thus is faster to use, However the main drawback to using it as a development machine is the available disk space limited to 5 Gb (not enough to build docker images for example) Here is the description of Google Cloud Shell Look at the documentation Question Can you describe it with your own words ? What would be the closest service that you can find on GCP that is similar to cloud shell ?","title":"Intro to Google Cloud Shell"},{"location":"1_2_gcp_setup.html#connect-to-google-cloud-shell","text":"Follow this guide for connecting to google cloud shell using the browser If this doesn't work on your machine for whichever reason, there is a workaround which requires having installed the google-cloud-sdOther references:k","title":"Connect to google cloud shell"},{"location":"1_2_gcp_setup.html#explore-google-cloud-shell","text":"Check available disk space Bash command to run df -h Check the OS name Bash command to run cat /etc/os-release Check the CPU model Bash command to run cat /proc/cpuinfo This is the hardware model... how many cores do you have available ? Which amount of RAM ? Help htop will give you your current usage and available cores, or you can do nproc","title":"Explore google cloud shell"},{"location":"1_2_gcp_setup.html#a-demo-of-cloud-shell-web-preview","text":"We will install Visual Studio Code Server , which is a cloud-based text editor, on Cloud Shell and preview it from your browser. There is already a code editor in Google Cloud Shell (based on Theia) but we want to showcase the web preview as well, so we will do it manually, You may enable boost mode Run curl -fsSL https://code-server.dev/install.sh | sh in your terminal to download & install code server Run code-server --port=8080 to start code server Shut it down ( CTRL+C ) then Fetch your password using cat ~/.config/code-server/config.yaml Re-run it Open web preview on port 8080 and log in You should be able to open files, get a terminal from inside a vscode inside your browser inside a VM ... Magic isn't it ? Warning It is possible that the browser-based method does not work on your machine, there is a troubleshooting guide on this (mainly it doesn't like too much privacy on your browser) The alternative solution would be to connect to it from your terminal / local shell using the google cloud sdk, Here is the documentation for this Command to run in this case: gcloud alpha cloud-shell ssh -- -L 8080:localhost:8080","title":"A demo of cloud shell web preview"},{"location":"1_2_gcp_setup.html#4-google-colaboratory","text":"Here, you will look at Google Colaboratory, which is a very handy tool for doing data science work (based on jupyter notebooks) on the cloud, using a preconfigured instance (which can access a GPU). You will be able to store data on Google Drive and to share I highly recommend using this for Jupyter based AML BE , but I invite you to discover google colab at home, or during AML BE because it's a useful tool but mastering it is not relevant for our cloud class","title":"4. Google Colaboratory"},{"location":"1_2_gcp_setup.html#intro-description-of-google-colaboratory","text":"Open Google Colab Some intro , another one Question Can you describe what it is ? Is it IaaS ? PaaS ? SaaS ? why exactly ? Info Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with Zero configuration required Free access to GPUs Easy sharing It offers a \"jupyter notebook - like\" interface, and allows to install your own dependencies by running bash commands inside the VM, with connection to google drive, google sheets You can manipulate the notebooks from your Google Drive and share it like it was a GDoc document It's essentially between SaaS and PaaS, it offers you a development platform without you having to manage anything except your code and your data (which are both data from the cloud provider point of view)","title":"Intro &amp; Description of Google Colaboratory"},{"location":"1_2_gcp_setup.html#loading-jupyter-notebooks-interacting-with-google-drive","text":"Open a notebook you previously ran on your computer (from AML class), you can run a notebook on github directly in google colab Try to run it inside google colab Link google colab and google drive and upload something on google drive (like an image) and display in on google colab","title":"Loading jupyter notebooks, interacting with google drive"},{"location":"1_2_gcp_setup.html#other-nice-usages-of-google-colab","text":"Writing markdown to generate reports Installing custom dependencies","title":"Other nice usages of Google Colab"},{"location":"1_3_containers.html","text":"From Virtualisation to Containerisation \ud83d\udd17 Link to slides","title":"Lecture"},{"location":"1_3_containers.html#from-virtualisation-to-containerisation","text":"Link to slides","title":"From Virtualisation to Containerisation"},{"location":"1_3_docker.html","text":"Docker: Hands on \ud83d\udd17 How to run this ? \ud83d\udd17 We will discover the basics of docker and you will be able to manipulate your first images and containers ! First, you should be inside jupyter lab using the google cloud deep learning VM created previously It's possible to do this from google cloud shell using code-server as well but you will be limited in disk space later on Why only two of these ? That's because both have git and docker installed and installing docker is a pain. If you are running linux on your laptop you may install docker and do everything from your computer Otherwise... don't ;) FIRST if you are using the deep learning vm Disconnect from your instance and relaunch it while mapping 8888 as well. You should have the vm jupyter's lab on 8080, and 8888 free Solution `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 -L 8081:localhost:8081 -L 8888:localhost:8888` 1. Manipulating docker for the 1st time \ud83d\udd17 Source: https://github.com/docker/labs To get started, let's run the following in our terminal: $ docker pull alpine The pull command fetches the alpine image from the Docker registry and saves it in our system. You can use the docker images command to see a list of all images on your system. $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B 1.1 Docker Run \ud83d\udd17 Great! Let's now run a Docker container based on this image. To do that you are going to use the docker run command. $ docker run alpine ls -l total 48 drwxr-xr-x 2 root root 4096 Mar 2 16:20 bin drwxr-xr-x 5 root root 360 Mar 18 09:47 dev drwxr-xr-x 13 root root 4096 Mar 18 09:47 etc drwxr-xr-x 2 root root 4096 Mar 2 16:20 home drwxr-xr-x 5 root root 4096 Mar 2 16:20 lib ...... ...... What happened? Behind the scenes, a lot of stuff happened. When you call run , The Docker client contacts the Docker daemon The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Store. (Since we have issued docker pull alpine before, the download step is not necessary) The Docker daemon creates the container and then runs a command in that container. The Docker daemon streams the output of the command to the Docker client When you run docker run alpine , you provided a command ( ls -l ), so Docker started the command specified and you saw the listing. Let's try something more exciting. $ docker run alpine echo \"hello from alpine\" hello from alpine OK, that's some actual output. In this case, the Docker client dutifully ran the echo command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Try another command. docker run alpine /bin/sh Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to docker run -it alpine /bin/sh . You are now inside the container shell and you can try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Since no containers are running, you see a blank line. Let's try a more useful variant: docker ps -a $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36171a5da744 alpine \"/bin/sh\" 5 minutes ago Exited (0) 2 minutes ago fervent_newton a6a9d46d0b2f alpine \"echo 'hello from alp\" 6 minutes ago Exited (0) 6 minutes ago lonely_kilby ff0a5c3750b9 alpine \"ls -l\" 8 minutes ago Exited (0) 8 minutes ago elated_ramanujan c317d0a9e3d2 hello-world \"/hello\" 34 seconds ago Exited (0) 12 minutes ago stupefied_mcclintock What you see above is a list of all containers that you ran. Notice that the STATUS column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now: $ docker run -it alpine /bin/sh / # ls bin dev etc home lib linuxrc media mnt proc root run sbin sys tmp usr var / # uname -a Linux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux Running the run command with the -it flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands. That concludes a whirlwind tour of the docker run command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run , use docker run --help to see a list of all flags it supports. As you proceed further, we'll see a few more variants of docker run . 1.2 Terminology \ud83d\udd17 In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem. Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run docker inspect alpine . In the demo above, you used the docker pull command to download the alpine image. When you executed the command docker run hello-world , it also did a docker pull behind the scenes to download the hello-world image. Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using docker run which you did using the alpine image that you downloaded. A list of running containers can be seen using the docker ps command. Docker daemon - The background service running on the host that manages building, running and distributing Docker containers. Docker client - The command line tool that allows the user to interact with the Docker daemon. Docker Store - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial. 2.0 Webapps with Docker \ud83d\udd17 Source: https://github.com/docker/labs Great! So you have now looked at docker run , played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker. 2.1 Run a static website in a container \ud83d\udd17 Note: Code for this section is in this repo in the website directory Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Store, run the container, and see how easy it is to set up a web server. The image that you are going to use is a single-page website that was already created for this demo and is available on the Docker Store as dockersamples/static-site . You can download and run the image directly in one go using docker run as follows. docker run -d dockersamples/static-site Files: Dockerfile hello_docker.html Note: The current version of this image doesn't run without the -d flag. The -d flag enables detached mode , which detaches the running container from the terminal/shell and returns your prompt after the container starts. We are debugging the problem with this image but for now, use -d even for this first example. So, what happens when you run this command? Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container. Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine? Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the docker run command to add this instruction. Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode. First, stop the container that you have just launched. In order to do this, we need the container ID. Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run docker ps to view the running containers. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7a0e504ca3e dockersamples/static-site \"/bin/sh -c 'cd /usr/\" 28 seconds ago Up 26 seconds 80 /tcp, 443 /tcp stupefied_mahavira Check out the CONTAINER ID column. You will need to use this CONTAINER ID value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the CONTAINER ID on our system; you should use the value that you see in your terminal. $ docker stop a7a0e504ca3e $ docker rm a7a0e504ca3e Note: A cool feature is that you do not need to specify the entire CONTAINER ID . You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up. Now, let's launch a container in detached mode as shown below: $ docker run --name static-site -e AUTHOR = \"Your Name\" -d -P dockersamples/static-site e61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810 In the above command: -d will create a container with the process detached from our terminal -P will publish all the exposed container ports to random ports on the Docker host -e is how you pass environment variables to the container --name allows you to specify a container name AUTHOR is the environment variable name and Your Name is the value that you can pass Now you can see the ports by running the docker port command. $ docker port static-site 443 /tcp -> 0 .0.0.0:32772 80 /tcp -> 0 .0.0.0:32773 If you are running Docker for Mac , Docker for Windows , or Docker on Linux, you can open http://localhost:[YOUR_PORT_FOR 80/tcp] . For our example this is http://localhost:32773 . If you are using Docker Machine on Mac or Windows, you can find the hostname on the command line using docker-machine as follows (assuming you are using the default machine). $ docker-machine ip default 192 .168.99.100 You can now open http://<YOUR_IPADDRESS>:[YOUR_PORT_FOR 80/tcp] to see your site live! For our example, this is: http://192.168.99.100:32773 . You can also run a second webserver at the same time, specifying a custom host port mapping to the container's webserver. $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site To deploy this on a real server you would just need to install Docker, and run the above docker command(as in this case you can see the AUTHOR is Docker which we passed as an environment variable). Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section. But first, let's stop and remove the containers since you won't be using them anymore. $ docker stop static-site $ docker rm static-site Let's use a shortcut to remove the second site: $ docker rm -f static-site-2 Run docker ps to make sure the containers are gone. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2.2 Docker Images \ud83d\udd17 In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally. Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE dockersamples/static-site latest 92a386b6e686 2 hours ago 190 .5 MB nginx latest af4b3d7d5401 3 hours ago 190 .5 MB python 2 .7 1c32174fd534 14 hours ago 676 .8 MB postgres 9 .4 88d845ac7a88 14 hours ago 263 .6 MB containous/traefik latest 27b4e0c6b2fd 4 days ago 20 .75 MB node 0 .10 42426a5cba5f 6 days ago 633 .7 MB redis latest 4f5f397d4b7c 7 days ago 177 .5 MB mongo latest 467eb21035a8 7 days ago 309 .7 MB alpine 3 .3 70c557e50ed6 8 days ago 4 .794 MB java 7 21f6ce84e43c 8 days ago 587 .7 MB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest . For example you could pull a specific version of ubuntu image as follows: $ docker pull ubuntu:12.04 If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named latest . So for example, the docker pull command given below will pull an image named ubuntu:latest : $ docker pull ubuntu To get a new Docker image you can either get it from a registry (such as the Docker Store) or create your own. There are hundreds of thousands of images available on Docker Store . You can also search for images directly from the command line using docker search . An important distinction with regard to images is between base images and child images . Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian. Child images are images that build on base images and add additional functionality. Another key concept is the idea of official images and user images . (Both of which can be base images or child images.) Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the python , node , alpine and nginx images are official (base) images. To find out more about them, check out the Official Images Documentation . User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as user/image-name . The user value in the image name is your Docker Store user or organization name. 2.3 Create your first image \ud83d\udd17 Note: The code for this section is in this repository in the flask-app directory. Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application. The goal of this exercise is to create a Docker image which will run a Flask app. We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile . Finally, we'll build the image, and then run it. Create a Python Flask app that displays random cat pix Write a Dockerfile Build the image Run your image Dockerfile commands summary 2.3.1 Create a Python Flask app that displays random cat pix \ud83d\udd17 For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat .gif every time it is loaded - because, you know, who doesn't like cats? Start by creating a directory called flask-app where we'll create the following files: app.py requirements.txt templates/index.html Dockerfile Make sure to cd flask-app before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image. app.py \ud83d\udd17 Create the app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) requirements.txt \ud83d\udd17 In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file: Flask==0.10.1 templates/index.html \ud83d\udd17 Create a directory called templates and create an index.html file in that directory with the following content in it: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> < p >< small > Courtesy: < a href = \"http://www.buzzfeed.com/copyranter/the-best-cat-gif-post-in-the-history-of-cat-gifs\" > Buzzfeed </ a ></ small ></ p > </ div > </ body > </ html > 2.3.2 Write a Dockerfile \ud83d\udd17 We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image . Since our application is written in Python, we will build our own Python image based on Alpine . We'll do that using a Dockerfile . A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles. Create a file called Dockerfile , and add content to it as described below. We'll start by specifying our base image, using the FROM keyword: FROM alpine:3.5 The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next: RUN apk add --update py2-pip Let's add the files that make up the Flask Application. Install all Python requirements for our app to run. This will be accomplished by adding the lines: COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt Copy the files you have created earlier into our image by using COPY command. COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ Specify the port number which needs to be exposed. Since our flask app is running on 5000 that's what we'll expose. EXPOSE 5000 The last step is the command for running the application which is simply - python ./app.py . Use the CMD command to do that: CMD [\"python\", \"/usr/src/app/app.py\"] The primary purpose of CMD is to tell the container which command it should run by default when it is started. Verify your Dockerfile. Our Dockerfile is now ready. This is how it looks: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [\"python\", \"/usr/src/app/app.py\"] 2.3.3 Build the image \ud83d\udd17 Now that you have your Dockerfile , you can build your image. The docker build command does the heavy-lifting of creating a docker image from a Dockerfile . The docker build command is quite simple - it takes an optional tag name with the -t flag, and the location of the directory containing the Dockerfile - the . indicates the current directory: docker build -t myfirstapp:1.0 $ docker build -t myfirstapp:1.0 . Sending build context to Docker daemon 9.728 kB Step 1 : FROM alpine:latest ---> 0d81fc72e790 Step 2 : RUN apk add --update py-pip ---> Running in 8abd4091b5f5 fetch http://dl-4.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz fetch http://dl-4.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz (1/12) Installing libbz2 (1.0.6-r4) (2/12) Installing expat (2.1.0-r2) (3/12) Installing libffi (3.2.1-r2) (4/12) Installing gdbm (1.11-r1) (5/12) Installing ncurses-terminfo-base (6.0-r6) (6/12) Installing ncurses-terminfo (6.0-r6) (7/12) Installing ncurses-libs (6.0-r6) (8/12) Installing readline (6.3.008-r4) (9/12) Installing sqlite-libs (3.9.2-r0) (10/12) Installing python (2.7.11-r3) (11/12) Installing py-setuptools (18.8-r0) (12/12) Installing py-pip (7.1.2-r0) Executing busybox-1.24.1-r7.trigger OK: 59 MiB in 23 packages ---> 976a232ac4ad Removing intermediate container 8abd4091b5f5 Step 3 : COPY requirements.txt /usr/src/app/ ---> 65b4be05340c Removing intermediate container 29ef53b58e0f Step 4 : RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt ---> Running in a1f26ded28e7 Collecting Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1)) Downloading Flask-0.10.1.tar.gz (544kB) Collecting Werkzeug>=0.7 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Werkzeug-0.11.4-py2.py3-none-any.whl (305kB) Collecting Jinja2>=2.4 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB) Collecting itsdangerous>=0.21 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading itsdangerous-0.24.tar.gz (46kB) Collecting MarkupSafe (from Jinja2>=2.4->Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading MarkupSafe-0.23.tar.gz Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask Running setup.py install for MarkupSafe Running setup.py install for itsdangerous Running setup.py install for Flask Successfully installed Flask-0.10.1 Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.4 itsdangerous-0.24 You are using pip version 7.1.2, however version 8.1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. ---> 8de73b0730c2 Removing intermediate container a1f26ded28e7 Step 5 : COPY app.py /usr/src/app/ ---> 6a3436fca83e Removing intermediate container d51b81a8b698 Step 6 : COPY templates/index.html /usr/src/app/templates/ ---> 8098386bee99 Removing intermediate container b783d7646f83 Step 7 : EXPOSE 5000 ---> Running in 31401b7dea40 ---> 5e9988d87da7 Removing intermediate container 31401b7dea40 Step 8 : CMD python /usr/src/app/app.py ---> Running in 78e324d26576 ---> 2f7357a0805d Removing intermediate container 78e324d26576 Successfully built 2f7357a0805d If you don't have the alpine:3.5 image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image ( <YOUR_USERNAME>/myfirstapp ) shows. 2.3.4 Run your image \ud83d\udd17 The next step in this section is to run the image and see if it actually works. $ docker run -p 8888 :5000 --name myfirstapp myfirstapp:1.0 * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) Head over to http://localhost:8888 and your app should be live. Note If you are using Docker Machine, you may need to open up another terminal and determine the container ip address using docker-machine ip default . Hit the Refresh button in the web browser to see a few more cat images. 2.3.5 Dockerfile commands summary \ud83d\udd17 Here's a quick summary of the few basic commands we used in our Dockerfile. FROM starts the Dockerfile. It is a requirement that the Dockerfile must start with the FROM command. Images are created in layers, which means you can use another image as the base image for your own. The FROM command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format username/imagename:version . RUN is used to build up the Image you're creating. For each RUN command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a RUN instruction is to place the full text of the shell command after the RUN (e.g., RUN mkdir /user/local/foo ). This will automatically run in a /bin/sh shell. You can define a different shell like this: RUN /bin/bash -c 'mkdir /user/local/foo' COPY copies local files into the container. CMD defines the commands that will run on the Image at start-up. Unlike a RUN , this does not create a new layer for the Image, but simply runs the command. There can only be one CMD per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the CMD run a script. CMD requires that you tell it where to run the command, unlike RUN . So example CMD commands would be: CMD [\"python\", \"./app.py\"] CMD [\"/bin/bash\", \"echo\", \"Hello World\"] EXPOSE creates a hint for users of an image which ports provide services. It is included in the information which can be retrieved via $ docker inspect <container-id> . Note: The EXPOSE command does not actually make any ports accessible to the host! Instead, this requires publishing ports by means of the -p flag when using $ docker run . PUSH pushes your image to Docker Cloud, or alternately to a private registry Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles . 3. Containers Registry \ud83d\udd17 Remember Container Registries ? Here as some explainers The main container registry is dockerhub, https://hub.docker.com/ All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before Example, the Python Image Google Cloud has a Container Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only. However, it requires naming the image in a specific fashion: eu.gcr.io/${PROJECT_ID}/name:tag Use the docker cli to tag your previous myfirstapp image to the right namespace docker tag myfirstapp eu.gcr.io/{PROJECT_ID}/myfirstapp:1.0 Upload it on container registry docker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG] If you have a problem of authentification, gcloud auth configure-docker Hint to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Go to container registry https://console.cloud.google.com/gcr, you should see your docker image :) 4. Data Science Standardized Environment \ud83d\udd17 4.1 Intro \ud83d\udd17 Those of us who work on a team know how hard it is to create a standardize development environment. Or if you have ever updated a dependency and had everything break, you understand the importance of keeping development environments isolated. Using Docker, we can create a project / team image with our development environment and mount a volume with our notebooks and data. The benefits of this workflow are that we can: Separate out projects Spin up a container to onboard new employees Build an automated testing pipeline to confirm upgrade dependencies do not break code 4.2 Kaggle Docker Image \ud83d\udd17 For this exercise we will use Kaggle Docker Image which is a fully configured docker image that can be used as a data science container Take a look at the documentation and the repository 4.3 Get the algorithm in ML git in your Virtual Machine \ud83d\udd17 From your vm, run git clone https://github.com/erachelson/MLclass.git , this should setup your AML class inside your VM Using code-server is not mandatory now 4.4 Mounting volumes and ports \ud83d\udd17 Now let's run the image. This container has a jupyter notebook accessible from port 8080 so we will need to map the host port 8888 (the one accessible from the ssh tunnel) to the docker port 8080, we will use port forwarding We will also need to make available the notebooks on the VM to the container... we will mount volumes . Your data is located in /home/${USER}/MLClass and we want to miunt it in /tmp/workdir docker run --rm -it \\ -p 8888 :8080 \\ -v /home/ ${ USER } /MLclass:/home/Mlclass \\ --workdir /home/ \\ gcr.io/kaggle-images/python \\ /bin/bash /run_jupyter.sh Note: this image is very large ! Options breakdown: * --rm remove the container when we stop it * -it run the container in interactive mode * -p forward port from host:container * other: options from the kaggle container You should now see a jupyter lab with mlclass accessible if you connect your browser (in your laptop) to port 8888 (localhost:8888) So basically, we mapped the ports local 8888 to vm 8888 and vm 8888 to docker 8080 5. Bonus - Using Google Cloud Tools for Docker \ud83d\udd17 Using cloud shell you should be able to do the Hello World Dockerfile exercise except that instead of using docker build you use Google Cloud Build Tutorial: https://cloud.google.com/cloud-build/docs/quickstart-docker Example command : gcloud builds submit --tag eu.gcr.io/$PROJECT_ID/{image}:{tag} . Help to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Example Try to build the hello world app 6. Bonus - Docker Compose \ud83d\udd17 https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6 https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md 7. Bonus - Going further \ud83d\udd17 https://container.training/","title":"TP"},{"location":"1_3_docker.html#docker-hands-on","text":"","title":"Docker: Hands on"},{"location":"1_3_docker.html#how-to-run-this","text":"We will discover the basics of docker and you will be able to manipulate your first images and containers ! First, you should be inside jupyter lab using the google cloud deep learning VM created previously It's possible to do this from google cloud shell using code-server as well but you will be limited in disk space later on Why only two of these ? That's because both have git and docker installed and installing docker is a pain. If you are running linux on your laptop you may install docker and do everything from your computer Otherwise... don't ;) FIRST if you are using the deep learning vm Disconnect from your instance and relaunch it while mapping 8888 as well. You should have the vm jupyter's lab on 8080, and 8888 free Solution `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 -L 8081:localhost:8081 -L 8888:localhost:8888`","title":"How to run this ?"},{"location":"1_3_docker.html#1-manipulating-docker-for-the-1st-time","text":"Source: https://github.com/docker/labs To get started, let's run the following in our terminal: $ docker pull alpine The pull command fetches the alpine image from the Docker registry and saves it in our system. You can use the docker images command to see a list of all images on your system. $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B","title":"1. Manipulating docker for the 1st time"},{"location":"1_3_docker.html#11-docker-run","text":"Great! Let's now run a Docker container based on this image. To do that you are going to use the docker run command. $ docker run alpine ls -l total 48 drwxr-xr-x 2 root root 4096 Mar 2 16:20 bin drwxr-xr-x 5 root root 360 Mar 18 09:47 dev drwxr-xr-x 13 root root 4096 Mar 18 09:47 etc drwxr-xr-x 2 root root 4096 Mar 2 16:20 home drwxr-xr-x 5 root root 4096 Mar 2 16:20 lib ...... ...... What happened? Behind the scenes, a lot of stuff happened. When you call run , The Docker client contacts the Docker daemon The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Store. (Since we have issued docker pull alpine before, the download step is not necessary) The Docker daemon creates the container and then runs a command in that container. The Docker daemon streams the output of the command to the Docker client When you run docker run alpine , you provided a command ( ls -l ), so Docker started the command specified and you saw the listing. Let's try something more exciting. $ docker run alpine echo \"hello from alpine\" hello from alpine OK, that's some actual output. In this case, the Docker client dutifully ran the echo command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Try another command. docker run alpine /bin/sh Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to docker run -it alpine /bin/sh . You are now inside the container shell and you can try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Since no containers are running, you see a blank line. Let's try a more useful variant: docker ps -a $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36171a5da744 alpine \"/bin/sh\" 5 minutes ago Exited (0) 2 minutes ago fervent_newton a6a9d46d0b2f alpine \"echo 'hello from alp\" 6 minutes ago Exited (0) 6 minutes ago lonely_kilby ff0a5c3750b9 alpine \"ls -l\" 8 minutes ago Exited (0) 8 minutes ago elated_ramanujan c317d0a9e3d2 hello-world \"/hello\" 34 seconds ago Exited (0) 12 minutes ago stupefied_mcclintock What you see above is a list of all containers that you ran. Notice that the STATUS column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now: $ docker run -it alpine /bin/sh / # ls bin dev etc home lib linuxrc media mnt proc root run sbin sys tmp usr var / # uname -a Linux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux Running the run command with the -it flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands. That concludes a whirlwind tour of the docker run command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run , use docker run --help to see a list of all flags it supports. As you proceed further, we'll see a few more variants of docker run .","title":"1.1 Docker Run"},{"location":"1_3_docker.html#12-terminology","text":"In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem. Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run docker inspect alpine . In the demo above, you used the docker pull command to download the alpine image. When you executed the command docker run hello-world , it also did a docker pull behind the scenes to download the hello-world image. Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using docker run which you did using the alpine image that you downloaded. A list of running containers can be seen using the docker ps command. Docker daemon - The background service running on the host that manages building, running and distributing Docker containers. Docker client - The command line tool that allows the user to interact with the Docker daemon. Docker Store - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial.","title":"1.2 Terminology"},{"location":"1_3_docker.html#20-webapps-with-docker","text":"Source: https://github.com/docker/labs Great! So you have now looked at docker run , played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker.","title":"2.0 Webapps with Docker"},{"location":"1_3_docker.html#21-run-a-static-website-in-a-container","text":"Note: Code for this section is in this repo in the website directory Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Store, run the container, and see how easy it is to set up a web server. The image that you are going to use is a single-page website that was already created for this demo and is available on the Docker Store as dockersamples/static-site . You can download and run the image directly in one go using docker run as follows. docker run -d dockersamples/static-site Files: Dockerfile hello_docker.html Note: The current version of this image doesn't run without the -d flag. The -d flag enables detached mode , which detaches the running container from the terminal/shell and returns your prompt after the container starts. We are debugging the problem with this image but for now, use -d even for this first example. So, what happens when you run this command? Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container. Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine? Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the docker run command to add this instruction. Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode. First, stop the container that you have just launched. In order to do this, we need the container ID. Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run docker ps to view the running containers. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7a0e504ca3e dockersamples/static-site \"/bin/sh -c 'cd /usr/\" 28 seconds ago Up 26 seconds 80 /tcp, 443 /tcp stupefied_mahavira Check out the CONTAINER ID column. You will need to use this CONTAINER ID value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the CONTAINER ID on our system; you should use the value that you see in your terminal. $ docker stop a7a0e504ca3e $ docker rm a7a0e504ca3e Note: A cool feature is that you do not need to specify the entire CONTAINER ID . You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up. Now, let's launch a container in detached mode as shown below: $ docker run --name static-site -e AUTHOR = \"Your Name\" -d -P dockersamples/static-site e61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810 In the above command: -d will create a container with the process detached from our terminal -P will publish all the exposed container ports to random ports on the Docker host -e is how you pass environment variables to the container --name allows you to specify a container name AUTHOR is the environment variable name and Your Name is the value that you can pass Now you can see the ports by running the docker port command. $ docker port static-site 443 /tcp -> 0 .0.0.0:32772 80 /tcp -> 0 .0.0.0:32773 If you are running Docker for Mac , Docker for Windows , or Docker on Linux, you can open http://localhost:[YOUR_PORT_FOR 80/tcp] . For our example this is http://localhost:32773 . If you are using Docker Machine on Mac or Windows, you can find the hostname on the command line using docker-machine as follows (assuming you are using the default machine). $ docker-machine ip default 192 .168.99.100 You can now open http://<YOUR_IPADDRESS>:[YOUR_PORT_FOR 80/tcp] to see your site live! For our example, this is: http://192.168.99.100:32773 . You can also run a second webserver at the same time, specifying a custom host port mapping to the container's webserver. $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site To deploy this on a real server you would just need to install Docker, and run the above docker command(as in this case you can see the AUTHOR is Docker which we passed as an environment variable). Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section. But first, let's stop and remove the containers since you won't be using them anymore. $ docker stop static-site $ docker rm static-site Let's use a shortcut to remove the second site: $ docker rm -f static-site-2 Run docker ps to make sure the containers are gone. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES","title":"2.1 Run a static website in a container"},{"location":"1_3_docker.html#22-docker-images","text":"In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally. Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE dockersamples/static-site latest 92a386b6e686 2 hours ago 190 .5 MB nginx latest af4b3d7d5401 3 hours ago 190 .5 MB python 2 .7 1c32174fd534 14 hours ago 676 .8 MB postgres 9 .4 88d845ac7a88 14 hours ago 263 .6 MB containous/traefik latest 27b4e0c6b2fd 4 days ago 20 .75 MB node 0 .10 42426a5cba5f 6 days ago 633 .7 MB redis latest 4f5f397d4b7c 7 days ago 177 .5 MB mongo latest 467eb21035a8 7 days ago 309 .7 MB alpine 3 .3 70c557e50ed6 8 days ago 4 .794 MB java 7 21f6ce84e43c 8 days ago 587 .7 MB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest . For example you could pull a specific version of ubuntu image as follows: $ docker pull ubuntu:12.04 If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named latest . So for example, the docker pull command given below will pull an image named ubuntu:latest : $ docker pull ubuntu To get a new Docker image you can either get it from a registry (such as the Docker Store) or create your own. There are hundreds of thousands of images available on Docker Store . You can also search for images directly from the command line using docker search . An important distinction with regard to images is between base images and child images . Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian. Child images are images that build on base images and add additional functionality. Another key concept is the idea of official images and user images . (Both of which can be base images or child images.) Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the python , node , alpine and nginx images are official (base) images. To find out more about them, check out the Official Images Documentation . User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as user/image-name . The user value in the image name is your Docker Store user or organization name.","title":"2.2 Docker Images"},{"location":"1_3_docker.html#23-create-your-first-image","text":"Note: The code for this section is in this repository in the flask-app directory. Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application. The goal of this exercise is to create a Docker image which will run a Flask app. We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile . Finally, we'll build the image, and then run it. Create a Python Flask app that displays random cat pix Write a Dockerfile Build the image Run your image Dockerfile commands summary","title":"2.3 Create your first image"},{"location":"1_3_docker.html#231-create-a-python-flask-app-that-displays-random-cat-pix","text":"For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat .gif every time it is loaded - because, you know, who doesn't like cats? Start by creating a directory called flask-app where we'll create the following files: app.py requirements.txt templates/index.html Dockerfile Make sure to cd flask-app before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image.","title":"2.3.1 Create a Python Flask app that displays random cat pix"},{"location":"1_3_docker.html#apppy","text":"Create the app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26388-1381844103-11.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr01/15/9/anigif_enhanced-buzz-31540-1381844535-8.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26390-1381844163-18.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-1376-1381846217-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3391-1381844336-26.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/10/anigif_enhanced-buzz-29111-1381845968-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/9/anigif_enhanced-buzz-3409-1381844582-13.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr02/15/9/anigif_enhanced-buzz-19667-1381844937-10.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr05/15/9/anigif_enhanced-buzz-26358-1381845043-13.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-18774-1381844645-6.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr06/15/9/anigif_enhanced-buzz-25158-1381844793-0.gif\" , \"http://img.buzzfeed.com/buzzfeed-static/static/2013-10/enhanced/webdr03/15/10/anigif_enhanced-buzz-11980-1381846269-1.gif\" ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" )","title":"app.py"},{"location":"1_3_docker.html#requirementstxt","text":"In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file: Flask==0.10.1","title":"requirements.txt"},{"location":"1_3_docker.html#templatesindexhtml","text":"Create a directory called templates and create an index.html file in that directory with the following content in it: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> < p >< small > Courtesy: < a href = \"http://www.buzzfeed.com/copyranter/the-best-cat-gif-post-in-the-history-of-cat-gifs\" > Buzzfeed </ a ></ small ></ p > </ div > </ body > </ html >","title":"templates/index.html"},{"location":"1_3_docker.html#232-write-a-dockerfile","text":"We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image . Since our application is written in Python, we will build our own Python image based on Alpine . We'll do that using a Dockerfile . A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles. Create a file called Dockerfile , and add content to it as described below. We'll start by specifying our base image, using the FROM keyword: FROM alpine:3.5 The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next: RUN apk add --update py2-pip Let's add the files that make up the Flask Application. Install all Python requirements for our app to run. This will be accomplished by adding the lines: COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt Copy the files you have created earlier into our image by using COPY command. COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ Specify the port number which needs to be exposed. Since our flask app is running on 5000 that's what we'll expose. EXPOSE 5000 The last step is the command for running the application which is simply - python ./app.py . Use the CMD command to do that: CMD [\"python\", \"/usr/src/app/app.py\"] The primary purpose of CMD is to tell the container which command it should run by default when it is started. Verify your Dockerfile. Our Dockerfile is now ready. This is how it looks: # our base image FROM alpine:3.5 # Install python and pip RUN apk add --update py2-pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [\"python\", \"/usr/src/app/app.py\"]","title":"2.3.2 Write a Dockerfile"},{"location":"1_3_docker.html#233-build-the-image","text":"Now that you have your Dockerfile , you can build your image. The docker build command does the heavy-lifting of creating a docker image from a Dockerfile . The docker build command is quite simple - it takes an optional tag name with the -t flag, and the location of the directory containing the Dockerfile - the . indicates the current directory: docker build -t myfirstapp:1.0 $ docker build -t myfirstapp:1.0 . Sending build context to Docker daemon 9.728 kB Step 1 : FROM alpine:latest ---> 0d81fc72e790 Step 2 : RUN apk add --update py-pip ---> Running in 8abd4091b5f5 fetch http://dl-4.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz fetch http://dl-4.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz (1/12) Installing libbz2 (1.0.6-r4) (2/12) Installing expat (2.1.0-r2) (3/12) Installing libffi (3.2.1-r2) (4/12) Installing gdbm (1.11-r1) (5/12) Installing ncurses-terminfo-base (6.0-r6) (6/12) Installing ncurses-terminfo (6.0-r6) (7/12) Installing ncurses-libs (6.0-r6) (8/12) Installing readline (6.3.008-r4) (9/12) Installing sqlite-libs (3.9.2-r0) (10/12) Installing python (2.7.11-r3) (11/12) Installing py-setuptools (18.8-r0) (12/12) Installing py-pip (7.1.2-r0) Executing busybox-1.24.1-r7.trigger OK: 59 MiB in 23 packages ---> 976a232ac4ad Removing intermediate container 8abd4091b5f5 Step 3 : COPY requirements.txt /usr/src/app/ ---> 65b4be05340c Removing intermediate container 29ef53b58e0f Step 4 : RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt ---> Running in a1f26ded28e7 Collecting Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1)) Downloading Flask-0.10.1.tar.gz (544kB) Collecting Werkzeug>=0.7 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Werkzeug-0.11.4-py2.py3-none-any.whl (305kB) Collecting Jinja2>=2.4 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB) Collecting itsdangerous>=0.21 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading itsdangerous-0.24.tar.gz (46kB) Collecting MarkupSafe (from Jinja2>=2.4->Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading MarkupSafe-0.23.tar.gz Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask Running setup.py install for MarkupSafe Running setup.py install for itsdangerous Running setup.py install for Flask Successfully installed Flask-0.10.1 Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.4 itsdangerous-0.24 You are using pip version 7.1.2, however version 8.1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. ---> 8de73b0730c2 Removing intermediate container a1f26ded28e7 Step 5 : COPY app.py /usr/src/app/ ---> 6a3436fca83e Removing intermediate container d51b81a8b698 Step 6 : COPY templates/index.html /usr/src/app/templates/ ---> 8098386bee99 Removing intermediate container b783d7646f83 Step 7 : EXPOSE 5000 ---> Running in 31401b7dea40 ---> 5e9988d87da7 Removing intermediate container 31401b7dea40 Step 8 : CMD python /usr/src/app/app.py ---> Running in 78e324d26576 ---> 2f7357a0805d Removing intermediate container 78e324d26576 Successfully built 2f7357a0805d If you don't have the alpine:3.5 image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image ( <YOUR_USERNAME>/myfirstapp ) shows.","title":"2.3.3 Build the image"},{"location":"1_3_docker.html#234-run-your-image","text":"The next step in this section is to run the image and see if it actually works. $ docker run -p 8888 :5000 --name myfirstapp myfirstapp:1.0 * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) Head over to http://localhost:8888 and your app should be live. Note If you are using Docker Machine, you may need to open up another terminal and determine the container ip address using docker-machine ip default . Hit the Refresh button in the web browser to see a few more cat images.","title":"2.3.4 Run your image"},{"location":"1_3_docker.html#235-dockerfile-commands-summary","text":"Here's a quick summary of the few basic commands we used in our Dockerfile. FROM starts the Dockerfile. It is a requirement that the Dockerfile must start with the FROM command. Images are created in layers, which means you can use another image as the base image for your own. The FROM command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format username/imagename:version . RUN is used to build up the Image you're creating. For each RUN command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a RUN instruction is to place the full text of the shell command after the RUN (e.g., RUN mkdir /user/local/foo ). This will automatically run in a /bin/sh shell. You can define a different shell like this: RUN /bin/bash -c 'mkdir /user/local/foo' COPY copies local files into the container. CMD defines the commands that will run on the Image at start-up. Unlike a RUN , this does not create a new layer for the Image, but simply runs the command. There can only be one CMD per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the CMD run a script. CMD requires that you tell it where to run the command, unlike RUN . So example CMD commands would be: CMD [\"python\", \"./app.py\"] CMD [\"/bin/bash\", \"echo\", \"Hello World\"] EXPOSE creates a hint for users of an image which ports provide services. It is included in the information which can be retrieved via $ docker inspect <container-id> . Note: The EXPOSE command does not actually make any ports accessible to the host! Instead, this requires publishing ports by means of the -p flag when using $ docker run . PUSH pushes your image to Docker Cloud, or alternately to a private registry Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles .","title":"2.3.5 Dockerfile commands summary"},{"location":"1_3_docker.html#3-containers-registry","text":"Remember Container Registries ? Here as some explainers The main container registry is dockerhub, https://hub.docker.com/ All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before Example, the Python Image Google Cloud has a Container Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only. However, it requires naming the image in a specific fashion: eu.gcr.io/${PROJECT_ID}/name:tag Use the docker cli to tag your previous myfirstapp image to the right namespace docker tag myfirstapp eu.gcr.io/{PROJECT_ID}/myfirstapp:1.0 Upload it on container registry docker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG] If you have a problem of authentification, gcloud auth configure-docker Hint to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Go to container registry https://console.cloud.google.com/gcr, you should see your docker image :)","title":"3. Containers Registry"},{"location":"1_3_docker.html#4-data-science-standardized-environment","text":"","title":"4.  Data Science Standardized Environment"},{"location":"1_3_docker.html#41-intro","text":"Those of us who work on a team know how hard it is to create a standardize development environment. Or if you have ever updated a dependency and had everything break, you understand the importance of keeping development environments isolated. Using Docker, we can create a project / team image with our development environment and mount a volume with our notebooks and data. The benefits of this workflow are that we can: Separate out projects Spin up a container to onboard new employees Build an automated testing pipeline to confirm upgrade dependencies do not break code","title":"4.1 Intro"},{"location":"1_3_docker.html#42-kaggle-docker-image","text":"For this exercise we will use Kaggle Docker Image which is a fully configured docker image that can be used as a data science container Take a look at the documentation and the repository","title":"4.2 Kaggle Docker Image"},{"location":"1_3_docker.html#43-get-the-algorithm-in-ml-git-in-your-virtual-machine","text":"From your vm, run git clone https://github.com/erachelson/MLclass.git , this should setup your AML class inside your VM Using code-server is not mandatory now","title":"4.3 Get the algorithm in ML git in your Virtual Machine"},{"location":"1_3_docker.html#44-mounting-volumes-and-ports","text":"Now let's run the image. This container has a jupyter notebook accessible from port 8080 so we will need to map the host port 8888 (the one accessible from the ssh tunnel) to the docker port 8080, we will use port forwarding We will also need to make available the notebooks on the VM to the container... we will mount volumes . Your data is located in /home/${USER}/MLClass and we want to miunt it in /tmp/workdir docker run --rm -it \\ -p 8888 :8080 \\ -v /home/ ${ USER } /MLclass:/home/Mlclass \\ --workdir /home/ \\ gcr.io/kaggle-images/python \\ /bin/bash /run_jupyter.sh Note: this image is very large ! Options breakdown: * --rm remove the container when we stop it * -it run the container in interactive mode * -p forward port from host:container * other: options from the kaggle container You should now see a jupyter lab with mlclass accessible if you connect your browser (in your laptop) to port 8888 (localhost:8888) So basically, we mapped the ports local 8888 to vm 8888 and vm 8888 to docker 8080","title":"4.4 Mounting volumes and ports"},{"location":"1_3_docker.html#5-bonus-using-google-cloud-tools-for-docker","text":"Using cloud shell you should be able to do the Hello World Dockerfile exercise except that instead of using docker build you use Google Cloud Build Tutorial: https://cloud.google.com/cloud-build/docs/quickstart-docker Example command : gcloud builds submit --tag eu.gcr.io/$PROJECT_ID/{image}:{tag} . Help to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Example Try to build the hello world app","title":"5. Bonus - Using Google Cloud Tools for Docker"},{"location":"1_3_docker.html#6-bonus-docker-compose","text":"https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6 https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md","title":"6. Bonus - Docker Compose"},{"location":"1_3_docker.html#7-bonus-going-further","text":"https://container.training/","title":"7. Bonus - Going further"},{"location":"1_4_be.html","text":"Bureau d'\u00e9tudes Cloud & Docker \ud83d\udd17 Objectives of this BE \ud83d\udd17 This Bureau d'\u00e9tudes (BE, for short) will guide you through the essential notions to be able to manipulate with regard to cloud computer and docker, We will illustrate the following: Creation and ssh connection to virtual machine instances Usage of managed storage capabilities Creating your own docker images Exchanging docker images through a Container Registry Pulling and running docker images created by your teammates In particular, this workflow: Warning Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions. How to run this BE \ud83d\udd17 There are two ways of interacting with google cloud platform, Locally with the google-cloud-sdk (4G network & gcloud installed) With the web-based Google Coud Shell We will be using the gcloud CLI in Google Cloud Shell or the gcloud sdk for the following: Create a GCE Virtual Machine Connect to SSH with port forwarding to said machine The rest will be done from the virtual machine except web preview that you can do from your browser Warning If you are on ISAE-EDU, using the google-cloud-sdk will not work, you will have to use your web browser exclusively For the rest of this walkthrough, if it is written \"from your local machine\", this will be either your web browser, or your laptop's terminal or google cloud shell If it is written \"inside the VM\", this means that you have to run the SSH tunnel first... \ud83d\ude4f\ud83c\udffb Use Google Chrome without any ad blockers for console.cloud.google.com if you have any issues Warning \u26a0\ufe0f ISAE-EDU is tricky and may prevent you from correctly connecting \u26a0\ufe0f Remember that you may be often disconnected of cloud shell so using the cloud sdk locally is often easier :) Team composition \ud83d\udd17 You should be in team of 5, however this will work with a minimum of 2 people. Designate a \"project manager\" (the person who is the most comfortable with the google cloud platform UI). She or He will have the hard task of giving access to his/her GCP project to the other team members to enable collaboration. This means that the project of the \"team leader\" will be billed a little more for the duration of this BE, so please be kind with the project and apply good cloud hygiene :) Each team member picks a different cute mascot and remembers it: \ud83d\udc08 cat \ud83d\udc15 dog \ud83d\udc7d (baby) yoda \ud83e\udd89 owl \ud83d\udc3c panda You should get access to a slack channel to collaborate; use it extensively :) 1 - Share access to projects \ud83d\udd17 The \"project manager\" must give access to his/her teammates using the IAM menu Add each team member by email address as editor (Quick Access -> Basic -> Editor) by following this tutorial Each member should then see in their respective project list the new project Warning This means that only one project will be billed. The project manager can remove accesss to the other teammates when the BE is finished. Do not forget to apply proper cloud hygiene and delete your instances when you are finished, even if you are not paying ! 2 - Create Google Compute Engine VM \ud83d\udd17 Each team member creates a separate machine on the same project , Here, you will create a Google Compute Engine instance, preconfigured with everything you need, If you use the CLI (either your local google cloud sdk or google cloud shell), you can use this First, set a variable with the name of your instance, export INSTANCE_NAME = \"be-cloud-mascot\" # RENAME THIS !!!!!!!!!! Then create your VM gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-2\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 50GB If you use the web interface, follow this Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a Virtual Machine and what is Google Compute Engine 3 - Connect using SSH to the instance \ud83d\udd17 Option A with google cloud sdk \ud83d\udd17 If you are using the google cloud sdk, you can connect to ssh using the usual command (refer to the first hands-on ) with SSH Tunneling Tunnel the following ports to your local machine: 8080: This is reserved for a jupyter lab session by default, it makes it easy to see & edit text 8081: You will neeed to run containers and expose them on a port Hint gcloud compute ssh { user } @ { instance } -- \\ -L { client-port } :localhost: { server-port } \\ -L { client-port-2 } :localhost: { server-port-2 } Go to your browser and connect to http://localhost:8080, you should be in a jupyter lab where you can access a terminal, a text editor etc... this will make your life simpler. Option B with the web browser (& google cloud shell) \ud83d\udd17 If you can't SSH using the command line you will need to either Use the in-browser SSH capabilities to connect to your instance (but that does not allow port forwarding so your life will be harder later on) Make a SSH tunnel between Google Cloud Shell and your VM with port forwarding AND use the web preview on port 8080 to access jupyter lab (and the web preview on port 8081 later on) Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a SSH Tunnel and what is port forwarding 4 - Get the necessary resources from Google Cloud Storage \ud83d\udd17 The resources are located at the URI gs://fchouteau-isae-cloud/be/${MASCOT} , Your ${MASCOT} name is either: cat dog owl panda yoda I advise you to export MASCOT=.... to remember it :) ONLY DOWNLOAD your mascot resources (no cheating ! this will only cause confusion later) Download them to your instance using the gcloud cli (refer to your previous work for more information) Hint gsutil -m cp -r { source } { destination } Remember that google storage URIs always begin with gs:// Go to ( cd ) the folder where you downloaded your resources You should see a file structure like this fchouteau@be-cloud-mascot:~/be$ tree yoda -L 2 yoda \u251c\u2500\u2500 app.py \u251c\u2500\u2500 AUTHOR.txt \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 favicon.ico \u251c\u2500\u2500 imgs \u2502 \u251c\u2500\u2500 1.gif \u2502 \u251c\u2500\u2500 2.gif \u2502 \u251c\u2500\u2500 3.gif \u2502 \u251c\u2500\u2500 4.gif \u2502 \u2514\u2500\u2500 5.gif \u2514\u2500\u2500 template.html.jinja2 1 directory, 10 files 5 - Build your docker image \ud83d\udd17 Question Look at the Dockerfile ( cat Dockerfile ), what does it seem to do ? Look at app.py ( cat app.py ). What is Flask ? What does it seem to do ? Edit the file AUTHOR.txt to add your name instead of the placeholder Refer to your previous work to build the image Danger On which port is your flask app running ? ( cat Dockerfile ) Note it carefully ! You will need to communicate it to your teammate :) Hint To edit a file, it's easier if you are on jupyter lab... Otherwise you can still use nano or vim Here a VIM tutorial https://www.openvim.com/ , good luck with that Otherwise it's simple (or not): vim {file}, switch to edit mode (type :edit ), edit your line, ESCAPE then exit VIM like this :wq! Nano is easier ! https://linuxize.com/post/how-to-use-nano-text-editor/ When building the image, name it appropriately... like eu.gcr.io/${PROJECT_ID}/webapp-gif:${MASCOT}-1.0 ! Hint to get your project id: PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) now if you list your images you should see it ! REPOSITORY TAG IMAGE ID CREATED SIZE eu.gcr.io/{your project}/{your-app} 1.0 d1c5993848bf 2 minutes ago 62.1MB Question Describe concisely (on slack) to your past self (before the pandemic) what is a Docker Image 6 - Push your Docker image in the Container Registry \ud83d\udd17 Now push your image on the shared container registry Help your team mates so that everybody can build his/her Docker Image Question Describe succintly (on slack) to your past self (before the pandemic) what is a Container Registry In the end, things should look like this 7 - Pull Docker Images from your teammates \ud83d\udd17 Select another mascot and pull the corresponding docker image from the registry List the docker images you have. You should have at least 2 including yours 8 - Run Docker Containers from their Docker Images \ud83d\udd17 Run your container while mapping the correct port to your VM 8081 . Which port is it ? Well, ask the one who built the image. When running the container, setup the USER environment variable to your name ! Hint the port is not the same as yours if you don't set the username, it will show later ;) 9 - Display the results & share them \ud83d\udd17 TECHNICALLY you just launched a webapp on the port 8081 of your remote instance. If you have a ssh tunnel directly from your laptop, ensure that you made a tunnel for your port 8081 to any port of your machine then, go to http://localhost:(your port) inside your browser. The resulting webpage should appear If you are using google cloud shell, open web preview on port 8081 (you should have a tunnel running between your google cloud shell and your instance) Success The webpage should show the mascot your chose to run The webpage should show the name of the author (not you) The webpage should show your name Bug If any of the three item above are missing, find the bug and solve it :) Example Try to refresh the webpage to make more gifs appear Share your result on slack 10. Cleanup the GCP project \ud83d\udd17 Remove your VMs Remove images from the container registry Success \ud83c\udf89 you have successfully finished the BE. You know how to manipulate the basic notions around cloud computing and docker so that you won't be completely lost when someone will talk about it If you have time left on your hands, do the Kubernetes TP !","title":"Cloud and Docker BE"},{"location":"1_4_be.html#bureau-detudes-cloud-docker","text":"","title":"Bureau d'\u00e9tudes Cloud &amp; Docker"},{"location":"1_4_be.html#objectives-of-this-be","text":"This Bureau d'\u00e9tudes (BE, for short) will guide you through the essential notions to be able to manipulate with regard to cloud computer and docker, We will illustrate the following: Creation and ssh connection to virtual machine instances Usage of managed storage capabilities Creating your own docker images Exchanging docker images through a Container Registry Pulling and running docker images created by your teammates In particular, this workflow: Warning Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions.","title":"Objectives of this BE"},{"location":"1_4_be.html#how-to-run-this-be","text":"There are two ways of interacting with google cloud platform, Locally with the google-cloud-sdk (4G network & gcloud installed) With the web-based Google Coud Shell We will be using the gcloud CLI in Google Cloud Shell or the gcloud sdk for the following: Create a GCE Virtual Machine Connect to SSH with port forwarding to said machine The rest will be done from the virtual machine except web preview that you can do from your browser Warning If you are on ISAE-EDU, using the google-cloud-sdk will not work, you will have to use your web browser exclusively For the rest of this walkthrough, if it is written \"from your local machine\", this will be either your web browser, or your laptop's terminal or google cloud shell If it is written \"inside the VM\", this means that you have to run the SSH tunnel first... \ud83d\ude4f\ud83c\udffb Use Google Chrome without any ad blockers for console.cloud.google.com if you have any issues Warning \u26a0\ufe0f ISAE-EDU is tricky and may prevent you from correctly connecting \u26a0\ufe0f Remember that you may be often disconnected of cloud shell so using the cloud sdk locally is often easier :)","title":"How to run this BE"},{"location":"1_4_be.html#team-composition","text":"You should be in team of 5, however this will work with a minimum of 2 people. Designate a \"project manager\" (the person who is the most comfortable with the google cloud platform UI). She or He will have the hard task of giving access to his/her GCP project to the other team members to enable collaboration. This means that the project of the \"team leader\" will be billed a little more for the duration of this BE, so please be kind with the project and apply good cloud hygiene :) Each team member picks a different cute mascot and remembers it: \ud83d\udc08 cat \ud83d\udc15 dog \ud83d\udc7d (baby) yoda \ud83e\udd89 owl \ud83d\udc3c panda You should get access to a slack channel to collaborate; use it extensively :)","title":"Team composition"},{"location":"1_4_be.html#1-share-access-to-projects","text":"The \"project manager\" must give access to his/her teammates using the IAM menu Add each team member by email address as editor (Quick Access -> Basic -> Editor) by following this tutorial Each member should then see in their respective project list the new project Warning This means that only one project will be billed. The project manager can remove accesss to the other teammates when the BE is finished. Do not forget to apply proper cloud hygiene and delete your instances when you are finished, even if you are not paying !","title":"1 - Share access to projects"},{"location":"1_4_be.html#2-create-google-compute-engine-vm","text":"Each team member creates a separate machine on the same project , Here, you will create a Google Compute Engine instance, preconfigured with everything you need, If you use the CLI (either your local google cloud sdk or google cloud shell), you can use this First, set a variable with the name of your instance, export INSTANCE_NAME = \"be-cloud-mascot\" # RENAME THIS !!!!!!!!!! Then create your VM gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-2\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 50GB If you use the web interface, follow this Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a Virtual Machine and what is Google Compute Engine","title":"2 - Create Google Compute Engine VM"},{"location":"1_4_be.html#3-connect-using-ssh-to-the-instance","text":"","title":"3 - Connect using SSH to the instance"},{"location":"1_4_be.html#option-a-with-google-cloud-sdk","text":"If you are using the google cloud sdk, you can connect to ssh using the usual command (refer to the first hands-on ) with SSH Tunneling Tunnel the following ports to your local machine: 8080: This is reserved for a jupyter lab session by default, it makes it easy to see & edit text 8081: You will neeed to run containers and expose them on a port Hint gcloud compute ssh { user } @ { instance } -- \\ -L { client-port } :localhost: { server-port } \\ -L { client-port-2 } :localhost: { server-port-2 } Go to your browser and connect to http://localhost:8080, you should be in a jupyter lab where you can access a terminal, a text editor etc... this will make your life simpler.","title":"Option A with google cloud sdk"},{"location":"1_4_be.html#option-b-with-the-web-browser-google-cloud-shell","text":"If you can't SSH using the command line you will need to either Use the in-browser SSH capabilities to connect to your instance (but that does not allow port forwarding so your life will be harder later on) Make a SSH tunnel between Google Cloud Shell and your VM with port forwarding AND use the web preview on port 8080 to access jupyter lab (and the web preview on port 8081 later on) Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a SSH Tunnel and what is port forwarding","title":"Option B with the web browser (&amp; google cloud shell)"},{"location":"1_4_be.html#4-get-the-necessary-resources-from-google-cloud-storage","text":"The resources are located at the URI gs://fchouteau-isae-cloud/be/${MASCOT} , Your ${MASCOT} name is either: cat dog owl panda yoda I advise you to export MASCOT=.... to remember it :) ONLY DOWNLOAD your mascot resources (no cheating ! this will only cause confusion later) Download them to your instance using the gcloud cli (refer to your previous work for more information) Hint gsutil -m cp -r { source } { destination } Remember that google storage URIs always begin with gs:// Go to ( cd ) the folder where you downloaded your resources You should see a file structure like this fchouteau@be-cloud-mascot:~/be$ tree yoda -L 2 yoda \u251c\u2500\u2500 app.py \u251c\u2500\u2500 AUTHOR.txt \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 favicon.ico \u251c\u2500\u2500 imgs \u2502 \u251c\u2500\u2500 1.gif \u2502 \u251c\u2500\u2500 2.gif \u2502 \u251c\u2500\u2500 3.gif \u2502 \u251c\u2500\u2500 4.gif \u2502 \u2514\u2500\u2500 5.gif \u2514\u2500\u2500 template.html.jinja2 1 directory, 10 files","title":"4 - Get the necessary resources from Google Cloud Storage"},{"location":"1_4_be.html#5-build-your-docker-image","text":"Question Look at the Dockerfile ( cat Dockerfile ), what does it seem to do ? Look at app.py ( cat app.py ). What is Flask ? What does it seem to do ? Edit the file AUTHOR.txt to add your name instead of the placeholder Refer to your previous work to build the image Danger On which port is your flask app running ? ( cat Dockerfile ) Note it carefully ! You will need to communicate it to your teammate :) Hint To edit a file, it's easier if you are on jupyter lab... Otherwise you can still use nano or vim Here a VIM tutorial https://www.openvim.com/ , good luck with that Otherwise it's simple (or not): vim {file}, switch to edit mode (type :edit ), edit your line, ESCAPE then exit VIM like this :wq! Nano is easier ! https://linuxize.com/post/how-to-use-nano-text-editor/ When building the image, name it appropriately... like eu.gcr.io/${PROJECT_ID}/webapp-gif:${MASCOT}-1.0 ! Hint to get your project id: PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) now if you list your images you should see it ! REPOSITORY TAG IMAGE ID CREATED SIZE eu.gcr.io/{your project}/{your-app} 1.0 d1c5993848bf 2 minutes ago 62.1MB Question Describe concisely (on slack) to your past self (before the pandemic) what is a Docker Image","title":"5 - Build your docker image"},{"location":"1_4_be.html#6-push-your-docker-image-in-the-container-registry","text":"Now push your image on the shared container registry Help your team mates so that everybody can build his/her Docker Image Question Describe succintly (on slack) to your past self (before the pandemic) what is a Container Registry In the end, things should look like this","title":"6 - Push your Docker image in the Container Registry"},{"location":"1_4_be.html#7-pull-docker-images-from-your-teammates","text":"Select another mascot and pull the corresponding docker image from the registry List the docker images you have. You should have at least 2 including yours","title":"7 - Pull Docker Images from your teammates"},{"location":"1_4_be.html#8-run-docker-containers-from-their-docker-images","text":"Run your container while mapping the correct port to your VM 8081 . Which port is it ? Well, ask the one who built the image. When running the container, setup the USER environment variable to your name ! Hint the port is not the same as yours if you don't set the username, it will show later ;)","title":"8 - Run Docker Containers from their Docker Images"},{"location":"1_4_be.html#9-display-the-results-share-them","text":"TECHNICALLY you just launched a webapp on the port 8081 of your remote instance. If you have a ssh tunnel directly from your laptop, ensure that you made a tunnel for your port 8081 to any port of your machine then, go to http://localhost:(your port) inside your browser. The resulting webpage should appear If you are using google cloud shell, open web preview on port 8081 (you should have a tunnel running between your google cloud shell and your instance) Success The webpage should show the mascot your chose to run The webpage should show the name of the author (not you) The webpage should show your name Bug If any of the three item above are missing, find the bug and solve it :) Example Try to refresh the webpage to make more gifs appear Share your result on slack","title":"9 - Display the results &amp; share them"},{"location":"1_4_be.html#10-cleanup-the-gcp-project","text":"Remove your VMs Remove images from the container registry Success \ud83c\udf89 you have successfully finished the BE. You know how to manipulate the basic notions around cloud computing and docker so that you won't be completely lost when someone will talk about it If you have time left on your hands, do the Kubernetes TP !","title":"10. Cleanup the GCP project"},{"location":"1_4_orchestration.html","text":"Orchestration & Kubernetes \ud83d\udd17 Link to slides","title":"Lecture"},{"location":"1_4_orchestration.html#orchestration-kubernetes","text":"Link to slides","title":"Orchestration &amp; Kubernetes"},{"location":"1_5_bonus.html","text":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine \ud83d\udd17 What is JupyterHub \ud83d\udd17 JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators. JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more. Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub). Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users. Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware. The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration. Zero to Jupyterhub using Kubernetes \ud83d\udd17 JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team). This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider. Instructions \ud83d\udd17 Go here and follow the instructions Use Google Kubernetes Engine to setup your cluster Info You will use the same method later in the year to setup a Dask Kubernetes cluster using helm Give some people the public IP of your cluster so that they can connect to it... try to make it scale !","title":"TP"},{"location":"1_5_bonus.html#kubernetes-zero-to-jupyterhub-using-google-kubernetes-engine","text":"","title":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine"},{"location":"1_5_bonus.html#what-is-jupyterhub","text":"JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators. JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more. Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub). Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users. Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware. The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration.","title":"What is JupyterHub"},{"location":"1_5_bonus.html#zero-to-jupyterhub-using-kubernetes","text":"JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team). This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider.","title":"Zero to Jupyterhub using Kubernetes"},{"location":"1_5_bonus.html#instructions","text":"Go here and follow the instructions Use Google Kubernetes Engine to setup your cluster Info You will use the same method later in the year to setup a Dask Kubernetes cluster using helm Give some people the public IP of your cluster so that they can connect to it... try to make it scale !","title":"Instructions"},{"location":"1_5_deployment.html","text":"From Virtualisation to Containerisation \ud83d\udd17 Link to slides","title":"Interactive Hands On"},{"location":"1_5_deployment.html#from-virtualisation-to-containerisation","text":"Link to slides","title":"From Virtualisation to Containerisation"},{"location":"1_6_conclusion.html","text":"Introduction \ud83d\udd17 Link to slides","title":"Conclusion"},{"location":"1_6_conclusion.html#introduction","text":"Link to slides","title":"Introduction"},{"location":"1_7_readings.html","text":"Readings \ud83d\udd17 About Cloud Computing \ud83d\udd17 Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... & Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade . ACM computing surveys (CSUR), 51(5), 1-38. On sustainable data centers and energy use (intro) The NIST Definitions of Cloud Computing Open Data: Open Sentinel 2 archive on AWS Environmental Impact of Cloud vs On Premise Environmental Impact of cloud vs on-premise medium blog post Paper from Natural Resources Defense Council on Cloud vs On-Premise Anecdotes about Cloud Computing About Containers \ud83d\udd17 Docker whitepaper: Docker and the way of the Devops What exactly is Docker ? Simple explanation from a medium blog post About Orchestration \ud83d\udd17 Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., & Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg . In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17). Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic","title":"Reading List"},{"location":"1_7_readings.html#readings","text":"","title":"Readings"},{"location":"1_7_readings.html#about-cloud-computing","text":"Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... & Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade . ACM computing surveys (CSUR), 51(5), 1-38. On sustainable data centers and energy use (intro) The NIST Definitions of Cloud Computing Open Data: Open Sentinel 2 archive on AWS Environmental Impact of Cloud vs On Premise Environmental Impact of cloud vs on-premise medium blog post Paper from Natural Resources Defense Council on Cloud vs On-Premise Anecdotes about Cloud Computing","title":"About Cloud Computing"},{"location":"1_7_readings.html#about-containers","text":"Docker whitepaper: Docker and the way of the Devops What exactly is Docker ? Simple explanation from a medium blog post","title":"About Containers"},{"location":"1_7_readings.html#about-orchestration","text":"Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., & Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg . In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17). Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic","title":"About Orchestration"},{"location":"2_1_overview.html","text":"Data Distribution \ud83d\udd17 Overview \ud83d\udd17 Link to slides Schedule \ud83d\udd17 Data Distribution Readings Data distribution 1h 06/01/2021 Spanner Functional programming 4h 06/01/2021 Julia Hadoop and MapReduce 3h 19/01/2021 MapReduce Spark 3h 19/01/2021 Spark , PySpark Dask on Kubernetes 3h 20/01/2021 Dask documentation Dask project 6h 27/01/2021 Dask Evaluation \ud83d\udd17 The final class will include a graded notebook using Dask on distributed hyperparameter optimization.","title":"Introduction"},{"location":"2_1_overview.html#data-distribution","text":"","title":"Data Distribution"},{"location":"2_1_overview.html#overview","text":"Link to slides","title":"Overview"},{"location":"2_1_overview.html#schedule","text":"Data Distribution Readings Data distribution 1h 06/01/2021 Spanner Functional programming 4h 06/01/2021 Julia Hadoop and MapReduce 3h 19/01/2021 MapReduce Spark 3h 19/01/2021 Spark , PySpark Dask on Kubernetes 3h 20/01/2021 Dask documentation Dask project 6h 27/01/2021 Dask","title":"Schedule"},{"location":"2_1_overview.html#evaluation","text":"The final class will include a graded notebook using Dask on distributed hyperparameter optimization.","title":"Evaluation"},{"location":"2_2_functional.html","text":"Functional Programming \ud83d\udd17 Functional Programming for Distributed Data \ud83d\udd17 Link to slides Introduction to Julia \ud83d\udd17 As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation: Julia download Julia kernel for Jupyter Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance. Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook. Github Colab Functional Programming in Julia \ud83d\udd17 Julia documentation explaining: Functions , showing that they are first-class the map function which is a higher-order function distributed computing allowing for transfer of functions between threads or workers Distributed Data in Julia \ud83d\udd17 Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: DistributedArrays - A general Array type which can be distributed over multiple workers. JuliaDB - A data structuring package which automatically handles distributed data storage and computation Spark.jl - A Julia interface to Apache Spark. Related blog post .","title":"Functional Programming"},{"location":"2_2_functional.html#functional-programming","text":"","title":"Functional Programming"},{"location":"2_2_functional.html#functional-programming-for-distributed-data","text":"Link to slides","title":"Functional Programming for Distributed Data"},{"location":"2_2_functional.html#introduction-to-julia","text":"As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation: Julia download Julia kernel for Jupyter Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance. Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook. Github Colab","title":"Introduction to Julia"},{"location":"2_2_functional.html#functional-programming-in-julia","text":"Julia documentation explaining: Functions , showing that they are first-class the map function which is a higher-order function distributed computing allowing for transfer of functions between threads or workers","title":"Functional Programming in Julia"},{"location":"2_2_functional.html#distributed-data-in-julia","text":"Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: DistributedArrays - A general Array type which can be distributed over multiple workers. JuliaDB - A data structuring package which automatically handles distributed data storage and computation Spark.jl - A Julia interface to Apache Spark. Related blog post .","title":"Distributed Data in Julia"},{"location":"2_3_mapreduce.html","text":"Hadoop and MapReduce \ud83d\udd17 In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example. Slides available on the LMS The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example. MapReduce notebook MapReduce notebook on Colab (requires adding Julia kernel installation)","title":"Hadoop and MapReduce"},{"location":"2_3_mapreduce.html#hadoop-and-mapreduce","text":"In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example. Slides available on the LMS The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example. MapReduce notebook MapReduce notebook on Colab (requires adding Julia kernel installation)","title":"Hadoop and MapReduce"},{"location":"2_4_spark.html","text":"Spark \ud83d\udd17 In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. Slides available on the LMS In the second part of this class, we use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means. Spark notebook Spark notebook on Colab","title":"Spark"},{"location":"2_4_spark.html#spark","text":"In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. Slides available on the LMS In the second part of this class, we use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means. Spark notebook Spark notebook on Colab","title":"Spark"},{"location":"2_5_dask.html","text":"Dask on Kubernetes \ud83d\udd17 In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project . Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn. Presentation Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits. Deploying a Dask Hub notebook Additional resources can be found in the dask documentation . This class builds on the orchestration class given in the fall, going into further detail on K8S specifics.","title":"Dask on Kubernetes"},{"location":"2_5_dask.html#dask-on-kubernetes","text":"In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project . Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn. Presentation Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits. Deploying a Dask Hub notebook Additional resources can be found in the dask documentation . This class builds on the orchestration class given in the fall, going into further detail on K8S specifics.","title":"Dask on Kubernetes"},{"location":"2_6_project.html","text":"Project - Dask \ud83d\udd17 Link to slides Dask tutorial BE notebook The evaluation for this class is a Dask notebook . You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class with instructions in this notebook . You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS . You may work with a partner, in this case make sure to specify in your notebook that you worked together.","title":"Dask project"},{"location":"2_6_project.html#project-dask","text":"Link to slides Dask tutorial BE notebook The evaluation for this class is a Dask notebook . You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class with instructions in this notebook . You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS . You may work with a partner, in this case make sure to specify in your notebook that you worked together.","title":"Project - Dask"},{"location":"3_1_databases.html","text":"Databases \ud83d\udd17 In this module on databases, database management systems will be covered, from basics and SQL to state-of-the-art DBMSs. Students will install and demonstrate the advantages of different DBMSs to their peers as a graded project. In this first class, we introduce the basics of database management systems and cover SQL in detail. Slides In the upcoming classes, students will need to install DBMSs on their local machines. For the next class , students should install PostgreSQL. Databases Databases overview 2h 03/02/2021 Database Systems PostgeSQL TP 3h 08/02/2021 PostgeSQL Project work day 2h 10/02/2021 Project presentations 2h 08/03/2021 Additional Resources \ud83d\udd17 Databases introduction (fr) : includes an explanation of DBMSs, SQL, and PostgreSQL. A comprehensive overview of database systems (en)","title":"Introduction"},{"location":"3_1_databases.html#databases","text":"In this module on databases, database management systems will be covered, from basics and SQL to state-of-the-art DBMSs. Students will install and demonstrate the advantages of different DBMSs to their peers as a graded project. In this first class, we introduce the basics of database management systems and cover SQL in detail. Slides In the upcoming classes, students will need to install DBMSs on their local machines. For the next class , students should install PostgreSQL. Databases Databases overview 2h 03/02/2021 Database Systems PostgeSQL TP 3h 08/02/2021 PostgeSQL Project work day 2h 10/02/2021 Project presentations 2h 08/03/2021","title":"Databases"},{"location":"3_1_databases.html#additional-resources","text":"Databases introduction (fr) : includes an explanation of DBMSs, SQL, and PostgreSQL. A comprehensive overview of database systems (en)","title":"Additional Resources"},{"location":"3_2_postgres.html","text":"PostgeSQL \ud83d\udd17 In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL. Based on the TP by Christophe Garion, CC BY-NC-SA 2015. Setup \ud83d\udd17 For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the postgresql service running on Linux. Installation on Arch Linux Installation on Ubuntu Installation on Windows Additionally, add your login user as a postgresql superuser to enable database creation. Once you've installated and configured PostgreSQL, create the first exercise database: $ createdb db-mexico86 Confirm with pgAdmin that your database db-mexico86 was created. You may need to create a server \"local\" with the host address 127.0.0.1 . Mexico86 database - simple queries \ud83d\udd17 This database contains data from the 1986 football World Cup. Download the creation and insertion scripts and run the scripts in the mexico folder. $ psql -d db-mexico86 mexico86/create-tables-std.sql Exercise 1.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Pays: ( nom , groupe) Typematch: ( type ) Match: ( paysl, paysv , butsl, butsv, type , date) You should be able to make queries now. You can either use PostgreSQL in interactive mode by running $ psql -d db-mexico86 or write your solutions in an SQL file and run the file: $ echo \"SELECT groupe FROM pays;\" > a.sql $ psql -d db-mexico86 -f a.sql You can also use the Query Editor in pgAdmin for a graphical interface. Exercise 1.2 : Write a query which lists the countries participating in the World Cup. Response nom --------------------- Argentine Italie Bulgarie R\u00e9publique de Cor\u00e9e Mexique Paraguay Belgique Irak URSS Hongrie France Canada Br\u00e9sil Espagne Irlande du Nord Alg\u00e9rie Danemark RFA Uruguay \u00c9cosse Maroc Angleterre Pologne Portugal (24 rows) Exercise 1.3 : Write a query which lists all matches as a pair of countries per match. Response paysl | paysv ---------------------|--------------------- Bulgarie | Italie Argentine | R\u00e9publique de Cor\u00e9e Italie | Argentine R\u00e9publique de Cor\u00e9e | Bulgarie R\u00e9publique de Cor\u00e9e | Italie Argentine | Bulgarie Belgique | Mexique Paraguay | Irak Mexique | Paraguay Irak | Belgique Irak | Mexique Paraguay | Belgique Canada | France URSS | Hongrie France | URSS Hongrie | Canada URSS | Canada Hongrie | France Espagne | Br\u00e9sil Alg\u00e9rie | Irlande du Nord Br\u00e9sil | Alg\u00e9rie Irlande du Nord | Espagne Irlande du Nord | Br\u00e9sil Alg\u00e9rie | Espagne Uruguay | RFA \u00c9cosse | Danemark Danemark | Uruguay RFA | \u00c9cosse \u00c9cosse | Uruguay Danemark | RFA Maroc | Pologne Portugal | Angleterre Angleterre | Maroc Pologne | Portugal Angleterre | Pologne Maroc | Portugal Br\u00e9sil | Pologne France | Italie Maroc | RFA Mexique | Bulgarie Argentine | Uruguay Angleterre | Paraguay URSS | Belgique Espagne | Danemark Br\u00e9sil | France RFA | Mexique Argentine | Angleterre Belgique | Espagne France | RFA Argentine | Belgique RFA | Argentine (51 rows) Exercise 1.4 : Write a query which lists the matches which took place on June 5, 1986. Response paysl | paysv ---------------------|----------- Italie | Argentine R\u00e9publique de Cor\u00e9e | Bulgarie France | URSS (3 rows) Exercise 1.5 : Write a query which lists the countries which France played against (hint, France could have played either side). Response pays --------- Br\u00e9sil Canada Hongrie Italie RFA URSS (6 rows) Exercise 1.6 : Write a query which returns the winner of the World Cup Response pays ----------- Argentine (1 row) Beer database \ud83d\udd17 We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts . $ createdb db-beer $ psql -d db-beer beer/create-tables-std.sql $ psql -d db-beer beer/insert.sql Exercise 2.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Frequente: ( buveur, bar ) Sert: ( bar, biere ) Aime: ( buveur, biere ) Write queries which respond to the following questions. Hint, understanding natural joins may help. Exercise 2.2 What is the list of bars which serve the beer that Martin likes? Response bar ------------------- Ancienne Belgique La Tireuse Le Filochard (3 rows) Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like? Response buveur -------- Bob David Emilie Martin (4 rows) Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like? Response buveur -------- Cecile Alice (2 rows) Complex queries - Mexico database \ud83d\udd17 Exercise 3.1 : Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information. Response paysl | paysv | buts | type | date ---------------------+---------------------+------+--------+------------ URSS | Belgique | 7 | 1/8 | 1986-06-15 France | Italie | 2 | 1/8 | 1986-06-17 Maroc | Pologne | 0 | Poule | 1986-06-02 RFA | Argentine | 5 | Finale | 1986-06-29 Br\u00e9sil | France | 2 | 1/4 | 1986-06-21 Italie | Argentine | 2 | Poule | 1986-06-05 Maroc | Portugal | 4 | Poule | 1986-06-11 Br\u00e9sil | Alg\u00e9rie | 1 | Poule | 1986-06-06 Paraguay | Belgique | 4 | Poule | 1986-06-11 Hongrie | France | 3 | Poule | 1986-06-09 Irak | Belgique | 3 | Poule | 1986-06-08 Danemark | RFA | 2 | Poule | 1986-06-13 Irlande du Nord | Espagne | 3 | Poule | 1986-06-07 Alg\u00e9rie | Irlande du Nord | 2 | Poule | 1986-06-03 RFA | Mexique | 0 | 1/4 | 1986-06-21 URSS | Hongrie | 6 | Poule | 1986-06-02 Mexique | Paraguay | 2 | Poule | 1986-06-07 Belgique | Espagne | 2 | 1/4 | 1986-06-22 Irak | Mexique | 1 | Poule | 1986-06-11 Espagne | Br\u00e9sil | 1 | Poule | 1986-06-01 Angleterre | Maroc | 0 | Poule | 1986-06-06 Irlande du Nord | Br\u00e9sil | 2 | Poule | 1986-06-12 Maroc | RFA | 1 | 1/8 | 1986-06-17 Belgique | Mexique | 3 | Poule | 1986-06-03 Bulgarie | Italie | 2 | Poule | 1986-05-31 \u00c9cosse | Uruguay | 0 | Poule | 1986-06-13 Alg\u00e9rie | Espagne | 3 | Poule | 1986-06-12 Argentine | Belgique | 2 | 1/2 | 1986-06-25 Br\u00e9sil | Pologne | 4 | 1/8 | 1986-06-16 Danemark | Uruguay | 7 | Poule | 1986-06-08 R\u00e9publique de Cor\u00e9e | Italie | 5 | Poule | 1986-06-10 Canada | France | 1 | Poule | 1986-06-01 Argentine | Uruguay | 1 | 1/8 | 1986-06-16 France | RFA | 2 | 1/2 | 1986-06-25 France | URSS | 2 | Poule | 1986-06-05 Uruguay | RFA | 2 | Poule | 1986-06-04 Angleterre | Pologne | 3 | Poule | 1986-06-11 Portugal | Angleterre | 1 | Poule | 1986-06-03 \u00c9cosse | Danemark | 1 | Poule | 1986-06-04 Angleterre | Paraguay | 3 | 1/8 | 1986-06-18 Hongrie | Canada | 2 | Poule | 1986-06-06 Argentine | R\u00e9publique de Cor\u00e9e | 4 | Poule | 1986-06-02 Pologne | Portugal | 1 | Poule | 1986-06-07 RFA | \u00c9cosse | 3 | Poule | 1986-06-08 Mexique | Bulgarie | 2 | 1/8 | 1986-06-15 URSS | Canada | 2 | Poule | 1986-06-09 Espagne | Danemark | 6 | 1/8 | 1986-06-18 Paraguay | Irak | 1 | Poule | 1986-06-04 Argentine | Bulgarie | 2 | Poule | 1986-06-10 Argentine | Angleterre | 3 | 1/4 | 1986-06-22 R\u00e9publique de Cor\u00e9e | Bulgarie | 2 | Poule | 1986-06-05 (51 rows) Exercise 3.2 : Write a query which caluculates the number of goals scored on average in all the matches of the French team. Response Moyenne buts -------------------- 2.0000000000000000 (1 row) Exercise 3.3 : Write a query which calculates the total number of goals scored only by the French team. Response buts ------ 8 (1 row) Exercise 3.4 : Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group. Response groupe | sum --------+----- A | 17 B | 14 C | 16 D | 12 E | 15 F | 9 (6 rows) Exercise 3.5 : Write a function vainquer which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs: SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8'); SELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4'); Response vainqueur ----------- Espagne (1 row) vainqueur ----------- Match nul (1 row) Exercise 3.6 : Write a function butsparequipe which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team. SELECT * FROM butsparequipe('France'); Response pays | total | moyenne --------+-------+-------------------- France | 8 | 1.3333333333333333 (1 row) Exercise 3.7 : Using the butsparequipe function, write a query which lists all countries and the points they scored. Response pays | total ---------------------+------- Argentine | 14 Italie | 5 Bulgarie | 2 R\u00e9publique de Cor\u00e9e | 4 Mexique | 6 Paraguay | 4 Belgique | 10 Irak | 1 URSS | 12 Hongrie | 2 France | 8 Canada | 0 Br\u00e9sil | 9 Espagne | 11 Irlande du Nord | 2 Alg\u00e9rie | 1 Danemark | 10 RFA | 8 Uruguay | 2 \u00c9cosse | 1 Maroc | 3 Angleterre | 7 Pologne | 1 Portugal | 2 (24 rows) Exercise 3.8 : Using the butsparequipe function, write a query which shows the country which scored the most points and the number of points they scored. Response pays | total -----------+------- Argentine | 14 (1 row) Pull the trigger \ud83d\udd17 In this exercise, we're going to create a TRIGGER , a mechanism which allows for automatically executing actions when an event occurs. Dans cet exercice, nous allons nous int\u00e9resser \u00e0 la cr\u00e9ation d\u2019untriggersur une base de donn\u00e9es tr\u00e8s simple.Rappelons qu\u2019untriggerest un m\u00e9canisme permettant d\u2019ex\u00e9cuter des actions lors de l\u2019arriv\u00e9e d\u2019un \u00e9v\u00e9nementparticulier (mise-\u00e0-jour d\u2019une relation, insertion d\u2019un nouveau tuple etc.).(a) cr\u00e9er une tablerel(nom, valeur)o\u00f9nomest une cha\u00eene de caract\u00e8res etvaleurun entier. On choisiranomcomme cl\u00e9 primaire.(b) ins\u00e9rer 5 tuples dans la table.(c) onsouhaitemaintenantquelorsdel\u2019insertiondenouveauxtupleslamoyenneactuelledesvaleurscontenuesdans la relation ne puisse pas d\u00e9cro\u00eetre Create the db-trigger database. $ createdb db-trigger Exercise 4.1 : Create a table rel(nom, value) where nom is a string of characters and value is an integer. nom will be the primary key Solution CREATE TABLE IF NOT EXISTS rel ( nom VARCHAR(20), valeur INTEGER, PRIMARY KEY (nom) ); Exercise 4.2 : Add 5 tuples into the table Solution INSERT INTO rel VALUES ('Alice', 10), ('Bob', 5), ('Carl', 20), ('Denise', 11), ('Esther', 6); Exercise 4.3 : Write a trigger such that, when adding new tuples, the average value of val cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised. The following insertion should work: INSERT INTO rel VALUES ('Fab', 15); SELECT * FROM rel; As we can see, the (Fab, 15) tuple was added: nom | valeur --------+-------- Alice | 10 Bob | 5 Carl | 20 Denise | 11 Esther | 6 Fab | 15 (6 rows) However, the following insertion should give an exception: INSERT INTO rel VALUES ('Guy', 2); Solution CREATE OR REPLACE FUNCTION verifier_moyenne() RETURNS trigger AS $verifier_moyenne$ DECLARE moyenne FLOAT; nb INTEGER; BEGIN moyenne := AVG(valeur) FROM rel; nb := COUNT(*) FROM rel; IF ((nb * moyenne + NEW.valeur) / (nb + 1)) < moyenne THEN RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!'; END IF; RETURN NEW; END; $verifier_moyenne$ LANGUAGE plpgsql; CREATE TRIGGER VerificationMoyenne BEFORE INSERT ON rel FOR EACH ROW EXECUTE PROCEDURE verifier_moyenne();","title":"PostgreSQL"},{"location":"3_2_postgres.html#postgesql","text":"In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL. Based on the TP by Christophe Garion, CC BY-NC-SA 2015.","title":"PostgeSQL"},{"location":"3_2_postgres.html#setup","text":"For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the postgresql service running on Linux. Installation on Arch Linux Installation on Ubuntu Installation on Windows Additionally, add your login user as a postgresql superuser to enable database creation. Once you've installated and configured PostgreSQL, create the first exercise database: $ createdb db-mexico86 Confirm with pgAdmin that your database db-mexico86 was created. You may need to create a server \"local\" with the host address 127.0.0.1 .","title":"Setup"},{"location":"3_2_postgres.html#mexico86-database-simple-queries","text":"This database contains data from the 1986 football World Cup. Download the creation and insertion scripts and run the scripts in the mexico folder. $ psql -d db-mexico86 mexico86/create-tables-std.sql Exercise 1.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Pays: ( nom , groupe) Typematch: ( type ) Match: ( paysl, paysv , butsl, butsv, type , date) You should be able to make queries now. You can either use PostgreSQL in interactive mode by running $ psql -d db-mexico86 or write your solutions in an SQL file and run the file: $ echo \"SELECT groupe FROM pays;\" > a.sql $ psql -d db-mexico86 -f a.sql You can also use the Query Editor in pgAdmin for a graphical interface. Exercise 1.2 : Write a query which lists the countries participating in the World Cup. Response nom --------------------- Argentine Italie Bulgarie R\u00e9publique de Cor\u00e9e Mexique Paraguay Belgique Irak URSS Hongrie France Canada Br\u00e9sil Espagne Irlande du Nord Alg\u00e9rie Danemark RFA Uruguay \u00c9cosse Maroc Angleterre Pologne Portugal (24 rows) Exercise 1.3 : Write a query which lists all matches as a pair of countries per match. Response paysl | paysv ---------------------|--------------------- Bulgarie | Italie Argentine | R\u00e9publique de Cor\u00e9e Italie | Argentine R\u00e9publique de Cor\u00e9e | Bulgarie R\u00e9publique de Cor\u00e9e | Italie Argentine | Bulgarie Belgique | Mexique Paraguay | Irak Mexique | Paraguay Irak | Belgique Irak | Mexique Paraguay | Belgique Canada | France URSS | Hongrie France | URSS Hongrie | Canada URSS | Canada Hongrie | France Espagne | Br\u00e9sil Alg\u00e9rie | Irlande du Nord Br\u00e9sil | Alg\u00e9rie Irlande du Nord | Espagne Irlande du Nord | Br\u00e9sil Alg\u00e9rie | Espagne Uruguay | RFA \u00c9cosse | Danemark Danemark | Uruguay RFA | \u00c9cosse \u00c9cosse | Uruguay Danemark | RFA Maroc | Pologne Portugal | Angleterre Angleterre | Maroc Pologne | Portugal Angleterre | Pologne Maroc | Portugal Br\u00e9sil | Pologne France | Italie Maroc | RFA Mexique | Bulgarie Argentine | Uruguay Angleterre | Paraguay URSS | Belgique Espagne | Danemark Br\u00e9sil | France RFA | Mexique Argentine | Angleterre Belgique | Espagne France | RFA Argentine | Belgique RFA | Argentine (51 rows) Exercise 1.4 : Write a query which lists the matches which took place on June 5, 1986. Response paysl | paysv ---------------------|----------- Italie | Argentine R\u00e9publique de Cor\u00e9e | Bulgarie France | URSS (3 rows) Exercise 1.5 : Write a query which lists the countries which France played against (hint, France could have played either side). Response pays --------- Br\u00e9sil Canada Hongrie Italie RFA URSS (6 rows) Exercise 1.6 : Write a query which returns the winner of the World Cup Response pays ----------- Argentine (1 row)","title":"Mexico86 database - simple queries"},{"location":"3_2_postgres.html#beer-database","text":"We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts . $ createdb db-beer $ psql -d db-beer beer/create-tables-std.sql $ psql -d db-beer beer/insert.sql Exercise 2.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Frequente: ( buveur, bar ) Sert: ( bar, biere ) Aime: ( buveur, biere ) Write queries which respond to the following questions. Hint, understanding natural joins may help. Exercise 2.2 What is the list of bars which serve the beer that Martin likes? Response bar ------------------- Ancienne Belgique La Tireuse Le Filochard (3 rows) Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like? Response buveur -------- Bob David Emilie Martin (4 rows) Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like? Response buveur -------- Cecile Alice (2 rows)","title":"Beer database"},{"location":"3_2_postgres.html#complex-queries-mexico-database","text":"Exercise 3.1 : Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information. Response paysl | paysv | buts | type | date ---------------------+---------------------+------+--------+------------ URSS | Belgique | 7 | 1/8 | 1986-06-15 France | Italie | 2 | 1/8 | 1986-06-17 Maroc | Pologne | 0 | Poule | 1986-06-02 RFA | Argentine | 5 | Finale | 1986-06-29 Br\u00e9sil | France | 2 | 1/4 | 1986-06-21 Italie | Argentine | 2 | Poule | 1986-06-05 Maroc | Portugal | 4 | Poule | 1986-06-11 Br\u00e9sil | Alg\u00e9rie | 1 | Poule | 1986-06-06 Paraguay | Belgique | 4 | Poule | 1986-06-11 Hongrie | France | 3 | Poule | 1986-06-09 Irak | Belgique | 3 | Poule | 1986-06-08 Danemark | RFA | 2 | Poule | 1986-06-13 Irlande du Nord | Espagne | 3 | Poule | 1986-06-07 Alg\u00e9rie | Irlande du Nord | 2 | Poule | 1986-06-03 RFA | Mexique | 0 | 1/4 | 1986-06-21 URSS | Hongrie | 6 | Poule | 1986-06-02 Mexique | Paraguay | 2 | Poule | 1986-06-07 Belgique | Espagne | 2 | 1/4 | 1986-06-22 Irak | Mexique | 1 | Poule | 1986-06-11 Espagne | Br\u00e9sil | 1 | Poule | 1986-06-01 Angleterre | Maroc | 0 | Poule | 1986-06-06 Irlande du Nord | Br\u00e9sil | 2 | Poule | 1986-06-12 Maroc | RFA | 1 | 1/8 | 1986-06-17 Belgique | Mexique | 3 | Poule | 1986-06-03 Bulgarie | Italie | 2 | Poule | 1986-05-31 \u00c9cosse | Uruguay | 0 | Poule | 1986-06-13 Alg\u00e9rie | Espagne | 3 | Poule | 1986-06-12 Argentine | Belgique | 2 | 1/2 | 1986-06-25 Br\u00e9sil | Pologne | 4 | 1/8 | 1986-06-16 Danemark | Uruguay | 7 | Poule | 1986-06-08 R\u00e9publique de Cor\u00e9e | Italie | 5 | Poule | 1986-06-10 Canada | France | 1 | Poule | 1986-06-01 Argentine | Uruguay | 1 | 1/8 | 1986-06-16 France | RFA | 2 | 1/2 | 1986-06-25 France | URSS | 2 | Poule | 1986-06-05 Uruguay | RFA | 2 | Poule | 1986-06-04 Angleterre | Pologne | 3 | Poule | 1986-06-11 Portugal | Angleterre | 1 | Poule | 1986-06-03 \u00c9cosse | Danemark | 1 | Poule | 1986-06-04 Angleterre | Paraguay | 3 | 1/8 | 1986-06-18 Hongrie | Canada | 2 | Poule | 1986-06-06 Argentine | R\u00e9publique de Cor\u00e9e | 4 | Poule | 1986-06-02 Pologne | Portugal | 1 | Poule | 1986-06-07 RFA | \u00c9cosse | 3 | Poule | 1986-06-08 Mexique | Bulgarie | 2 | 1/8 | 1986-06-15 URSS | Canada | 2 | Poule | 1986-06-09 Espagne | Danemark | 6 | 1/8 | 1986-06-18 Paraguay | Irak | 1 | Poule | 1986-06-04 Argentine | Bulgarie | 2 | Poule | 1986-06-10 Argentine | Angleterre | 3 | 1/4 | 1986-06-22 R\u00e9publique de Cor\u00e9e | Bulgarie | 2 | Poule | 1986-06-05 (51 rows) Exercise 3.2 : Write a query which caluculates the number of goals scored on average in all the matches of the French team. Response Moyenne buts -------------------- 2.0000000000000000 (1 row) Exercise 3.3 : Write a query which calculates the total number of goals scored only by the French team. Response buts ------ 8 (1 row) Exercise 3.4 : Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group. Response groupe | sum --------+----- A | 17 B | 14 C | 16 D | 12 E | 15 F | 9 (6 rows) Exercise 3.5 : Write a function vainquer which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs: SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8'); SELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4'); Response vainqueur ----------- Espagne (1 row) vainqueur ----------- Match nul (1 row) Exercise 3.6 : Write a function butsparequipe which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team. SELECT * FROM butsparequipe('France'); Response pays | total | moyenne --------+-------+-------------------- France | 8 | 1.3333333333333333 (1 row) Exercise 3.7 : Using the butsparequipe function, write a query which lists all countries and the points they scored. Response pays | total ---------------------+------- Argentine | 14 Italie | 5 Bulgarie | 2 R\u00e9publique de Cor\u00e9e | 4 Mexique | 6 Paraguay | 4 Belgique | 10 Irak | 1 URSS | 12 Hongrie | 2 France | 8 Canada | 0 Br\u00e9sil | 9 Espagne | 11 Irlande du Nord | 2 Alg\u00e9rie | 1 Danemark | 10 RFA | 8 Uruguay | 2 \u00c9cosse | 1 Maroc | 3 Angleterre | 7 Pologne | 1 Portugal | 2 (24 rows) Exercise 3.8 : Using the butsparequipe function, write a query which shows the country which scored the most points and the number of points they scored. Response pays | total -----------+------- Argentine | 14 (1 row)","title":"Complex queries - Mexico database"},{"location":"3_2_postgres.html#pull-the-trigger","text":"In this exercise, we're going to create a TRIGGER , a mechanism which allows for automatically executing actions when an event occurs. Dans cet exercice, nous allons nous int\u00e9resser \u00e0 la cr\u00e9ation d\u2019untriggersur une base de donn\u00e9es tr\u00e8s simple.Rappelons qu\u2019untriggerest un m\u00e9canisme permettant d\u2019ex\u00e9cuter des actions lors de l\u2019arriv\u00e9e d\u2019un \u00e9v\u00e9nementparticulier (mise-\u00e0-jour d\u2019une relation, insertion d\u2019un nouveau tuple etc.).(a) cr\u00e9er une tablerel(nom, valeur)o\u00f9nomest une cha\u00eene de caract\u00e8res etvaleurun entier. On choisiranomcomme cl\u00e9 primaire.(b) ins\u00e9rer 5 tuples dans la table.(c) onsouhaitemaintenantquelorsdel\u2019insertiondenouveauxtupleslamoyenneactuelledesvaleurscontenuesdans la relation ne puisse pas d\u00e9cro\u00eetre Create the db-trigger database. $ createdb db-trigger Exercise 4.1 : Create a table rel(nom, value) where nom is a string of characters and value is an integer. nom will be the primary key Solution CREATE TABLE IF NOT EXISTS rel ( nom VARCHAR(20), valeur INTEGER, PRIMARY KEY (nom) ); Exercise 4.2 : Add 5 tuples into the table Solution INSERT INTO rel VALUES ('Alice', 10), ('Bob', 5), ('Carl', 20), ('Denise', 11), ('Esther', 6); Exercise 4.3 : Write a trigger such that, when adding new tuples, the average value of val cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised. The following insertion should work: INSERT INTO rel VALUES ('Fab', 15); SELECT * FROM rel; As we can see, the (Fab, 15) tuple was added: nom | valeur --------+-------- Alice | 10 Bob | 5 Carl | 20 Denise | 11 Esther | 6 Fab | 15 (6 rows) However, the following insertion should give an exception: INSERT INTO rel VALUES ('Guy', 2); Solution CREATE OR REPLACE FUNCTION verifier_moyenne() RETURNS trigger AS $verifier_moyenne$ DECLARE moyenne FLOAT; nb INTEGER; BEGIN moyenne := AVG(valeur) FROM rel; nb := COUNT(*) FROM rel; IF ((nb * moyenne + NEW.valeur) / (nb + 1)) < moyenne THEN RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!'; END IF; RETURN NEW; END; $verifier_moyenne$ LANGUAGE plpgsql; CREATE TRIGGER VerificationMoyenne BEFORE INSERT ON rel FOR EACH ROW EXECUTE PROCEDURE verifier_moyenne();","title":"Pull the trigger"},{"location":"3_3_project.html","text":"Databases Project \ud83d\udd17 The evaluation of the databases class is a presentation of a specific database. You can work in teams of 4 of your choosing. You are working in a company which is looking to replace a relational DBMS currently in use. Each team should present a feasability study of a specific DBMS, showing its advantages, disadvantages, and use cases. Two different teams will work on each DBMS. We will organize the subjects on 10/02/2021, which is a work class dedicated to the project. The DBMSs are: InfluxDB , a distributed DBMS optimized for timeseries data Riak , a key/value DBMS inspired Amazon BD. HBase , a column-based DBMS inspired by BigTable MongoDB , a DBMS for documents used, for example, by CERN CouchDB , a DBMS for documents used, for example, by Amadeus Neo4j , a DBMS for graph databases Redis , a very efficient key/value DBMS Each team should: install their DBMS test the DBMS on a relevant database (datasets from Google , and kaggle ) compare their DBMS with a relational database system prepare a presentation of their DBMS and example database which presents convincing argument for using this DBMS. Presentations will take place on March 8th. You should upload your presentation materials to the LMS before midnight on March 7th.","title":"Project"},{"location":"3_3_project.html#databases-project","text":"The evaluation of the databases class is a presentation of a specific database. You can work in teams of 4 of your choosing. You are working in a company which is looking to replace a relational DBMS currently in use. Each team should present a feasability study of a specific DBMS, showing its advantages, disadvantages, and use cases. Two different teams will work on each DBMS. We will organize the subjects on 10/02/2021, which is a work class dedicated to the project. The DBMSs are: InfluxDB , a distributed DBMS optimized for timeseries data Riak , a key/value DBMS inspired Amazon BD. HBase , a column-based DBMS inspired by BigTable MongoDB , a DBMS for documents used, for example, by CERN CouchDB , a DBMS for documents used, for example, by Amadeus Neo4j , a DBMS for graph databases Redis , a very efficient key/value DBMS Each team should: install their DBMS test the DBMS on a relevant database (datasets from Google , and kaggle ) compare their DBMS with a relational database system prepare a presentation of their DBMS and example database which presents convincing argument for using this DBMS. Presentations will take place on March 8th. You should upload your presentation materials to the LMS before midnight on March 7th.","title":"Databases Project"}]}
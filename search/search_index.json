{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Tools of Big Data \ud83d\udd17 The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data. The objective of this class is that students gain: First hand experience with and detailed knowledge of computing models, notably cloud computing An understanding of distributed programming models and data distribution Broad knowledge of many databases and their respective strengths As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes. Class structure \ud83d\udd17 The class is structured in three parts: Data storage \ud83d\udd17 In the first 10 hours of the course, state-of-the-art databases will be presented. Students will install and demonstrate the advantages of different databases to their peers as a graded project. Data computation \ud83d\udd17 20 hours on the computing platforms used in the data ecosystem. We will briefly cover cluster computing and then go in depth on cloud computing, using Google Cloud Platform as an example. Finally, a class on GPU computing will be given in coordination with the deep learning section of the AML class. Data distribution \ud83d\udd17 20 hours on the distribution of data, with a focus on distributed programming models. We will introduce functional programming and MapReduce, then use these concepts in a practical session on Spark. Finally, students will do a graded exercise with Dask. Class schedule \ud83d\udd17 Introduction Readings Introduction to Big Data 2h 27/09/2021 Global Datasphere Data Storage Databases overview 2h 27/09/2021 Databases and SQL PostgeSQL TP 3h 29/09/2021 PostgeSQL Databases Project 3h 06/10/2021 Project presentations 3h 02/11/2021 Data Computation Readings Cloud Computing & Google Cloud Platform 3h 18/01/2022 Readings Containers 3h 19/01/2022 Readings Cloud Compute BE 3h 25/01/2022 Deployement, BE 2, Orchestration 3h 25/01/2022 Readings GPU computing 3h 3h 01/02/2022 02/02/2022 GPGPU TP Quiz and Recap 2h 02/02/2022 Data Distribution Readings Hadoop and MapReduce 3h 08/02/2022 MapReduce Spark 4h 08/02/2022 Spark PySpark Dask on Kubernetes 3h 14/02/2022 Dask documentation Dask project 6h 16/02/2022 Dask","title":"Syllabus"},{"location":"index.html#tools-of-big-data","text":"The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data. The objective of this class is that students gain: First hand experience with and detailed knowledge of computing models, notably cloud computing An understanding of distributed programming models and data distribution Broad knowledge of many databases and their respective strengths As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes.","title":"Tools of Big Data"},{"location":"index.html#class-structure","text":"The class is structured in three parts:","title":"Class structure"},{"location":"index.html#data-storage","text":"In the first 10 hours of the course, state-of-the-art databases will be presented. Students will install and demonstrate the advantages of different databases to their peers as a graded project.","title":"Data storage"},{"location":"index.html#data-computation","text":"20 hours on the computing platforms used in the data ecosystem. We will briefly cover cluster computing and then go in depth on cloud computing, using Google Cloud Platform as an example. Finally, a class on GPU computing will be given in coordination with the deep learning section of the AML class.","title":"Data computation"},{"location":"index.html#data-distribution","text":"20 hours on the distribution of data, with a focus on distributed programming models. We will introduce functional programming and MapReduce, then use these concepts in a practical session on Spark. Finally, students will do a graded exercise with Dask.","title":"Data distribution"},{"location":"index.html#class-schedule","text":"Introduction Readings Introduction to Big Data 2h 27/09/2021 Global Datasphere Data Storage Databases overview 2h 27/09/2021 Databases and SQL PostgeSQL TP 3h 29/09/2021 PostgeSQL Databases Project 3h 06/10/2021 Project presentations 3h 02/11/2021 Data Computation Readings Cloud Computing & Google Cloud Platform 3h 18/01/2022 Readings Containers 3h 19/01/2022 Readings Cloud Compute BE 3h 25/01/2022 Deployement, BE 2, Orchestration 3h 25/01/2022 Readings GPU computing 3h 3h 01/02/2022 02/02/2022 GPGPU TP Quiz and Recap 2h 02/02/2022 Data Distribution Readings Hadoop and MapReduce 3h 08/02/2022 MapReduce Spark 4h 08/02/2022 Spark PySpark Dask on Kubernetes 3h 14/02/2022 Dask documentation Dask project 6h 16/02/2022 Dask","title":"Class schedule"},{"location":"0_0_intro.html","text":"Tools of Big Data - Overview \ud83d\udd17 Link to slides","title":"Presentation"},{"location":"0_0_intro.html#tools-of-big-data-overview","text":"Link to slides","title":"Tools of Big Data - Overview"},{"location":"0_1_databases.html","text":"Data Storage \ud83d\udd17 In this module on databases, database management systems will be covered, from basics and SQL to state-of-the-art DBMSs. Students will install and demonstrate the advantages of different DBMSs to their peers as a graded project. In this first class, we introduce the basics of database management systems and cover SQL in detail. We present the concept of ACID (atomicity, consistency, isolation, durability) in SQL databases. Slides In the upcoming classes, students will need to install DBMSs on their local machines. For the next class , students should install PostgreSQL. Additional Resources \ud83d\udd17 Databases introduction (fr) : includes an explanation of DBMSs, SQL, and PostgreSQL. A comprehensive overview of database systems (en)","title":"Introduction"},{"location":"0_1_databases.html#data-storage","text":"In this module on databases, database management systems will be covered, from basics and SQL to state-of-the-art DBMSs. Students will install and demonstrate the advantages of different DBMSs to their peers as a graded project. In this first class, we introduce the basics of database management systems and cover SQL in detail. We present the concept of ACID (atomicity, consistency, isolation, durability) in SQL databases. Slides In the upcoming classes, students will need to install DBMSs on their local machines. For the next class , students should install PostgreSQL.","title":"Data Storage"},{"location":"0_1_databases.html#additional-resources","text":"Databases introduction (fr) : includes an explanation of DBMSs, SQL, and PostgreSQL. A comprehensive overview of database systems (en)","title":"Additional Resources"},{"location":"0_2_postgres.html","text":"PostgeSQL \ud83d\udd17 In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL. Based on the TP by Christophe Garion, CC BY-NC-SA 2015. Setup \ud83d\udd17 Before class, please install PostgreSQL and pgAdmin. PostgreSQL installation \ud83d\udd17 For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the postgresql service running. Installation on Ubuntu Installation on Mac OS Installation on Arch Linux Installation on Windows Subsystem for Linux Installation on Windows (and add the PostgreSQL binaries to your path ) Additionally, add your login user as a postgresql superuser to enable database creation with your user: # bash shell in Linux or OSX $ sudo su -l postgres [ postgres ] $ createuser --interactive pgAdmin \ud83d\udd17 You can do all exercises directly through the psql shell for this class. However, it is useful to have a graphical confirmation of the database configuration. pgAdmin is one of many front-ends for Postgres. Install it by following the instructions on the pgAdmin site. Setup - database creation \ud83d\udd17 Once you've installated and configured PostgreSQL, create the first exercise database: # bash shell in Linux or OSX or windows powershell $ createdb db-mexico86 you can also do this through an SQL shell: # SQL shell postgres =# CREATE DATABASE \"db-mexico86\" ; Confirm with pgAdmin that your database db-mexico86 was created. If you don't have any servers, create one by right-clicking. The host address is 127.0.0.1 and the maintenance database and username should be postgres . In pgAdmin, if you are asked for a password and don't know what your password is, you can reset the password of the postgres user: change password postgres =# ALTER USER postgres WITH PASSWORD \"newpassword\" ; Mexico86 database - simple queries \ud83d\udd17 This database contains data from the 1986 football World Cup. You can download the database creation script individually: $ wget https://raw.githubusercontent.com/SupaeroDataScience/OBD/master/scripts/mexico86/create-tables-std.sql Or git clone the class repository and navigate to the creation and insertion scripts . Once you have the scripts, run the database creation script in the mexico folder. # bash shell in Linux or OSX, or windows powershell $ psql -d db-mexico86 -f mexico86/create-tables-std.sql If that doesn't work, you can copy the script into the Query Tool in pgAdmin. Exercise 1.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Pays: ( nom , groupe) Typematch: ( type ) Match: ( paysl, paysv , butsl, butsv, type , date) You should be able to make queries now. You can either use PostgreSQL in interactive mode by running $ psql -d db-mexico86 or write your solutions in an SQL file and run the file: $ echo \"SELECT groupe FROM pays;\" > a.sql $ psql -d db-mexico86 -f a.sql You can also use the Query Editor in pgAdmin for a graphical interface. Exercise 1.2 : Write a query which lists the countries participating in the World Cup. Response nom --------------------- Argentine Italie Bulgarie Cor\u00e9e Mexique Paraguay Belgique Irak URSS Hongrie France Canada Br\u00e9sil Espagne Irlande du Nord Alg\u00e9rie Danemark RFA Uruguay \u00c9cosse Maroc Angleterre Pologne Portugal (24 rows) Exercise 1.3 : Write a query which lists all matches as a pair of countries per match. Response paysl | paysv ---------------------|--------------------- Bulgarie | Italie Argentine | Cor\u00e9e Italie | Argentine Cor\u00e9e | Bulgarie Cor\u00e9e | Italie Argentine | Bulgarie Belgique | Mexique Paraguay | Irak Mexique | Paraguay Irak | Belgique Irak | Mexique Paraguay | Belgique Canada | France URSS | Hongrie France | URSS Hongrie | Canada URSS | Canada Hongrie | France Espagne | Br\u00e9sil Alg\u00e9rie | Irlande du Nord Br\u00e9sil | Alg\u00e9rie Irlande du Nord | Espagne Irlande du Nord | Br\u00e9sil Alg\u00e9rie | Espagne Uruguay | RFA \u00c9cosse | Danemark Danemark | Uruguay RFA | \u00c9cosse \u00c9cosse | Uruguay Danemark | RFA Maroc | Pologne Portugal | Angleterre Angleterre | Maroc Pologne | Portugal Angleterre | Pologne Maroc | Portugal Br\u00e9sil | Pologne France | Italie Maroc | RFA Mexique | Bulgarie Argentine | Uruguay Angleterre | Paraguay URSS | Belgique Espagne | Danemark Br\u00e9sil | France RFA | Mexique Argentine | Angleterre Belgique | Espagne France | RFA Argentine | Belgique RFA | Argentine (51 rows) Exercise 1.4 : Write a query which lists the matches which took place on June 5, 1986. Response paysl | paysv ---------------------|----------- Italie | Argentine Cor\u00e9e | Bulgarie France | URSS (3 rows) Exercise 1.5 : Write a query which lists the countries which France played against (hint, France could have played either side). Response pays --------- Br\u00e9sil Canada Hongrie Italie RFA URSS (6 rows) Exercise 1.6 : Write a query which returns the winner of the World Cup Response pays ----------- Argentine (1 row) Beer database \ud83d\udd17 We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts . $ createdb db-beer $ psql -d db-beer -f beer/create-tables-std.sql $ psql -d db-beer -f beer/insert.sql Exercise 2.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Frequente: ( buveur, bar ) Sert: ( bar, biere ) Aime: ( buveur, biere ) Write queries which respond to the following questions. Hint, understanding natural joins may help. Exercise 2.2 What is the list of bars which serve the beer that Martin likes? Response bar ------------------- Ancienne Belgique La Tireuse Le Filochard (3 rows) Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like? Response buveur -------- Bob David Emilie Martin (4 rows) Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like? Response buveur -------- Cecile Alice (2 rows) Complex queries - Mexico database \ud83d\udd17 Exercise 3.1 : Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information. Response paysl | paysv | buts | type | date ---------------------+---------------------+------+--------+------------ URSS | Belgique | 7 | 1/8 | 1986-06-15 France | Italie | 2 | 1/8 | 1986-06-17 Maroc | Pologne | 0 | Poule | 1986-06-02 RFA | Argentine | 5 | Finale | 1986-06-29 Br\u00e9sil | France | 2 | 1/4 | 1986-06-21 Italie | Argentine | 2 | Poule | 1986-06-05 Maroc | Portugal | 4 | Poule | 1986-06-11 Br\u00e9sil | Alg\u00e9rie | 1 | Poule | 1986-06-06 Paraguay | Belgique | 4 | Poule | 1986-06-11 Hongrie | France | 3 | Poule | 1986-06-09 Irak | Belgique | 3 | Poule | 1986-06-08 Danemark | RFA | 2 | Poule | 1986-06-13 Irlande du Nord | Espagne | 3 | Poule | 1986-06-07 Alg\u00e9rie | Irlande du Nord | 2 | Poule | 1986-06-03 RFA | Mexique | 0 | 1/4 | 1986-06-21 URSS | Hongrie | 6 | Poule | 1986-06-02 Mexique | Paraguay | 2 | Poule | 1986-06-07 Belgique | Espagne | 2 | 1/4 | 1986-06-22 Irak | Mexique | 1 | Poule | 1986-06-11 Espagne | Br\u00e9sil | 1 | Poule | 1986-06-01 Angleterre | Maroc | 0 | Poule | 1986-06-06 Irlande du Nord | Br\u00e9sil | 2 | Poule | 1986-06-12 Maroc | RFA | 1 | 1/8 | 1986-06-17 Belgique | Mexique | 3 | Poule | 1986-06-03 Bulgarie | Italie | 2 | Poule | 1986-05-31 \u00c9cosse | Uruguay | 0 | Poule | 1986-06-13 Alg\u00e9rie | Espagne | 3 | Poule | 1986-06-12 Argentine | Belgique | 2 | 1/2 | 1986-06-25 Br\u00e9sil | Pologne | 4 | 1/8 | 1986-06-16 Danemark | Uruguay | 7 | Poule | 1986-06-08 Cor\u00e9e | Italie | 5 | Poule | 1986-06-10 Canada | France | 1 | Poule | 1986-06-01 Argentine | Uruguay | 1 | 1/8 | 1986-06-16 France | RFA | 2 | 1/2 | 1986-06-25 France | URSS | 2 | Poule | 1986-06-05 Uruguay | RFA | 2 | Poule | 1986-06-04 Angleterre | Pologne | 3 | Poule | 1986-06-11 Portugal | Angleterre | 1 | Poule | 1986-06-03 \u00c9cosse | Danemark | 1 | Poule | 1986-06-04 Angleterre | Paraguay | 3 | 1/8 | 1986-06-18 Hongrie | Canada | 2 | Poule | 1986-06-06 Argentine | Cor\u00e9e | 4 | Poule | 1986-06-02 Pologne | Portugal | 1 | Poule | 1986-06-07 RFA | \u00c9cosse | 3 | Poule | 1986-06-08 Mexique | Bulgarie | 2 | 1/8 | 1986-06-15 URSS | Canada | 2 | Poule | 1986-06-09 Espagne | Danemark | 6 | 1/8 | 1986-06-18 Paraguay | Irak | 1 | Poule | 1986-06-04 Argentine | Bulgarie | 2 | Poule | 1986-06-10 Argentine | Angleterre | 3 | 1/4 | 1986-06-22 Cor\u00e9e | Bulgarie | 2 | Poule | 1986-06-05 (51 rows) Exercise 3.2 : Write a query which caluculates the number of goals scored on average in all the matches of the French team. Response Moyenne buts -------------------- 2.0000000000000000 (1 row) Exercise 3.3 : Write a query which calculates the total number of goals scored only by the French team. Response buts ------ 8 (1 row) Exercise 3.4 : Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group. Response groupe | sum --------+----- A | 17 B | 14 C | 16 D | 12 E | 15 F | 9 (6 rows) Exercise 3.5 : Write a function vainquer which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs: SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8'); SELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4'); Response vainqueur ----------- Espagne (1 row) vainqueur ----------- Match nul (1 row) Exercise 3.6 : Write a function butsparequipe which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team. SELECT * FROM butsparequipe('France'); Response pays | total | moyenne --------+-------+-------------------- France | 8 | 1.3333333333333333 (1 row) Exercise 3.7 : Using the butsparequipe function, write a query which lists all countries and the points they scored. Response pays | total ---------------------+------- Argentine | 14 Italie | 5 Bulgarie | 2 Cor\u00e9e | 4 Mexique | 6 Paraguay | 4 Belgique | 10 Irak | 1 URSS | 12 Hongrie | 2 France | 8 Canada | 0 Br\u00e9sil | 9 Espagne | 11 Irlande du Nord | 2 Alg\u00e9rie | 1 Danemark | 10 RFA | 8 Uruguay | 2 \u00c9cosse | 1 Maroc | 3 Angleterre | 7 Pologne | 1 Portugal | 2 (24 rows) Exercise 3.8 : Using the butsparequipe function, write a query which shows the country which scored the most points and the number of points they scored. Response pays | total -----------+------- Argentine | 14 (1 row) Pull the trigger \ud83d\udd17 In this exercise, we're going to create a TRIGGER , a mechanism which allows for automatically executing actions when an event occurs. Create the db-trigger database. $ createdb db-trigger Exercise 4.1 : Create a table rel(nom, value) where nom is a string of characters and value is an integer. nom will be the primary key Solution CREATE TABLE IF NOT EXISTS rel ( nom VARCHAR(20), valeur INTEGER, PRIMARY KEY (nom) ); Exercise 4.2 : Add 5 tuples into the table Solution INSERT INTO rel VALUES ('Alice', 10), ('Bob', 5), ('Carl', 20), ('Denise', 11), ('Esther', 6); Exercise 4.3 : Write a trigger such that, when adding new tuples, the average value of val cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised. The following insertion should work: INSERT INTO rel VALUES ('Fab', 15); SELECT * FROM rel; As we can see, the (Fab, 15) tuple was added: nom | valeur --------+-------- Alice | 10 Bob | 5 Carl | 20 Denise | 11 Esther | 6 Fab | 15 (6 rows) However, the following insertion should give an exception: INSERT INTO rel VALUES ('Guy', 2); Solution CREATE OR REPLACE FUNCTION verifier_moyenne() RETURNS trigger AS $verifier_moyenne$ DECLARE moyenne FLOAT; nb INTEGER; BEGIN moyenne := AVG(valeur) FROM rel; nb := COUNT(*) FROM rel; IF ((nb * moyenne + NEW.valeur) / (nb + 1)) < moyenne THEN RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!'; END IF; RETURN NEW; END; $verifier_moyenne$ LANGUAGE plpgsql; CREATE TRIGGER VerificationMoyenne BEFORE INSERT ON rel FOR EACH ROW EXECUTE PROCEDURE verifier_moyenne();","title":"PostgreSQL"},{"location":"0_2_postgres.html#postgesql","text":"In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL. Based on the TP by Christophe Garion, CC BY-NC-SA 2015.","title":"PostgeSQL"},{"location":"0_2_postgres.html#setup","text":"Before class, please install PostgreSQL and pgAdmin.","title":"Setup"},{"location":"0_2_postgres.html#postgresql-installation","text":"For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the postgresql service running. Installation on Ubuntu Installation on Mac OS Installation on Arch Linux Installation on Windows Subsystem for Linux Installation on Windows (and add the PostgreSQL binaries to your path ) Additionally, add your login user as a postgresql superuser to enable database creation with your user: # bash shell in Linux or OSX $ sudo su -l postgres [ postgres ] $ createuser --interactive","title":"PostgreSQL installation"},{"location":"0_2_postgres.html#pgadmin","text":"You can do all exercises directly through the psql shell for this class. However, it is useful to have a graphical confirmation of the database configuration. pgAdmin is one of many front-ends for Postgres. Install it by following the instructions on the pgAdmin site.","title":"pgAdmin"},{"location":"0_2_postgres.html#setup-database-creation","text":"Once you've installated and configured PostgreSQL, create the first exercise database: # bash shell in Linux or OSX or windows powershell $ createdb db-mexico86 you can also do this through an SQL shell: # SQL shell postgres =# CREATE DATABASE \"db-mexico86\" ; Confirm with pgAdmin that your database db-mexico86 was created. If you don't have any servers, create one by right-clicking. The host address is 127.0.0.1 and the maintenance database and username should be postgres . In pgAdmin, if you are asked for a password and don't know what your password is, you can reset the password of the postgres user: change password postgres =# ALTER USER postgres WITH PASSWORD \"newpassword\" ;","title":"Setup - database creation"},{"location":"0_2_postgres.html#mexico86-database-simple-queries","text":"This database contains data from the 1986 football World Cup. You can download the database creation script individually: $ wget https://raw.githubusercontent.com/SupaeroDataScience/OBD/master/scripts/mexico86/create-tables-std.sql Or git clone the class repository and navigate to the creation and insertion scripts . Once you have the scripts, run the database creation script in the mexico folder. # bash shell in Linux or OSX, or windows powershell $ psql -d db-mexico86 -f mexico86/create-tables-std.sql If that doesn't work, you can copy the script into the Query Tool in pgAdmin. Exercise 1.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Pays: ( nom , groupe) Typematch: ( type ) Match: ( paysl, paysv , butsl, butsv, type , date) You should be able to make queries now. You can either use PostgreSQL in interactive mode by running $ psql -d db-mexico86 or write your solutions in an SQL file and run the file: $ echo \"SELECT groupe FROM pays;\" > a.sql $ psql -d db-mexico86 -f a.sql You can also use the Query Editor in pgAdmin for a graphical interface. Exercise 1.2 : Write a query which lists the countries participating in the World Cup. Response nom --------------------- Argentine Italie Bulgarie Cor\u00e9e Mexique Paraguay Belgique Irak URSS Hongrie France Canada Br\u00e9sil Espagne Irlande du Nord Alg\u00e9rie Danemark RFA Uruguay \u00c9cosse Maroc Angleterre Pologne Portugal (24 rows) Exercise 1.3 : Write a query which lists all matches as a pair of countries per match. Response paysl | paysv ---------------------|--------------------- Bulgarie | Italie Argentine | Cor\u00e9e Italie | Argentine Cor\u00e9e | Bulgarie Cor\u00e9e | Italie Argentine | Bulgarie Belgique | Mexique Paraguay | Irak Mexique | Paraguay Irak | Belgique Irak | Mexique Paraguay | Belgique Canada | France URSS | Hongrie France | URSS Hongrie | Canada URSS | Canada Hongrie | France Espagne | Br\u00e9sil Alg\u00e9rie | Irlande du Nord Br\u00e9sil | Alg\u00e9rie Irlande du Nord | Espagne Irlande du Nord | Br\u00e9sil Alg\u00e9rie | Espagne Uruguay | RFA \u00c9cosse | Danemark Danemark | Uruguay RFA | \u00c9cosse \u00c9cosse | Uruguay Danemark | RFA Maroc | Pologne Portugal | Angleterre Angleterre | Maroc Pologne | Portugal Angleterre | Pologne Maroc | Portugal Br\u00e9sil | Pologne France | Italie Maroc | RFA Mexique | Bulgarie Argentine | Uruguay Angleterre | Paraguay URSS | Belgique Espagne | Danemark Br\u00e9sil | France RFA | Mexique Argentine | Angleterre Belgique | Espagne France | RFA Argentine | Belgique RFA | Argentine (51 rows) Exercise 1.4 : Write a query which lists the matches which took place on June 5, 1986. Response paysl | paysv ---------------------|----------- Italie | Argentine Cor\u00e9e | Bulgarie France | URSS (3 rows) Exercise 1.5 : Write a query which lists the countries which France played against (hint, France could have played either side). Response pays --------- Br\u00e9sil Canada Hongrie Italie RFA URSS (6 rows) Exercise 1.6 : Write a query which returns the winner of the World Cup Response pays ----------- Argentine (1 row)","title":"Mexico86 database - simple queries"},{"location":"0_2_postgres.html#beer-database","text":"We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts . $ createdb db-beer $ psql -d db-beer -f beer/create-tables-std.sql $ psql -d db-beer -f beer/insert.sql Exercise 2.1 : Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin. Response Frequente: ( buveur, bar ) Sert: ( bar, biere ) Aime: ( buveur, biere ) Write queries which respond to the following questions. Hint, understanding natural joins may help. Exercise 2.2 What is the list of bars which serve the beer that Martin likes? Response bar ------------------- Ancienne Belgique La Tireuse Le Filochard (3 rows) Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like? Response buveur -------- Bob David Emilie Martin (4 rows) Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like? Response buveur -------- Cecile Alice (2 rows)","title":"Beer database"},{"location":"0_2_postgres.html#complex-queries-mexico-database","text":"Exercise 3.1 : Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information. Response paysl | paysv | buts | type | date ---------------------+---------------------+------+--------+------------ URSS | Belgique | 7 | 1/8 | 1986-06-15 France | Italie | 2 | 1/8 | 1986-06-17 Maroc | Pologne | 0 | Poule | 1986-06-02 RFA | Argentine | 5 | Finale | 1986-06-29 Br\u00e9sil | France | 2 | 1/4 | 1986-06-21 Italie | Argentine | 2 | Poule | 1986-06-05 Maroc | Portugal | 4 | Poule | 1986-06-11 Br\u00e9sil | Alg\u00e9rie | 1 | Poule | 1986-06-06 Paraguay | Belgique | 4 | Poule | 1986-06-11 Hongrie | France | 3 | Poule | 1986-06-09 Irak | Belgique | 3 | Poule | 1986-06-08 Danemark | RFA | 2 | Poule | 1986-06-13 Irlande du Nord | Espagne | 3 | Poule | 1986-06-07 Alg\u00e9rie | Irlande du Nord | 2 | Poule | 1986-06-03 RFA | Mexique | 0 | 1/4 | 1986-06-21 URSS | Hongrie | 6 | Poule | 1986-06-02 Mexique | Paraguay | 2 | Poule | 1986-06-07 Belgique | Espagne | 2 | 1/4 | 1986-06-22 Irak | Mexique | 1 | Poule | 1986-06-11 Espagne | Br\u00e9sil | 1 | Poule | 1986-06-01 Angleterre | Maroc | 0 | Poule | 1986-06-06 Irlande du Nord | Br\u00e9sil | 2 | Poule | 1986-06-12 Maroc | RFA | 1 | 1/8 | 1986-06-17 Belgique | Mexique | 3 | Poule | 1986-06-03 Bulgarie | Italie | 2 | Poule | 1986-05-31 \u00c9cosse | Uruguay | 0 | Poule | 1986-06-13 Alg\u00e9rie | Espagne | 3 | Poule | 1986-06-12 Argentine | Belgique | 2 | 1/2 | 1986-06-25 Br\u00e9sil | Pologne | 4 | 1/8 | 1986-06-16 Danemark | Uruguay | 7 | Poule | 1986-06-08 Cor\u00e9e | Italie | 5 | Poule | 1986-06-10 Canada | France | 1 | Poule | 1986-06-01 Argentine | Uruguay | 1 | 1/8 | 1986-06-16 France | RFA | 2 | 1/2 | 1986-06-25 France | URSS | 2 | Poule | 1986-06-05 Uruguay | RFA | 2 | Poule | 1986-06-04 Angleterre | Pologne | 3 | Poule | 1986-06-11 Portugal | Angleterre | 1 | Poule | 1986-06-03 \u00c9cosse | Danemark | 1 | Poule | 1986-06-04 Angleterre | Paraguay | 3 | 1/8 | 1986-06-18 Hongrie | Canada | 2 | Poule | 1986-06-06 Argentine | Cor\u00e9e | 4 | Poule | 1986-06-02 Pologne | Portugal | 1 | Poule | 1986-06-07 RFA | \u00c9cosse | 3 | Poule | 1986-06-08 Mexique | Bulgarie | 2 | 1/8 | 1986-06-15 URSS | Canada | 2 | Poule | 1986-06-09 Espagne | Danemark | 6 | 1/8 | 1986-06-18 Paraguay | Irak | 1 | Poule | 1986-06-04 Argentine | Bulgarie | 2 | Poule | 1986-06-10 Argentine | Angleterre | 3 | 1/4 | 1986-06-22 Cor\u00e9e | Bulgarie | 2 | Poule | 1986-06-05 (51 rows) Exercise 3.2 : Write a query which caluculates the number of goals scored on average in all the matches of the French team. Response Moyenne buts -------------------- 2.0000000000000000 (1 row) Exercise 3.3 : Write a query which calculates the total number of goals scored only by the French team. Response buts ------ 8 (1 row) Exercise 3.4 : Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group. Response groupe | sum --------+----- A | 17 B | 14 C | 16 D | 12 E | 15 F | 9 (6 rows) Exercise 3.5 : Write a function vainquer which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs: SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8'); SELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4'); Response vainqueur ----------- Espagne (1 row) vainqueur ----------- Match nul (1 row) Exercise 3.6 : Write a function butsparequipe which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team. SELECT * FROM butsparequipe('France'); Response pays | total | moyenne --------+-------+-------------------- France | 8 | 1.3333333333333333 (1 row) Exercise 3.7 : Using the butsparequipe function, write a query which lists all countries and the points they scored. Response pays | total ---------------------+------- Argentine | 14 Italie | 5 Bulgarie | 2 Cor\u00e9e | 4 Mexique | 6 Paraguay | 4 Belgique | 10 Irak | 1 URSS | 12 Hongrie | 2 France | 8 Canada | 0 Br\u00e9sil | 9 Espagne | 11 Irlande du Nord | 2 Alg\u00e9rie | 1 Danemark | 10 RFA | 8 Uruguay | 2 \u00c9cosse | 1 Maroc | 3 Angleterre | 7 Pologne | 1 Portugal | 2 (24 rows) Exercise 3.8 : Using the butsparequipe function, write a query which shows the country which scored the most points and the number of points they scored. Response pays | total -----------+------- Argentine | 14 (1 row)","title":"Complex queries - Mexico database"},{"location":"0_2_postgres.html#pull-the-trigger","text":"In this exercise, we're going to create a TRIGGER , a mechanism which allows for automatically executing actions when an event occurs. Create the db-trigger database. $ createdb db-trigger Exercise 4.1 : Create a table rel(nom, value) where nom is a string of characters and value is an integer. nom will be the primary key Solution CREATE TABLE IF NOT EXISTS rel ( nom VARCHAR(20), valeur INTEGER, PRIMARY KEY (nom) ); Exercise 4.2 : Add 5 tuples into the table Solution INSERT INTO rel VALUES ('Alice', 10), ('Bob', 5), ('Carl', 20), ('Denise', 11), ('Esther', 6); Exercise 4.3 : Write a trigger such that, when adding new tuples, the average value of val cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised. The following insertion should work: INSERT INTO rel VALUES ('Fab', 15); SELECT * FROM rel; As we can see, the (Fab, 15) tuple was added: nom | valeur --------+-------- Alice | 10 Bob | 5 Carl | 20 Denise | 11 Esther | 6 Fab | 15 (6 rows) However, the following insertion should give an exception: INSERT INTO rel VALUES ('Guy', 2); Solution CREATE OR REPLACE FUNCTION verifier_moyenne() RETURNS trigger AS $verifier_moyenne$ DECLARE moyenne FLOAT; nb INTEGER; BEGIN moyenne := AVG(valeur) FROM rel; nb := COUNT(*) FROM rel; IF ((nb * moyenne + NEW.valeur) / (nb + 1)) < moyenne THEN RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!'; END IF; RETURN NEW; END; $verifier_moyenne$ LANGUAGE plpgsql; CREATE TRIGGER VerificationMoyenne BEFORE INSERT ON rel FOR EACH ROW EXECUTE PROCEDURE verifier_moyenne();","title":"Pull the trigger"},{"location":"0_3_project.html","text":"NoSQL Databases Project \ud83d\udd17 The evaluation of the databases class is a presentation of a specific DBMS. You can work in teams of 4 of your choosing. The idea of this project is to study new DBMSs, with a focus on NoSQL DBMSs. There is a presentation on the differences between relational databases and various NoSQL DBMSs and another here on the history of NoSQL. In this project, students should compare their DBMS with a relational DBMS (PostgreSQL) to understand the advantages and disadvantages of various NoSQL DBMSs. You are working in a company which is looking to replace a relational DBMS currently in use. Each team should present a feasability study of a specific DBMS, showing its advantages, disadvantages, and use cases. We will organize the subjects on 06/10/2021, which is a work class dedicated to the project. Up to two different teams can work on each DBMS. The possible DBMSs are: MongoDB , a DBMS for documents used, for example, by CERN Cassandra , a distributed data storage system for handling very large amounts of structured data Redis , a very efficient key/value DBMS HBase , a distributed and non-relationed column-based DBMS Neo4j , a native graph DBMS RavenDB , a document DBMS with ACID integrity Couchbase , a document DBMS for interactive web applications CouchDB , a JSON-based DBMS with native JavaScript support InfluxDB , a distributed DBMS optimized for timeseries data OrientDB , a DMBS for graph data Each team should: install their DBMS test the DBMS on a relevant database (datasets from Google , and kaggle ) compare their DBMS with a relational database system prepare a presentation of their DBMS and example database which presents convincing argument for using this DBMS evaluate how ACID or BASE principles are met by their DBMS A good example from previous years is here . This was in the form of a report, but currently a report is not required, just the oral presentation. Presentations will take place on 02/11/2021. You should upload your presentation materials to the LMS before midnight on 01/11/2021.","title":"Project"},{"location":"0_3_project.html#nosql-databases-project","text":"The evaluation of the databases class is a presentation of a specific DBMS. You can work in teams of 4 of your choosing. The idea of this project is to study new DBMSs, with a focus on NoSQL DBMSs. There is a presentation on the differences between relational databases and various NoSQL DBMSs and another here on the history of NoSQL. In this project, students should compare their DBMS with a relational DBMS (PostgreSQL) to understand the advantages and disadvantages of various NoSQL DBMSs. You are working in a company which is looking to replace a relational DBMS currently in use. Each team should present a feasability study of a specific DBMS, showing its advantages, disadvantages, and use cases. We will organize the subjects on 06/10/2021, which is a work class dedicated to the project. Up to two different teams can work on each DBMS. The possible DBMSs are: MongoDB , a DBMS for documents used, for example, by CERN Cassandra , a distributed data storage system for handling very large amounts of structured data Redis , a very efficient key/value DBMS HBase , a distributed and non-relationed column-based DBMS Neo4j , a native graph DBMS RavenDB , a document DBMS with ACID integrity Couchbase , a document DBMS for interactive web applications CouchDB , a JSON-based DBMS with native JavaScript support InfluxDB , a distributed DBMS optimized for timeseries data OrientDB , a DMBS for graph data Each team should: install their DBMS test the DBMS on a relevant database (datasets from Google , and kaggle ) compare their DBMS with a relational database system prepare a presentation of their DBMS and example database which presents convincing argument for using this DBMS evaluate how ACID or BASE principles are met by their DBMS A good example from previous years is here . This was in the form of a report, but currently a report is not required, just the oral presentation. Presentations will take place on 02/11/2021. You should upload your presentation materials to the LMS before midnight on 01/11/2021.","title":"NoSQL Databases Project"},{"location":"1_1_overview.html","text":"Data Computation Part 1: Cloud Computing, Containers & Orchestration \ud83d\udd17 Back to home All slides Syllabus \ud83d\udd17 Introduction \ud83d\udd17 Introduction to data computation module Cloud Computing (4h) \ud83d\udd17 Intro to cloud computing & google cloud platform Date Type Link Description 07/10 Lecture Intro to Cloud Computing A lecture about an introduction to \"what is the cloud\" 07/10 Lecture Using Cloud Computing in your daily job What does it mean to \"use the cloud\" ? 07/10 Lecture Intro to Google Cloud Platform A quick intro to GCP 07/10 14/10 Hands-on GCP Part 1: The Basics My first steps with GCP, Google Cloud Shell 14/10 Hands-on GCP Part 2: My first VMs, Storage, IAAS & PAAS Here I will create a GCE instance, interact with GCS, discover managed products 20/10 Recap Important notions about Cloud Computing A recap of important notions Containers (2h) \ud83d\udd17 Intro to containers & docker Date Type Link Description 14/10 Lecture From virtualisation to Containerisation, Docker What are containers and why do we need them ? What is Docker ? 14/10 Hands-on Docker Discover Docker 20/10 Recap Important notions about Docker A recap of important notions Orchestration & Deployment (1h) \ud83d\udd17 Intro to container orchestra features Date Type Link Description 20/10 Lecture Microservices, Orchestration, Kubernetes Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes 20/10 Interactive A development env on Kubernetes and Deploying ML models into production Interactive demos of deployment & scalability with Docker and Kubernetes At Home Bonus Hands-on My first kubernetes cluster for a development environment Where we deploy a development environment on Kubernetes 20/10 Recap Important notions about Orchestration A recap of important notions Final BE (3h) \ud83d\udd17 A small workshop that puts everything together: Docker, Kubernetes, to deploy an image classifier in a scalable fashion Date Type Link Description 20/10 BE Explanations Walkthrough A final Bureau d'\u00e9tudes to wrap everything together Concluding Slides \ud83d\udd17 Finally, it's over ! What's next \ud83d\udd17 Date Duration Title 18/11 2h Cluster Compute 01/12 3h GPU 02/12 3h GPU","title":"Introduction"},{"location":"1_1_overview.html#data-computation-part-1-cloud-computing-containers-orchestration","text":"Back to home All slides","title":"Data Computation Part 1: Cloud Computing, Containers &amp; Orchestration"},{"location":"1_1_overview.html#syllabus","text":"","title":"Syllabus"},{"location":"1_1_overview.html#introduction","text":"Introduction to data computation module","title":"Introduction"},{"location":"1_1_overview.html#cloud-computing-4h","text":"Intro to cloud computing & google cloud platform Date Type Link Description 07/10 Lecture Intro to Cloud Computing A lecture about an introduction to \"what is the cloud\" 07/10 Lecture Using Cloud Computing in your daily job What does it mean to \"use the cloud\" ? 07/10 Lecture Intro to Google Cloud Platform A quick intro to GCP 07/10 14/10 Hands-on GCP Part 1: The Basics My first steps with GCP, Google Cloud Shell 14/10 Hands-on GCP Part 2: My first VMs, Storage, IAAS & PAAS Here I will create a GCE instance, interact with GCS, discover managed products 20/10 Recap Important notions about Cloud Computing A recap of important notions","title":"Cloud Computing (4h)"},{"location":"1_1_overview.html#containers-2h","text":"Intro to containers & docker Date Type Link Description 14/10 Lecture From virtualisation to Containerisation, Docker What are containers and why do we need them ? What is Docker ? 14/10 Hands-on Docker Discover Docker 20/10 Recap Important notions about Docker A recap of important notions","title":"Containers (2h)"},{"location":"1_1_overview.html#orchestration-deployment-1h","text":"Intro to container orchestra features Date Type Link Description 20/10 Lecture Microservices, Orchestration, Kubernetes Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes 20/10 Interactive A development env on Kubernetes and Deploying ML models into production Interactive demos of deployment & scalability with Docker and Kubernetes At Home Bonus Hands-on My first kubernetes cluster for a development environment Where we deploy a development environment on Kubernetes 20/10 Recap Important notions about Orchestration A recap of important notions","title":"Orchestration &amp; Deployment (1h)"},{"location":"1_1_overview.html#final-be-3h","text":"A small workshop that puts everything together: Docker, Kubernetes, to deploy an image classifier in a scalable fashion Date Type Link Description 20/10 BE Explanations Walkthrough A final Bureau d'\u00e9tudes to wrap everything together","title":"Final BE (3h)"},{"location":"1_1_overview.html#concluding-slides","text":"Finally, it's over !","title":"Concluding Slides"},{"location":"1_1_overview.html#whats-next","text":"Date Duration Title 18/11 2h Cluster Compute 01/12 3h GPU 02/12 3h GPU","title":"What's next"},{"location":"1_2_cloud.html","text":"Cloud Computing \ud83d\udd17 Intro to cloud computing \ud83d\udd17 Link to slides Using Cloud Computing \ud83d\udd17 Link to slides Google Cloud Platform \ud83d\udd17 Link to slides","title":"Lectures"},{"location":"1_2_cloud.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"1_2_cloud.html#intro-to-cloud-computing","text":"Link to slides","title":"Intro to cloud computing"},{"location":"1_2_cloud.html#using-cloud-computing","text":"Link to slides","title":"Using Cloud Computing"},{"location":"1_2_cloud.html#google-cloud-platform","text":"Link to slides","title":"Google Cloud Platform"},{"location":"1_2_gcp_handson.html","text":"GCP Hands On, first VMs, Google Cloud Storage \ud83d\udd17 How to run it ? \ud83d\udd17 The easiest way to run this workshop is to have google cloud sdk on your machine, However, like demonstrated earlier, you can use google cloud shell as the \"front end\" for GCP to run all gcloud commands from inside this VM However you have to have google chrome without too much privacy tools and a good wifi without firewall for it to work perfectly :) SSH from the cloud shell should also be possible, and you will be able to use the web preview from your cloud shell to display a port on another machine (double ssh tunnel !) Otherwise for SSHing you will be able to use the web based ssh tools and port forwarding tools of google cloud platform: https://cloud.google.com/compute/docs/ssh-in-browser 1. My first Google Compute Engine Instance \ud83d\udd17 First, we will make our first steps by creating a compute engine instance (a vm) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it What is google cloud compute engine ? try to describe it with your own words Creating my VM using the console \ud83d\udd17 Create your VM from the google cloud interface : Go to this link and follow the \"CONSOLE\" instruction Create an instance with the following parameters type: n1-standard-1 zone: europe-west4-a/b/c/d (the netherlands) or europe-west1-(b,c,d) Belgium os: ubuntu 20.04 boot disk size: 10 Gb Give it a name of your choice (that you can remember) DO NOT SHUT IT DOWN for now Note : If you were using the command line : gcloud compute instances create { name } --project ={ your-project } --zone ={ your-zone } \\ --machine-type = n1-standard-1 \\ --image = ubuntu-2004-focal-v20220110 \\ --image-project = ubuntu-os-cloud Connecting to SSH \ud83d\udd17 Warning If you can't SSH to the machine, use cloud shell to SSH to the machine Connect to ssh from google cloud shell or your terminal to the machine Solution `gcloud compute ssh {USER}@{MACHINE NAME}` Check available disk space Bash command to run `df -h` Check the OS name Bash command to run `cat /etc/os-release` Check the CPU model Solution `cat /proc/cpuinfo` Check instance google cloud properties Solution `cat /proc/cpuinfo` The magic of redimensioning VMs \ud83d\udd17 Shutdown the VM (from the web browser), check the previous codelab to see how to do it Select it and click on EDIT Change the machine type to n1-standard-2 ( link to documentation ) Relaunch it, reconnect to it and try to list the CPUs & RAM amount again Magic isn't it ? Note: If you had any files and specific configuration, they would still be here ! Transfering files from the computer to this machine \ud83d\udd17 We will use the terminal to transfer some files from * your computer to** this machine, If you use cloud shell you can do it as well : create a dummy file in cloud shell Follow this link to learn how to use the gcloud cli tool to transfer files to your instance TOC For experts, it's possible to do it manually using rsync from ssh or scp Transfer some files to your /home/${USER} directory List them from your instance ( ls ) How do we do the opposite ? See below, 2. Interacting with Google Cloud Storage \ud83d\udd17 Here we will discover google cloud storage, upload some files from your computer and download them from your instance in the cloud What is Google Cloud Storage ? Try to describe it with your own words This codelab will guide you through the basics of google cloud storage Use this codelab something from your computer to google cloud storage from the web browser. DO NOT DELETE THE FILES YET Now we will download it using the google cloud CLI tool, Here's the documentation List the content of the bucket you just created (if you deleted it previously, create a new one) Upload a file to a bucket Download a file from a bucket What if we want to do the same from the VM ? Now go back to your machine Try to list bucket, download and upload files Is it possible ? If not, it's because you have to allow the instance to access google cloud storage Shutdown the VM and edit it (like we did when we resized the instance) Check \"access scopes\", select \"set access for each api\", and select \"storage / admin\" Now restart you machine, connect back to it. You should be able to upload to google cloud storage now files now Now you can delete the bucket you just created You can delete the VM as well, we will not use it DELETE THE INSTANCE NOW 3. Google Compute Engine from the CLI and \"deep learning VMs\" \ud83d\udd17 Here we will use the google cloud sdk to create a more complex VM with a pre-installed image and connect to its jupyter server This will be useful for the next part of our workshop because both git and docker are already installed ! Google Cloud Platform comes with a set of services targeted at data scientists called AI Platform , among them are Deep Learning VMs which are essentially preinstalled VMs (more or less the same configuration as google colab) with some bonuses. What are \"Deep Learning VMs\" ? Try to use your own words What would be the alternative if you wanted to get a machine with the same installation ? Create a google compute engine instance using the command line \ud83d\udd17 Instead of using the browser to create this machine, we will be using the CLI to create instances export INSTANCE_NAME = \"fch-dlvm-1\" # RENAME THIS !!!!!!!!!! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --image-family = \"common-cpu\" \\ --image-project = deeplearning-platform-release \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --machine-type = \"n1-standard-2\" \\ --boot-disk-size = 120GB Notice the similarities between the first VM you created and this one, What changed ? If you want to learn more about compute images, image families etc... go here Connect to ssh to this machine \ud83d\udd17 Connect to your instance using the gcloud cli & ssh Warning If you can't SSH to the machine, use cloud shell to SSH to the machine, then the web preview on port 8080 This time, you will forward some ports as well Solution TOC {:toc} `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 Go to your local browser and type http://localhost:8080 , you should be in a jupyter notebook under the user jupyter You can try to play with the jupyter lab (that has a code editor and terminal capabilities) to get a feel of manipulating a remote instance Try to pip3 list to check all dependencies installed ! Shutdown the instance, or delete it. You may need it for later part of the workshops however, if you don't use cloud shell 4. Introduction to infrastructure as code \ud83d\udd17 This tutorial will guide you through google cloud deployment manager, which is a way to deploy google compute engine instances using configuration files Don't forget to adapt machine configurations and zone to your use case (see above) 5. Persistent sessions with TMUX \ud83d\udd17 https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ Connect to your instance using SSH Question: What happens if you start a long computation and disconnect ? Check that tmux is installed on the remote instance (run tmux ). if not install it Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ To check you have understood you should be able to: Connect to your remote instance with ssh Start a tmux session Launch a process (for example top ) inside it Detach from the session ( CTRL+B :detach ) Kill the ssh connection Connect again tmux attach to your session Your process should still be here ! Congratulations :) DELETE ALL THE INSTANCES YOU CREATED NOW","title":"TP 2 GCP Hands On"},{"location":"1_2_gcp_handson.html#gcp-hands-on-first-vms-google-cloud-storage","text":"","title":"GCP Hands On, first VMs, Google Cloud Storage"},{"location":"1_2_gcp_handson.html#how-to-run-it","text":"The easiest way to run this workshop is to have google cloud sdk on your machine, However, like demonstrated earlier, you can use google cloud shell as the \"front end\" for GCP to run all gcloud commands from inside this VM However you have to have google chrome without too much privacy tools and a good wifi without firewall for it to work perfectly :) SSH from the cloud shell should also be possible, and you will be able to use the web preview from your cloud shell to display a port on another machine (double ssh tunnel !) Otherwise for SSHing you will be able to use the web based ssh tools and port forwarding tools of google cloud platform: https://cloud.google.com/compute/docs/ssh-in-browser","title":"How to run it ?"},{"location":"1_2_gcp_handson.html#1-my-first-google-compute-engine-instance","text":"First, we will make our first steps by creating a compute engine instance (a vm) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it What is google cloud compute engine ? try to describe it with your own words","title":"1. My first Google Compute Engine Instance"},{"location":"1_2_gcp_handson.html#creating-my-vm-using-the-console","text":"Create your VM from the google cloud interface : Go to this link and follow the \"CONSOLE\" instruction Create an instance with the following parameters type: n1-standard-1 zone: europe-west4-a/b/c/d (the netherlands) or europe-west1-(b,c,d) Belgium os: ubuntu 20.04 boot disk size: 10 Gb Give it a name of your choice (that you can remember) DO NOT SHUT IT DOWN for now Note : If you were using the command line : gcloud compute instances create { name } --project ={ your-project } --zone ={ your-zone } \\ --machine-type = n1-standard-1 \\ --image = ubuntu-2004-focal-v20220110 \\ --image-project = ubuntu-os-cloud","title":"Creating my VM using the console"},{"location":"1_2_gcp_handson.html#connecting-to-ssh","text":"Warning If you can't SSH to the machine, use cloud shell to SSH to the machine Connect to ssh from google cloud shell or your terminal to the machine Solution `gcloud compute ssh {USER}@{MACHINE NAME}` Check available disk space Bash command to run `df -h` Check the OS name Bash command to run `cat /etc/os-release` Check the CPU model Solution `cat /proc/cpuinfo` Check instance google cloud properties Solution `cat /proc/cpuinfo`","title":"Connecting to SSH"},{"location":"1_2_gcp_handson.html#the-magic-of-redimensioning-vms","text":"Shutdown the VM (from the web browser), check the previous codelab to see how to do it Select it and click on EDIT Change the machine type to n1-standard-2 ( link to documentation ) Relaunch it, reconnect to it and try to list the CPUs & RAM amount again Magic isn't it ? Note: If you had any files and specific configuration, they would still be here !","title":"The magic of redimensioning VMs"},{"location":"1_2_gcp_handson.html#transfering-files-from-the-computer-to-this-machine","text":"We will use the terminal to transfer some files from * your computer to** this machine, If you use cloud shell you can do it as well : create a dummy file in cloud shell Follow this link to learn how to use the gcloud cli tool to transfer files to your instance TOC For experts, it's possible to do it manually using rsync from ssh or scp Transfer some files to your /home/${USER} directory List them from your instance ( ls ) How do we do the opposite ? See below,","title":"Transfering files from the computer to this machine"},{"location":"1_2_gcp_handson.html#2-interacting-with-google-cloud-storage","text":"Here we will discover google cloud storage, upload some files from your computer and download them from your instance in the cloud What is Google Cloud Storage ? Try to describe it with your own words This codelab will guide you through the basics of google cloud storage Use this codelab something from your computer to google cloud storage from the web browser. DO NOT DELETE THE FILES YET Now we will download it using the google cloud CLI tool, Here's the documentation List the content of the bucket you just created (if you deleted it previously, create a new one) Upload a file to a bucket Download a file from a bucket What if we want to do the same from the VM ? Now go back to your machine Try to list bucket, download and upload files Is it possible ? If not, it's because you have to allow the instance to access google cloud storage Shutdown the VM and edit it (like we did when we resized the instance) Check \"access scopes\", select \"set access for each api\", and select \"storage / admin\" Now restart you machine, connect back to it. You should be able to upload to google cloud storage now files now Now you can delete the bucket you just created You can delete the VM as well, we will not use it DELETE THE INSTANCE NOW","title":"2. Interacting with Google Cloud Storage"},{"location":"1_2_gcp_handson.html#3-google-compute-engine-from-the-cli-and-deep-learning-vms","text":"Here we will use the google cloud sdk to create a more complex VM with a pre-installed image and connect to its jupyter server This will be useful for the next part of our workshop because both git and docker are already installed ! Google Cloud Platform comes with a set of services targeted at data scientists called AI Platform , among them are Deep Learning VMs which are essentially preinstalled VMs (more or less the same configuration as google colab) with some bonuses. What are \"Deep Learning VMs\" ? Try to use your own words What would be the alternative if you wanted to get a machine with the same installation ?","title":"3. Google Compute Engine from the CLI and \"deep learning VMs\""},{"location":"1_2_gcp_handson.html#create-a-google-compute-engine-instance-using-the-command-line","text":"Instead of using the browser to create this machine, we will be using the CLI to create instances export INSTANCE_NAME = \"fch-dlvm-1\" # RENAME THIS !!!!!!!!!! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --image-family = \"common-cpu\" \\ --image-project = deeplearning-platform-release \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --machine-type = \"n1-standard-2\" \\ --boot-disk-size = 120GB Notice the similarities between the first VM you created and this one, What changed ? If you want to learn more about compute images, image families etc... go here","title":"Create a google compute engine instance using the command line"},{"location":"1_2_gcp_handson.html#connect-to-ssh-to-this-machine","text":"Connect to your instance using the gcloud cli & ssh Warning If you can't SSH to the machine, use cloud shell to SSH to the machine, then the web preview on port 8080 This time, you will forward some ports as well Solution TOC {:toc} `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 Go to your local browser and type http://localhost:8080 , you should be in a jupyter notebook under the user jupyter You can try to play with the jupyter lab (that has a code editor and terminal capabilities) to get a feel of manipulating a remote instance Try to pip3 list to check all dependencies installed ! Shutdown the instance, or delete it. You may need it for later part of the workshops however, if you don't use cloud shell","title":"Connect to ssh to this machine"},{"location":"1_2_gcp_handson.html#4-introduction-to-infrastructure-as-code","text":"This tutorial will guide you through google cloud deployment manager, which is a way to deploy google compute engine instances using configuration files Don't forget to adapt machine configurations and zone to your use case (see above)","title":"4. Introduction to infrastructure as code"},{"location":"1_2_gcp_handson.html#5-persistent-sessions-with-tmux","text":"https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ Connect to your instance using SSH Question: What happens if you start a long computation and disconnect ? Check that tmux is installed on the remote instance (run tmux ). if not install it Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ To check you have understood you should be able to: Connect to your remote instance with ssh Start a tmux session Launch a process (for example top ) inside it Detach from the session ( CTRL+B :detach ) Kill the ssh connection Connect again tmux attach to your session Your process should still be here ! Congratulations :) DELETE ALL THE INSTANCES YOU CREATED NOW","title":"5. Persistent sessions with TMUX"},{"location":"1_2_gcp_setup.html","text":"GCP Initial Setup & First Steps \ud83d\udd17 Abstract In this hands on you will configure your GCP account, the google cloud SDK and access the cloud console using Google Cloud Shell, You will also discover a very useful tool, a managed jupyter notebook service from google named Google Colab which may be very important for your future developments this year 1. Create your GCP Account \ud83d\udd17 Overview link Create an account within Google cloud Platform using your ISAE e-mail Use the code given by Dennis to get your free credits You should have 300$ + a \"free tier\" available to you From the interface you should create a project with a name of your choice 2. Install Google Cloud SDK & Configure the shell \ud83d\udd17 If you want to interact with GCP from your computer, you will need to install the Google Cloud SDK , which will also install a shell if you are on windows If you don't, you will have to do everything from google cloud shell (it's not as easy), so I recommend installing the SDK. The best ways to interact with google cloud SDK is with a terminal so in that order: Linux (either VM or native): https://cloud.google.com/sdk/docs/install#linux MacOS: https://cloud.google.com/sdk/docs/install#mac Windows Subsystem for Linux: see Linux Windows: https://cloud.google.com/sdk/docs/install#windows If you are on windows, you should launch the google cloud sdk shell now, Then you can configure the google cloud sdk with your account 3. My first \"VM\", Google Cloud Shell \ud83d\udd17 Intro to Google Cloud Shell \ud83d\udd17 Google Cloud Shell is a \"managed VM\" made available to interact with the GCP platform without needing to configure locally the google cloud sdk. It is useful if you only have a web browser, but it may not work and it's not as easy as using a local terminal Compared to configured a VM by yourself, this one comes loaded with developer tools and gcp authentification correctly set up, and thus is faster to use, However the main drawback to using it as a development machine is the available disk space limited to 5 Gb (not enough to build docker images for example) Here is the description of Google Cloud Shell Look at the documentation Question Can you describe it with your own words ? What would be the closest service that you can find on GCP that is similar to cloud shell ? Connect to google cloud shell \ud83d\udd17 Go to https://shell.cloud.google.com/ Follow this guide for connecting to google cloud shell using the browser If this doesn't work on your machine for whichever reason, there is a workaround which requires having installed the google-cloud-sdk Explore google cloud shell \ud83d\udd17 Check available disk space Bash command to run df -h Check the OS name Bash command to run cat /etc/os-release Check the CPU model Bash command to run cat /proc/cpuinfo This is the hardware model... how many cores do you have available ? Which amount of RAM ? Help htop will give you your current usage and available cores, or you can do nproc A demo of cloud shell web preview \ud83d\udd17 We will install Visual Studio Code Server , which is a cloud-based text editor, on Cloud Shell and preview it from your browser. There is already a code editor in Google Cloud Shell (based on Theia) but we want to showcase the web preview as well, so we will do it manually, You may enable boost mode Run curl -fsSL https://code-server.dev/install.sh | sh in your terminal to download & install code server Run code-server --port=8080 to start code server, it will auto generate a password Shut it down ( CTRL+C ) then Fetch your password using cat ~/.config/code-server/config.yaml Re-run it Open web preview on port 8080 and log in You should be able to open files, get a terminal from inside a vscode inside your browser inside a VM ... Magic isn't it ? Warning It is possible that the browser-based method does not work on your machine, there is a troubleshooting guide on this (mainly it doesn't like too much privacy on your browser) The alternative solution would be to connect to it from your terminal / local shell using the google cloud sdk, Here is the documentation for this Command to run in this case: gcloud alpha cloud-shell ssh -- -L 8080:localhost:8080 !!! why are we using cloud shell ? * Cloud Shell is basically a managed VM with the google cloud SDK configured * It even has some nice tools installed such as docker so you can use it for the next workshops * The huge bonus is that it streams through your web browser so it can bypass ISAE-EDU blocking of ssh * However, it may have some stability issues 4. Optional - Google Colaboratory \ud83d\udd17 Abstract Previous versions of this class happened before the ML Class so there was an introduction to google collab. You should have extensively used this tool before, so skip this :) Here, you will look at Google Colaboratory, which is a very handy tool for doing data science work (based on jupyter notebooks) on the cloud, using a preconfigured instance (which can access a GPU). You will be able to store data on Google Drive and to share I highly recommend using this for Jupyter based AML BE , but I invite you to discover google colab at home, or during AML BE because it's a useful tool but mastering it is not relevant for our cloud class Intro & Description of Google Colaboratory \ud83d\udd17 Open Google Colab Some intro , another one Question Can you describe what it is ? Is it IaaS ? PaaS ? SaaS ? why exactly ? Info Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with Zero configuration required Free access to GPUs Easy sharing It offers a \"jupyter notebook - like\" interface, and allows to install your own dependencies by running bash commands inside the VM, with connection to google drive, google sheets You can manipulate the notebooks from your Google Drive and share it like it was a GDoc document It's essentially between SaaS and PaaS, it offers you a development platform without you having to manage anything except your code and your data (which are both data from the cloud provider point of view) Loading jupyter notebooks, interacting with google drive \ud83d\udd17 Open a notebook you previously ran on your computer (from AML class), you can run a notebook on github directly in google colab Try to run it inside google colab Link google colab and google drive and upload something on google drive (like an image) and display in on google colab Other nice usages of Google Colab \ud83d\udd17 Writing markdown to generate reports Installing custom dependencies","title":"TP 1 GCP Initial Setup & First Steps"},{"location":"1_2_gcp_setup.html#gcp-initial-setup-first-steps","text":"Abstract In this hands on you will configure your GCP account, the google cloud SDK and access the cloud console using Google Cloud Shell, You will also discover a very useful tool, a managed jupyter notebook service from google named Google Colab which may be very important for your future developments this year","title":"GCP Initial Setup &amp; First Steps"},{"location":"1_2_gcp_setup.html#1-create-your-gcp-account","text":"Overview link Create an account within Google cloud Platform using your ISAE e-mail Use the code given by Dennis to get your free credits You should have 300$ + a \"free tier\" available to you From the interface you should create a project with a name of your choice","title":"1. Create your GCP Account"},{"location":"1_2_gcp_setup.html#2-install-google-cloud-sdk-configure-the-shell","text":"If you want to interact with GCP from your computer, you will need to install the Google Cloud SDK , which will also install a shell if you are on windows If you don't, you will have to do everything from google cloud shell (it's not as easy), so I recommend installing the SDK. The best ways to interact with google cloud SDK is with a terminal so in that order: Linux (either VM or native): https://cloud.google.com/sdk/docs/install#linux MacOS: https://cloud.google.com/sdk/docs/install#mac Windows Subsystem for Linux: see Linux Windows: https://cloud.google.com/sdk/docs/install#windows If you are on windows, you should launch the google cloud sdk shell now, Then you can configure the google cloud sdk with your account","title":"2. Install Google Cloud SDK &amp; Configure the shell"},{"location":"1_2_gcp_setup.html#3-my-first-vm-google-cloud-shell","text":"","title":"3. My first \"VM\", Google Cloud Shell"},{"location":"1_2_gcp_setup.html#intro-to-google-cloud-shell","text":"Google Cloud Shell is a \"managed VM\" made available to interact with the GCP platform without needing to configure locally the google cloud sdk. It is useful if you only have a web browser, but it may not work and it's not as easy as using a local terminal Compared to configured a VM by yourself, this one comes loaded with developer tools and gcp authentification correctly set up, and thus is faster to use, However the main drawback to using it as a development machine is the available disk space limited to 5 Gb (not enough to build docker images for example) Here is the description of Google Cloud Shell Look at the documentation Question Can you describe it with your own words ? What would be the closest service that you can find on GCP that is similar to cloud shell ?","title":"Intro to Google Cloud Shell"},{"location":"1_2_gcp_setup.html#connect-to-google-cloud-shell","text":"Go to https://shell.cloud.google.com/ Follow this guide for connecting to google cloud shell using the browser If this doesn't work on your machine for whichever reason, there is a workaround which requires having installed the google-cloud-sdk","title":"Connect to google cloud shell"},{"location":"1_2_gcp_setup.html#explore-google-cloud-shell","text":"Check available disk space Bash command to run df -h Check the OS name Bash command to run cat /etc/os-release Check the CPU model Bash command to run cat /proc/cpuinfo This is the hardware model... how many cores do you have available ? Which amount of RAM ? Help htop will give you your current usage and available cores, or you can do nproc","title":"Explore google cloud shell"},{"location":"1_2_gcp_setup.html#a-demo-of-cloud-shell-web-preview","text":"We will install Visual Studio Code Server , which is a cloud-based text editor, on Cloud Shell and preview it from your browser. There is already a code editor in Google Cloud Shell (based on Theia) but we want to showcase the web preview as well, so we will do it manually, You may enable boost mode Run curl -fsSL https://code-server.dev/install.sh | sh in your terminal to download & install code server Run code-server --port=8080 to start code server, it will auto generate a password Shut it down ( CTRL+C ) then Fetch your password using cat ~/.config/code-server/config.yaml Re-run it Open web preview on port 8080 and log in You should be able to open files, get a terminal from inside a vscode inside your browser inside a VM ... Magic isn't it ? Warning It is possible that the browser-based method does not work on your machine, there is a troubleshooting guide on this (mainly it doesn't like too much privacy on your browser) The alternative solution would be to connect to it from your terminal / local shell using the google cloud sdk, Here is the documentation for this Command to run in this case: gcloud alpha cloud-shell ssh -- -L 8080:localhost:8080 !!! why are we using cloud shell ? * Cloud Shell is basically a managed VM with the google cloud SDK configured * It even has some nice tools installed such as docker so you can use it for the next workshops * The huge bonus is that it streams through your web browser so it can bypass ISAE-EDU blocking of ssh * However, it may have some stability issues","title":"A demo of cloud shell web preview"},{"location":"1_2_gcp_setup.html#4-optional-google-colaboratory","text":"Abstract Previous versions of this class happened before the ML Class so there was an introduction to google collab. You should have extensively used this tool before, so skip this :) Here, you will look at Google Colaboratory, which is a very handy tool for doing data science work (based on jupyter notebooks) on the cloud, using a preconfigured instance (which can access a GPU). You will be able to store data on Google Drive and to share I highly recommend using this for Jupyter based AML BE , but I invite you to discover google colab at home, or during AML BE because it's a useful tool but mastering it is not relevant for our cloud class","title":"4. Optional - Google Colaboratory"},{"location":"1_2_gcp_setup.html#intro-description-of-google-colaboratory","text":"Open Google Colab Some intro , another one Question Can you describe what it is ? Is it IaaS ? PaaS ? SaaS ? why exactly ? Info Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with Zero configuration required Free access to GPUs Easy sharing It offers a \"jupyter notebook - like\" interface, and allows to install your own dependencies by running bash commands inside the VM, with connection to google drive, google sheets You can manipulate the notebooks from your Google Drive and share it like it was a GDoc document It's essentially between SaaS and PaaS, it offers you a development platform without you having to manage anything except your code and your data (which are both data from the cloud provider point of view)","title":"Intro &amp; Description of Google Colaboratory"},{"location":"1_2_gcp_setup.html#loading-jupyter-notebooks-interacting-with-google-drive","text":"Open a notebook you previously ran on your computer (from AML class), you can run a notebook on github directly in google colab Try to run it inside google colab Link google colab and google drive and upload something on google drive (like an image) and display in on google colab","title":"Loading jupyter notebooks, interacting with google drive"},{"location":"1_2_gcp_setup.html#other-nice-usages-of-google-colab","text":"Writing markdown to generate reports Installing custom dependencies","title":"Other nice usages of Google Colab"},{"location":"1_3_containers.html","text":"From Virtualisation to Containerisation \ud83d\udd17 Link to slides","title":"Lecture"},{"location":"1_3_containers.html#from-virtualisation-to-containerisation","text":"Link to slides","title":"From Virtualisation to Containerisation"},{"location":"1_3_docker_tp.html","text":"Docker: Hands on \ud83d\udd17 How to run this ? \ud83d\udd17 We will discover the basics of docker and you will be able to manipulate your first images and containers ! Two solutions to do that : 1) You are inside sing the google cloud deep learning VM created previously 2) It's also possible to do this from google cloud shell ( shell.cloud.google.com ) as well but you will be limited in disk space later on : Skip 5. if that is the case. Why only two of these ? That's because both have git and docker installed and installing docker is a pain. If you are running linux on your laptop you may install docker and do everything from your computer Otherwise... don't ;) If you are using the deep learning vm Disconnect from your instance and relaunch it while mapping 8888 as well. You should have the vm jupyter's lab on 8080, and 8888 free Solution `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 -L 8081:localhost:8081 -L 8888:localhost:8888` 1. Manipulating docker for the 1st time \ud83d\udd17 Source: https://github.com/docker/labs To get started, let's run the following in our terminal: $ docker pull alpine The pull command fetches the alpine image from the Docker registry and saves it in our system. You can use the docker images command to see a list of all images on your system. $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B 1.1 Docker Run \ud83d\udd17 Great! Let's now run a Docker container based on this image. To do that you are going to use the docker run command. $ docker run alpine ls -l total 48 drwxr-xr-x 2 root root 4096 Mar 2 16:20 bin drwxr-xr-x 5 root root 360 Mar 18 09:47 dev drwxr-xr-x 13 root root 4096 Mar 18 09:47 etc drwxr-xr-x 2 root root 4096 Mar 2 16:20 home drwxr-xr-x 5 root root 4096 Mar 2 16:20 lib ...... ...... What happened? Behind the scenes, a lot of stuff happened. When you call run , The Docker client contacts the Docker daemon The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Store. (Since we have issued docker pull alpine before, the download step is not necessary) The Docker daemon creates the container and then runs a command in that container. The Docker daemon streams the output of the command to the Docker client When you run docker run alpine , you provided a command ( ls -l ), so Docker started the command specified and you saw the listing. Let's try something more exciting. $ docker run alpine echo \"hello from alpine\" hello from alpine OK, that's some actual output. In this case, the Docker client dutifully ran the echo command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Try another command. docker run alpine /bin/sh Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to docker run -it alpine /bin/sh . You are now inside the container shell and you can try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Since no containers are running, you see a blank line. Let's try a more useful variant: docker ps -a $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36171a5da744 alpine \"/bin/sh\" 5 minutes ago Exited (0) 2 minutes ago fervent_newton a6a9d46d0b2f alpine \"echo 'hello from alp\" 6 minutes ago Exited (0) 6 minutes ago lonely_kilby ff0a5c3750b9 alpine \"ls -l\" 8 minutes ago Exited (0) 8 minutes ago elated_ramanujan c317d0a9e3d2 hello-world \"/hello\" 34 seconds ago Exited (0) 12 minutes ago stupefied_mcclintock What you see above is a list of all containers that you ran. Notice that the STATUS column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now: $ docker run -it alpine /bin/sh / # ls bin dev etc home lib linuxrc media mnt proc root run sbin sys tmp usr var / # uname -a Linux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux Running the run command with the -it flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands. That concludes a whirlwind tour of the docker run command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run , use docker run --help to see a list of all flags it supports. As you proceed further, we'll see a few more variants of docker run . 1.2 Terminology \ud83d\udd17 In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem. Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run docker inspect alpine . In the demo above, you used the docker pull command to download the alpine image. When you executed the command docker run hello-world , it also did a docker pull behind the scenes to download the hello-world image. Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using docker run which you did using the alpine image that you downloaded. A list of running containers can be seen using the docker ps command. Docker daemon - The background service running on the host that manages building, running and distributing Docker containers. Docker client - The command line tool that allows the user to interact with the Docker daemon. Docker Store - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial. 2.0 Webapps with Docker \ud83d\udd17 Source: https://github.com/docker/labs Great! So you have now looked at docker run , played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker. 2.1 Run a static website in a container \ud83d\udd17 Note: Code for this section is in this repo in the website directory Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Store, run the container, and see how easy it is to set up a web server. The image that you are going to use is a single-page website that was already created for this demo and is available on the Docker Store as dockersamples/static-site . You can download and run the image directly in one go using docker run as follows. docker run -d dockersamples/static-site Files: Dockerfile hello_docker.html Note: The current version of this image doesn't run without the -d flag. The -d flag enables detached mode , which detaches the running container from the terminal/shell and returns your prompt after the container starts. We are debugging the problem with this image but for now, use -d even for this first example. So, what happens when you run this command? Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container. Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine? Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the docker run command to add this instruction. Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode. First, stop the container that you have just launched. In order to do this, we need the container ID. Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run docker ps to view the running containers. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7a0e504ca3e dockersamples/static-site \"/bin/sh -c 'cd /usr/\" 28 seconds ago Up 26 seconds 80 /tcp, 443 /tcp stupefied_mahavira Check out the CONTAINER ID column. You will need to use this CONTAINER ID value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the CONTAINER ID on our system; you should use the value that you see in your terminal. $ docker stop a7a0e504ca3e $ docker rm a7a0e504ca3e Note: A cool feature is that you do not need to specify the entire CONTAINER ID . You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up. Now, let's launch a container in detached mode as shown below: $ docker run --name static-site -e AUTHOR = \"Your Name\" -d -P dockersamples/static-site e61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810 In the above command: -d will create a container with the process detached from our terminal -P will publish all the exposed container ports to random ports on the Docker host -e is how you pass environment variables to the container --name allows you to specify a container name AUTHOR is the environment variable name and Your Name is the value that you can pass Now you can see the ports by running the docker port command. $ docker port static-site 443 /tcp -> 0 .0.0.0:32772 80 /tcp -> 0 .0.0.0:32773 **if you are on a distant machine and you have mapped 8888:localhost:8888, you have to run $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site to be able to connect to localhost:8888 and see the website if you are on cloud shell, open the web preview on 8888 If you are running Docker for Mac , Docker for Windows , or Docker on Linux, you can open http://localhost:[YOUR_PORT_FOR 80/tcp] . For our example this is http://localhost:32773 . If you are using Docker Machine on Mac or Windows, you can find the hostname on the command line using docker-machine as follows (assuming you are using the default machine). $ docker-machine ip default 192 .168.99.100 You can now open http://<YOUR_IPADDRESS>:[YOUR_PORT_FOR 80/tcp] to see your site live! For our example, this is: http://192.168.99.100:32773 . You can also run a second webserver at the same time, specifying a custom host port mapping to the container's webserver. $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site To deploy this on a real server you would just need to install Docker, and run the above docker command(as in this case you can see the AUTHOR is Docker which we passed as an environment variable). Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section. But first, let's stop and remove the containers since you won't be using them anymore. $ docker stop static-site $ docker rm static-site Let's use a shortcut to remove the second site: $ docker rm -f static-site-2 Run docker ps to make sure the containers are gone. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2.2 Docker Images \ud83d\udd17 In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally. Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE dockersamples/static-site latest 92a386b6e686 2 hours ago 190 .5 MB nginx latest af4b3d7d5401 3 hours ago 190 .5 MB python 2 .7 1c32174fd534 14 hours ago 676 .8 MB postgres 9 .4 88d845ac7a88 14 hours ago 263 .6 MB containous/traefik latest 27b4e0c6b2fd 4 days ago 20 .75 MB node 0 .10 42426a5cba5f 6 days ago 633 .7 MB redis latest 4f5f397d4b7c 7 days ago 177 .5 MB mongo latest 467eb21035a8 7 days ago 309 .7 MB alpine 3 .3 70c557e50ed6 8 days ago 4 .794 MB java 7 21f6ce84e43c 8 days ago 587 .7 MB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest . For example you could pull a specific version of ubuntu image as follows: $ docker pull ubuntu:12.04 If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named latest . So for example, the docker pull command given below will pull an image named ubuntu:latest : $ docker pull ubuntu To get a new Docker image you can either get it from a registry (such as the Docker Store) or create your own. There are hundreds of thousands of images available on Docker Store . You can also search for images directly from the command line using docker search . An important distinction with regard to images is between base images and child images . Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian. Child images are images that build on base images and add additional functionality. Another key concept is the idea of official images and user images . (Both of which can be base images or child images.) Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the python , node , alpine and nginx images are official (base) images. To find out more about them, check out the Official Images Documentation . User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as user/image-name . The user value in the image name is your Docker Store user or organization name. 2.3 Create your first image \ud83d\udd17 Note: The code for this section is in this repository in the flask-app directory. Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application. The goal of this exercise is to create a Docker image which will run a Flask app. We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile . Finally, we'll build the image, and then run it. Create a Python Flask app that displays random cat pix Write a Dockerfile Build the image Run your image Dockerfile commands summary 2.3.1 Create a Python Flask app that displays random cat pix \ud83d\udd17 For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat .gif every time it is loaded - because, you know, who doesn't like cats? Start by creating a directory called flask-app where we'll create the following files: app.py requirements.txt templates/index.html Dockerfile Make sure to cd flask-app before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image. app.py \ud83d\udd17 Create the app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif1.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif2.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif3.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif4.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif5.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif6.gif\" , ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" ) requirements.txt \ud83d\udd17 In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file: flask typer templates/index.html \ud83d\udd17 Create a directory called templates and create an index.html file in that directory with the following content in it: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html > 2.3.2 Write a Dockerfile \ud83d\udd17 We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image . Since our application is written in Python, we will build our own Python image based on Alpine . We'll do that using a Dockerfile . A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles. Create a file called Dockerfile , and add content to it as described below. We'll start by specifying our base image, using the FROM keyword: FROM alpine:3.15 The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next: RUN apk add --update py3-pip Let's add the files that make up the Flask Application. Install all Python requirements for our app to run. This will be accomplished by adding the lines: COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt Copy the files you have created earlier into our image by using COPY command. COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ Specify the port number which needs to be exposed. Since our flask app is running on 5000 that's what we'll expose. EXPOSE 5000 The last step is the command for running the application which is simply - python ./app.py . Use the CMD command to do that: CMD [\"python3\", \"/usr/src/app/app.py\"] The primary purpose of CMD is to tell the container which command it should run by default when it is started. Verify your Dockerfile. Our Dockerfile is now ready. This is how it looks: # our base image FROM alpine:3.15 # Install python and pip RUN apk add --update py3-pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip3 install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [\"python3\", \"/usr/src/app/app.py\"] 2.3.3 Build the image \ud83d\udd17 Now that you have your Dockerfile , you can build your image. The docker build command does the heavy-lifting of creating a docker image from a Dockerfile . The docker build command is quite simple - it takes an optional tag name with the -t flag, and the location of the directory containing the Dockerfile - the . indicates the current directory: docker build -t myfirstapp:1.0 . $ docker build -t myfirstapp:1.0 . Sending build context to Docker daemon 9.728 kB Step 1 : FROM alpine:latest ---> 0d81fc72e790 Step 2 : RUN apk add --update py-pip ---> Running in 8abd4091b5f5 fetch http://dl-4.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz fetch http://dl-4.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz (1/12) Installing libbz2 (1.0.6-r4) (2/12) Installing expat (2.1.0-r2) (3/12) Installing libffi (3.2.1-r2) (4/12) Installing gdbm (1.11-r1) (5/12) Installing ncurses-terminfo-base (6.0-r6) (6/12) Installing ncurses-terminfo (6.0-r6) (7/12) Installing ncurses-libs (6.0-r6) (8/12) Installing readline (6.3.008-r4) (9/12) Installing sqlite-libs (3.9.2-r0) (10/12) Installing python (2.7.11-r3) (11/12) Installing py-setuptools (18.8-r0) (12/12) Installing py-pip (7.1.2-r0) Executing busybox-1.24.1-r7.trigger OK: 59 MiB in 23 packages ---> 976a232ac4ad Removing intermediate container 8abd4091b5f5 Step 3 : COPY requirements.txt /usr/src/app/ ---> 65b4be05340c Removing intermediate container 29ef53b58e0f Step 4 : RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt ---> Running in a1f26ded28e7 Collecting Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1)) Downloading Flask-0.10.1.tar.gz (544kB) Collecting Werkzeug>=0.7 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Werkzeug-0.11.4-py2.py3-none-any.whl (305kB) Collecting Jinja2>=2.4 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB) Collecting itsdangerous>=0.21 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading itsdangerous-0.24.tar.gz (46kB) Collecting MarkupSafe (from Jinja2>=2.4->Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading MarkupSafe-0.23.tar.gz Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask Running setup.py install for MarkupSafe Running setup.py install for itsdangerous Running setup.py install for Flask Successfully installed Flask-0.10.1 Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.4 itsdangerous-0.24 You are using pip version 7.1.2, however version 8.1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. ---> 8de73b0730c2 Removing intermediate container a1f26ded28e7 Step 5 : COPY app.py /usr/src/app/ ---> 6a3436fca83e Removing intermediate container d51b81a8b698 Step 6 : COPY templates/index.html /usr/src/app/templates/ ---> 8098386bee99 Removing intermediate container b783d7646f83 Step 7 : EXPOSE 5000 ---> Running in 31401b7dea40 ---> 5e9988d87da7 Removing intermediate container 31401b7dea40 Step 8 : CMD python /usr/src/app/app.py ---> Running in 78e324d26576 ---> 2f7357a0805d Removing intermediate container 78e324d26576 Successfully built 2f7357a0805d If you don't have the alpine:3.5 image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image ( <YOUR_USERNAME>/myfirstapp ) shows. 2.3.4 Run your image \ud83d\udd17 The next step in this section is to run the image and see if it actually works. $ docker run -p 8888 :5000 --name myfirstapp myfirstapp:1.0 * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) Head over to http://localhost:8888 and your app should be live. Note If you are using Docker Machine, you may need to open up another terminal and determine the container ip address using docker-machine ip default . Hit the Refresh button in the web browser to see a few more cat images. 2.3.5 Dockerfile commands summary \ud83d\udd17 Here's a quick summary of the few basic commands we used in our Dockerfile. FROM starts the Dockerfile. It is a requirement that the Dockerfile must start with the FROM command. Images are created in layers, which means you can use another image as the base image for your own. The FROM command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format username/imagename:version . RUN is used to build up the Image you're creating. For each RUN command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a RUN instruction is to place the full text of the shell command after the RUN (e.g., RUN mkdir /user/local/foo ). This will automatically run in a /bin/sh shell. You can define a different shell like this: RUN /bin/bash -c 'mkdir /user/local/foo' COPY copies local files into the container. CMD defines the commands that will run on the Image at start-up. Unlike a RUN , this does not create a new layer for the Image, but simply runs the command. There can only be one CMD per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the CMD run a script. CMD requires that you tell it where to run the command, unlike RUN . So example CMD commands would be: CMD [\"python\", \"./app.py\"] CMD [\"/bin/bash\", \"echo\", \"Hello World\"] EXPOSE creates a hint for users of an image which ports provide services. It is included in the information which can be retrieved via $ docker inspect <container-id> . Note: The EXPOSE command does not actually make any ports accessible to the host! Instead, this requires publishing ports by means of the -p flag when using $ docker run . PUSH pushes your image to Docker Cloud, or alternately to a private registry Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles . 3. Running CLI apps packaged in docker while mounting volumes \ud83d\udd17 Instead of serving web app you can also run applications, like command line interfaces or training scripts, packaged in docker. It is very useful to deliver packaged apps with specific installation to other users. This is often used when you want to package your machine learning environment to run training in distributed fashion. Usually you just edit config files in an external editor and pass it to the docker Let's modify the app.py in 2. with the following import typer from typing import Optional from pathlib import Path app = typer . Typer () @app . command () def hello ( name : str ): typer . echo ( f \"Hello { name } \" ) @app . command () def run_config ( config : Optional [ Path ] = typer . Option ( None )): if config is None : typer . echo ( \"No config file\" ) raise typer . Abort () if config . is_file (): text = config . read_text () typer . echo ( f \"Config file contents: \\n { text } \" ) elif config . is_dir (): typer . echo ( \"Config is a directory, will use all its config files\" ) elif not config . exists (): typer . echo ( \"The config doesn't exist\" ) if __name__ == \"__main__\" : app () With your terminal you would call it using python app.py hello {my name} or python app.py run-config --config-file {my config} Modify the dockerfile : Replace CMD [\"python3\", \"/usr/src/app/app.py\"] By ENTRYPOINT [\"python3\", \"/usr/src/app/app.py\"] Rebuild your docker image (maybe git it another name) Now to run the CLI you just have to docker run --rm {your image} {your args} . Try it with docker run {...} hello {your name} In order to pass a config file, or data to your docker, you need to make it available to your docker. To do that, we have to mount volumes Create a dummy config file ( config.txt ) in another folder (ex: config/ ) then mount it when you run the docker: docker run --rm \\ -v /home/${USER}/configs:/home/configs \\ --workdir /home/ \\ {your image} \\ run-config --config-file {path to your config in DOCKER, eg /home/configs/config.txt} Note that since you mounted volumes, you must pass the path in the docker to your config file for it to work 4. Containers Registry \ud83d\udd17 Remember Container Registries ? Here as some explainers The main container registry is dockerhub, https://hub.docker.com/ All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before Example, the Python Image Google Cloud has a Container Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only. However, it requires naming the image in a specific fashion: eu.gcr.io/${PROJECT_ID}/name:tag Use the docker cli to tag your previous myfirstapp image to the right namespace docker tag myfirstapp eu.gcr.io/{PROJECT_ID}/{a-unique-name-describing-your-app}:1.0 Upload it on container registry docker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG] If you have a problem of authentification, gcloud auth configure-docker Hint to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Go to container registry https://console.cloud.google.com/gcr, you should see your docker image :) 5. Data Science Standardized Environment \ud83d\udd17 5.1 Intro \ud83d\udd17 Those of us who work on a team know how hard it is to create a standardize development environment. Or if you have ever updated a dependency and had everything break, you understand the importance of keeping development environments isolated. Using Docker, we can create a project / team image with our development environment and mount a volume with our notebooks and data. The benefits of this workflow are that we can: Separate out projects Spin up a container to onboard new employees Build an automated testing pipeline to confirm upgrade dependencies do not break code 5.2 Kaggle Docker Image \ud83d\udd17 For this exercise we will use Kaggle Docker Image which is a fully configured docker image that can be used as a data science container Take a look at the documentation and the repository 5.3 Get the algorithm in ML git in your Virtual Machine \ud83d\udd17 From your vm, run git clone https://github.com/erachelson/MLclass.git , this should setup your AML class inside your VM 5.4 Mounting volumes and ports \ud83d\udd17 Now let's run the image. This container has a jupyter notebook accessible from port 8080 so we will need to map the host port 8888 (the one accessible from the ssh tunnel) to the docker port 8080, we will use port forwarding We will also need to make available the notebooks on the VM to the container... we will mount volumes . Your data is located in /home/${USER}/MLClass and we want to miunt it in /tmp/workdir docker run --rm -it \\ -p 8888 :8080 \\ -v /home/ ${ USER } /MLclass:/home/Mlclass \\ --workdir /home/ \\ gcr.io/kaggle-images/python \\ /bin/bash /run_jupyter.sh Note: this image is very large ! Options breakdown: * --rm remove the container when we stop it * -it run the container in interactive mode * -p forward port from host:container * other: options from the kaggle container You should now see a jupyter lab with mlclass accessible if you connect your browser (in your laptop) to port 8888 (localhost:8888) So basically, we mapped the ports local 8888 to vm 8888 and vm 8888 to docker 8080 6. Bonus - Using Google Cloud Tools for Docker \ud83d\udd17 Using cloud shell you should be able to do the Hello World Dockerfile exercise except that instead of using docker build you use Google Cloud Build Tutorial: https://cloud.google.com/cloud-build/docs/quickstart-docker Example command : gcloud builds submit --tag eu.gcr.io/$PROJECT_ID/{image}:{tag} . Help to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Example Try to build the hello world app 7. Bonus - Docker Compose \ud83d\udd17 https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6 https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md 8. Bonus - Going further \ud83d\udd17 https://container.training/","title":"TP"},{"location":"1_3_docker_tp.html#docker-hands-on","text":"","title":"Docker: Hands on"},{"location":"1_3_docker_tp.html#how-to-run-this","text":"We will discover the basics of docker and you will be able to manipulate your first images and containers ! Two solutions to do that : 1) You are inside sing the google cloud deep learning VM created previously 2) It's also possible to do this from google cloud shell ( shell.cloud.google.com ) as well but you will be limited in disk space later on : Skip 5. if that is the case. Why only two of these ? That's because both have git and docker installed and installing docker is a pain. If you are running linux on your laptop you may install docker and do everything from your computer Otherwise... don't ;) If you are using the deep learning vm Disconnect from your instance and relaunch it while mapping 8888 as well. You should have the vm jupyter's lab on 8080, and 8888 free Solution `gcloud compute ssh user@machine-name --zone europe-west4-a -- -L 8080:localhost:8080 -L 8081:localhost:8081 -L 8888:localhost:8888`","title":"How to run this ?"},{"location":"1_3_docker_tp.html#1-manipulating-docker-for-the-1st-time","text":"Source: https://github.com/docker/labs To get started, let's run the following in our terminal: $ docker pull alpine The pull command fetches the alpine image from the Docker registry and saves it in our system. You can use the docker images command to see a list of all images on your system. $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B","title":"1. Manipulating docker for the 1st time"},{"location":"1_3_docker_tp.html#11-docker-run","text":"Great! Let's now run a Docker container based on this image. To do that you are going to use the docker run command. $ docker run alpine ls -l total 48 drwxr-xr-x 2 root root 4096 Mar 2 16:20 bin drwxr-xr-x 5 root root 360 Mar 18 09:47 dev drwxr-xr-x 13 root root 4096 Mar 18 09:47 etc drwxr-xr-x 2 root root 4096 Mar 2 16:20 home drwxr-xr-x 5 root root 4096 Mar 2 16:20 lib ...... ...... What happened? Behind the scenes, a lot of stuff happened. When you call run , The Docker client contacts the Docker daemon The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Store. (Since we have issued docker pull alpine before, the download step is not necessary) The Docker daemon creates the container and then runs a command in that container. The Docker daemon streams the output of the command to the Docker client When you run docker run alpine , you provided a command ( ls -l ), so Docker started the command specified and you saw the listing. Let's try something more exciting. $ docker run alpine echo \"hello from alpine\" hello from alpine OK, that's some actual output. In this case, the Docker client dutifully ran the echo command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Try another command. docker run alpine /bin/sh Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to docker run -it alpine /bin/sh . You are now inside the container shell and you can try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Since no containers are running, you see a blank line. Let's try a more useful variant: docker ps -a $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36171a5da744 alpine \"/bin/sh\" 5 minutes ago Exited (0) 2 minutes ago fervent_newton a6a9d46d0b2f alpine \"echo 'hello from alp\" 6 minutes ago Exited (0) 6 minutes ago lonely_kilby ff0a5c3750b9 alpine \"ls -l\" 8 minutes ago Exited (0) 8 minutes ago elated_ramanujan c317d0a9e3d2 hello-world \"/hello\" 34 seconds ago Exited (0) 12 minutes ago stupefied_mcclintock What you see above is a list of all containers that you ran. Notice that the STATUS column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now: $ docker run -it alpine /bin/sh / # ls bin dev etc home lib linuxrc media mnt proc root run sbin sys tmp usr var / # uname -a Linux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux Running the run command with the -it flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands. That concludes a whirlwind tour of the docker run command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run , use docker run --help to see a list of all flags it supports. As you proceed further, we'll see a few more variants of docker run .","title":"1.1 Docker Run"},{"location":"1_3_docker_tp.html#12-terminology","text":"In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem. Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run docker inspect alpine . In the demo above, you used the docker pull command to download the alpine image. When you executed the command docker run hello-world , it also did a docker pull behind the scenes to download the hello-world image. Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using docker run which you did using the alpine image that you downloaded. A list of running containers can be seen using the docker ps command. Docker daemon - The background service running on the host that manages building, running and distributing Docker containers. Docker client - The command line tool that allows the user to interact with the Docker daemon. Docker Store - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial.","title":"1.2 Terminology"},{"location":"1_3_docker_tp.html#20-webapps-with-docker","text":"Source: https://github.com/docker/labs Great! So you have now looked at docker run , played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker.","title":"2.0 Webapps with Docker"},{"location":"1_3_docker_tp.html#21-run-a-static-website-in-a-container","text":"Note: Code for this section is in this repo in the website directory Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Store, run the container, and see how easy it is to set up a web server. The image that you are going to use is a single-page website that was already created for this demo and is available on the Docker Store as dockersamples/static-site . You can download and run the image directly in one go using docker run as follows. docker run -d dockersamples/static-site Files: Dockerfile hello_docker.html Note: The current version of this image doesn't run without the -d flag. The -d flag enables detached mode , which detaches the running container from the terminal/shell and returns your prompt after the container starts. We are debugging the problem with this image but for now, use -d even for this first example. So, what happens when you run this command? Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container. Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine? Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the docker run command to add this instruction. Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode. First, stop the container that you have just launched. In order to do this, we need the container ID. Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run docker ps to view the running containers. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a7a0e504ca3e dockersamples/static-site \"/bin/sh -c 'cd /usr/\" 28 seconds ago Up 26 seconds 80 /tcp, 443 /tcp stupefied_mahavira Check out the CONTAINER ID column. You will need to use this CONTAINER ID value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the CONTAINER ID on our system; you should use the value that you see in your terminal. $ docker stop a7a0e504ca3e $ docker rm a7a0e504ca3e Note: A cool feature is that you do not need to specify the entire CONTAINER ID . You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up. Now, let's launch a container in detached mode as shown below: $ docker run --name static-site -e AUTHOR = \"Your Name\" -d -P dockersamples/static-site e61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810 In the above command: -d will create a container with the process detached from our terminal -P will publish all the exposed container ports to random ports on the Docker host -e is how you pass environment variables to the container --name allows you to specify a container name AUTHOR is the environment variable name and Your Name is the value that you can pass Now you can see the ports by running the docker port command. $ docker port static-site 443 /tcp -> 0 .0.0.0:32772 80 /tcp -> 0 .0.0.0:32773 **if you are on a distant machine and you have mapped 8888:localhost:8888, you have to run $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site to be able to connect to localhost:8888 and see the website if you are on cloud shell, open the web preview on 8888 If you are running Docker for Mac , Docker for Windows , or Docker on Linux, you can open http://localhost:[YOUR_PORT_FOR 80/tcp] . For our example this is http://localhost:32773 . If you are using Docker Machine on Mac or Windows, you can find the hostname on the command line using docker-machine as follows (assuming you are using the default machine). $ docker-machine ip default 192 .168.99.100 You can now open http://<YOUR_IPADDRESS>:[YOUR_PORT_FOR 80/tcp] to see your site live! For our example, this is: http://192.168.99.100:32773 . You can also run a second webserver at the same time, specifying a custom host port mapping to the container's webserver. $ docker run --name static-site-2 -e AUTHOR = \"Your Name\" -d -p 8888 :80 dockersamples/static-site To deploy this on a real server you would just need to install Docker, and run the above docker command(as in this case you can see the AUTHOR is Docker which we passed as an environment variable). Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section. But first, let's stop and remove the containers since you won't be using them anymore. $ docker stop static-site $ docker rm static-site Let's use a shortcut to remove the second site: $ docker rm -f static-site-2 Run docker ps to make sure the containers are gone. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES","title":"2.1 Run a static website in a container"},{"location":"1_3_docker_tp.html#22-docker-images","text":"In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally. Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE dockersamples/static-site latest 92a386b6e686 2 hours ago 190 .5 MB nginx latest af4b3d7d5401 3 hours ago 190 .5 MB python 2 .7 1c32174fd534 14 hours ago 676 .8 MB postgres 9 .4 88d845ac7a88 14 hours ago 263 .6 MB containous/traefik latest 27b4e0c6b2fd 4 days ago 20 .75 MB node 0 .10 42426a5cba5f 6 days ago 633 .7 MB redis latest 4f5f397d4b7c 7 days ago 177 .5 MB mongo latest 467eb21035a8 7 days ago 309 .7 MB alpine 3 .3 70c557e50ed6 8 days ago 4 .794 MB java 7 21f6ce84e43c 8 days ago 587 .7 MB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest . For example you could pull a specific version of ubuntu image as follows: $ docker pull ubuntu:12.04 If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named latest . So for example, the docker pull command given below will pull an image named ubuntu:latest : $ docker pull ubuntu To get a new Docker image you can either get it from a registry (such as the Docker Store) or create your own. There are hundreds of thousands of images available on Docker Store . You can also search for images directly from the command line using docker search . An important distinction with regard to images is between base images and child images . Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian. Child images are images that build on base images and add additional functionality. Another key concept is the idea of official images and user images . (Both of which can be base images or child images.) Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the python , node , alpine and nginx images are official (base) images. To find out more about them, check out the Official Images Documentation . User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as user/image-name . The user value in the image name is your Docker Store user or organization name.","title":"2.2 Docker Images"},{"location":"1_3_docker_tp.html#23-create-your-first-image","text":"Note: The code for this section is in this repository in the flask-app directory. Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application. The goal of this exercise is to create a Docker image which will run a Flask app. We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile . Finally, we'll build the image, and then run it. Create a Python Flask app that displays random cat pix Write a Dockerfile Build the image Run your image Dockerfile commands summary","title":"2.3 Create your first image"},{"location":"1_3_docker_tp.html#231-create-a-python-flask-app-that-displays-random-cat-pix","text":"For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat .gif every time it is loaded - because, you know, who doesn't like cats? Start by creating a directory called flask-app where we'll create the following files: app.py requirements.txt templates/index.html Dockerfile Make sure to cd flask-app before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image.","title":"2.3.1 Create a Python Flask app that displays random cat pix"},{"location":"1_3_docker_tp.html#apppy","text":"Create the app.py with the following content: from flask import Flask , render_template import random app = Flask ( __name__ ) # list of cat images images = [ \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif1.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif2.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif3.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif4.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif5.gif\" , \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif6.gif\" , ] @app . route ( '/' ) def index (): url = random . choice ( images ) return render_template ( 'index.html' , url = url ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" )","title":"app.py"},{"location":"1_3_docker_tp.html#requirementstxt","text":"In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file: flask typer","title":"requirements.txt"},{"location":"1_3_docker_tp.html#templatesindexhtml","text":"Create a directory called templates and create an index.html file in that directory with the following content in it: < html > < head > < style type = \"text/css\" > body { background : black ; color : white ; } div . container { max-width : 500 px ; margin : 100 px auto ; border : 20 px solid white ; padding : 10 px ; text-align : center ; } h4 { text-transform : uppercase ; } </ style > </ head > < body > < div class = \"container\" > < h4 > Cat Gif of the day </ h4 > < img src = \"{{url}}\" /> </ div > </ body > </ html >","title":"templates/index.html"},{"location":"1_3_docker_tp.html#232-write-a-dockerfile","text":"We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image . Since our application is written in Python, we will build our own Python image based on Alpine . We'll do that using a Dockerfile . A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles. Create a file called Dockerfile , and add content to it as described below. We'll start by specifying our base image, using the FROM keyword: FROM alpine:3.15 The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next: RUN apk add --update py3-pip Let's add the files that make up the Flask Application. Install all Python requirements for our app to run. This will be accomplished by adding the lines: COPY requirements.txt /usr/src/app/ RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt Copy the files you have created earlier into our image by using COPY command. COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ Specify the port number which needs to be exposed. Since our flask app is running on 5000 that's what we'll expose. EXPOSE 5000 The last step is the command for running the application which is simply - python ./app.py . Use the CMD command to do that: CMD [\"python3\", \"/usr/src/app/app.py\"] The primary purpose of CMD is to tell the container which command it should run by default when it is started. Verify your Dockerfile. Our Dockerfile is now ready. This is how it looks: # our base image FROM alpine:3.15 # Install python and pip RUN apk add --update py3-pip # install Python modules needed by the Python app COPY requirements.txt /usr/src/app/ RUN pip3 install --no-cache-dir -r /usr/src/app/requirements.txt # copy files required for the app to run COPY app.py /usr/src/app/ COPY templates/index.html /usr/src/app/templates/ # tell the port number the container should expose EXPOSE 5000 # run the application CMD [\"python3\", \"/usr/src/app/app.py\"]","title":"2.3.2 Write a Dockerfile"},{"location":"1_3_docker_tp.html#233-build-the-image","text":"Now that you have your Dockerfile , you can build your image. The docker build command does the heavy-lifting of creating a docker image from a Dockerfile . The docker build command is quite simple - it takes an optional tag name with the -t flag, and the location of the directory containing the Dockerfile - the . indicates the current directory: docker build -t myfirstapp:1.0 . $ docker build -t myfirstapp:1.0 . Sending build context to Docker daemon 9.728 kB Step 1 : FROM alpine:latest ---> 0d81fc72e790 Step 2 : RUN apk add --update py-pip ---> Running in 8abd4091b5f5 fetch http://dl-4.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz fetch http://dl-4.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz (1/12) Installing libbz2 (1.0.6-r4) (2/12) Installing expat (2.1.0-r2) (3/12) Installing libffi (3.2.1-r2) (4/12) Installing gdbm (1.11-r1) (5/12) Installing ncurses-terminfo-base (6.0-r6) (6/12) Installing ncurses-terminfo (6.0-r6) (7/12) Installing ncurses-libs (6.0-r6) (8/12) Installing readline (6.3.008-r4) (9/12) Installing sqlite-libs (3.9.2-r0) (10/12) Installing python (2.7.11-r3) (11/12) Installing py-setuptools (18.8-r0) (12/12) Installing py-pip (7.1.2-r0) Executing busybox-1.24.1-r7.trigger OK: 59 MiB in 23 packages ---> 976a232ac4ad Removing intermediate container 8abd4091b5f5 Step 3 : COPY requirements.txt /usr/src/app/ ---> 65b4be05340c Removing intermediate container 29ef53b58e0f Step 4 : RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt ---> Running in a1f26ded28e7 Collecting Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1)) Downloading Flask-0.10.1.tar.gz (544kB) Collecting Werkzeug>=0.7 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Werkzeug-0.11.4-py2.py3-none-any.whl (305kB) Collecting Jinja2>=2.4 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB) Collecting itsdangerous>=0.21 (from Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading itsdangerous-0.24.tar.gz (46kB) Collecting MarkupSafe (from Jinja2>=2.4->Flask==0.10.1->-r /usr/src/app/requirements.txt (line 1)) Downloading MarkupSafe-0.23.tar.gz Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask Running setup.py install for MarkupSafe Running setup.py install for itsdangerous Running setup.py install for Flask Successfully installed Flask-0.10.1 Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.4 itsdangerous-0.24 You are using pip version 7.1.2, however version 8.1.1 is available. You should consider upgrading via the 'pip install --upgrade pip' command. ---> 8de73b0730c2 Removing intermediate container a1f26ded28e7 Step 5 : COPY app.py /usr/src/app/ ---> 6a3436fca83e Removing intermediate container d51b81a8b698 Step 6 : COPY templates/index.html /usr/src/app/templates/ ---> 8098386bee99 Removing intermediate container b783d7646f83 Step 7 : EXPOSE 5000 ---> Running in 31401b7dea40 ---> 5e9988d87da7 Removing intermediate container 31401b7dea40 Step 8 : CMD python /usr/src/app/app.py ---> Running in 78e324d26576 ---> 2f7357a0805d Removing intermediate container 78e324d26576 Successfully built 2f7357a0805d If you don't have the alpine:3.5 image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image ( <YOUR_USERNAME>/myfirstapp ) shows.","title":"2.3.3 Build the image"},{"location":"1_3_docker_tp.html#234-run-your-image","text":"The next step in this section is to run the image and see if it actually works. $ docker run -p 8888 :5000 --name myfirstapp myfirstapp:1.0 * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) Head over to http://localhost:8888 and your app should be live. Note If you are using Docker Machine, you may need to open up another terminal and determine the container ip address using docker-machine ip default . Hit the Refresh button in the web browser to see a few more cat images.","title":"2.3.4 Run your image"},{"location":"1_3_docker_tp.html#235-dockerfile-commands-summary","text":"Here's a quick summary of the few basic commands we used in our Dockerfile. FROM starts the Dockerfile. It is a requirement that the Dockerfile must start with the FROM command. Images are created in layers, which means you can use another image as the base image for your own. The FROM command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format username/imagename:version . RUN is used to build up the Image you're creating. For each RUN command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a RUN instruction is to place the full text of the shell command after the RUN (e.g., RUN mkdir /user/local/foo ). This will automatically run in a /bin/sh shell. You can define a different shell like this: RUN /bin/bash -c 'mkdir /user/local/foo' COPY copies local files into the container. CMD defines the commands that will run on the Image at start-up. Unlike a RUN , this does not create a new layer for the Image, but simply runs the command. There can only be one CMD per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the CMD run a script. CMD requires that you tell it where to run the command, unlike RUN . So example CMD commands would be: CMD [\"python\", \"./app.py\"] CMD [\"/bin/bash\", \"echo\", \"Hello World\"] EXPOSE creates a hint for users of an image which ports provide services. It is included in the information which can be retrieved via $ docker inspect <container-id> . Note: The EXPOSE command does not actually make any ports accessible to the host! Instead, this requires publishing ports by means of the -p flag when using $ docker run . PUSH pushes your image to Docker Cloud, or alternately to a private registry Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles .","title":"2.3.5 Dockerfile commands summary"},{"location":"1_3_docker_tp.html#3-running-cli-apps-packaged-in-docker-while-mounting-volumes","text":"Instead of serving web app you can also run applications, like command line interfaces or training scripts, packaged in docker. It is very useful to deliver packaged apps with specific installation to other users. This is often used when you want to package your machine learning environment to run training in distributed fashion. Usually you just edit config files in an external editor and pass it to the docker Let's modify the app.py in 2. with the following import typer from typing import Optional from pathlib import Path app = typer . Typer () @app . command () def hello ( name : str ): typer . echo ( f \"Hello { name } \" ) @app . command () def run_config ( config : Optional [ Path ] = typer . Option ( None )): if config is None : typer . echo ( \"No config file\" ) raise typer . Abort () if config . is_file (): text = config . read_text () typer . echo ( f \"Config file contents: \\n { text } \" ) elif config . is_dir (): typer . echo ( \"Config is a directory, will use all its config files\" ) elif not config . exists (): typer . echo ( \"The config doesn't exist\" ) if __name__ == \"__main__\" : app () With your terminal you would call it using python app.py hello {my name} or python app.py run-config --config-file {my config} Modify the dockerfile : Replace CMD [\"python3\", \"/usr/src/app/app.py\"] By ENTRYPOINT [\"python3\", \"/usr/src/app/app.py\"] Rebuild your docker image (maybe git it another name) Now to run the CLI you just have to docker run --rm {your image} {your args} . Try it with docker run {...} hello {your name} In order to pass a config file, or data to your docker, you need to make it available to your docker. To do that, we have to mount volumes Create a dummy config file ( config.txt ) in another folder (ex: config/ ) then mount it when you run the docker: docker run --rm \\ -v /home/${USER}/configs:/home/configs \\ --workdir /home/ \\ {your image} \\ run-config --config-file {path to your config in DOCKER, eg /home/configs/config.txt} Note that since you mounted volumes, you must pass the path in the docker to your config file for it to work","title":"3. Running CLI apps packaged in docker while mounting volumes"},{"location":"1_3_docker_tp.html#4-containers-registry","text":"Remember Container Registries ? Here as some explainers The main container registry is dockerhub, https://hub.docker.com/ All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before Example, the Python Image Google Cloud has a Container Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only. However, it requires naming the image in a specific fashion: eu.gcr.io/${PROJECT_ID}/name:tag Use the docker cli to tag your previous myfirstapp image to the right namespace docker tag myfirstapp eu.gcr.io/{PROJECT_ID}/{a-unique-name-describing-your-app}:1.0 Upload it on container registry docker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG] If you have a problem of authentification, gcloud auth configure-docker Hint to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Go to container registry https://console.cloud.google.com/gcr, you should see your docker image :)","title":"4. Containers Registry"},{"location":"1_3_docker_tp.html#5-data-science-standardized-environment","text":"","title":"5.  Data Science Standardized Environment"},{"location":"1_3_docker_tp.html#51-intro","text":"Those of us who work on a team know how hard it is to create a standardize development environment. Or if you have ever updated a dependency and had everything break, you understand the importance of keeping development environments isolated. Using Docker, we can create a project / team image with our development environment and mount a volume with our notebooks and data. The benefits of this workflow are that we can: Separate out projects Spin up a container to onboard new employees Build an automated testing pipeline to confirm upgrade dependencies do not break code","title":"5.1 Intro"},{"location":"1_3_docker_tp.html#52-kaggle-docker-image","text":"For this exercise we will use Kaggle Docker Image which is a fully configured docker image that can be used as a data science container Take a look at the documentation and the repository","title":"5.2 Kaggle Docker Image"},{"location":"1_3_docker_tp.html#53-get-the-algorithm-in-ml-git-in-your-virtual-machine","text":"From your vm, run git clone https://github.com/erachelson/MLclass.git , this should setup your AML class inside your VM","title":"5.3 Get the algorithm in ML git in your Virtual Machine"},{"location":"1_3_docker_tp.html#54-mounting-volumes-and-ports","text":"Now let's run the image. This container has a jupyter notebook accessible from port 8080 so we will need to map the host port 8888 (the one accessible from the ssh tunnel) to the docker port 8080, we will use port forwarding We will also need to make available the notebooks on the VM to the container... we will mount volumes . Your data is located in /home/${USER}/MLClass and we want to miunt it in /tmp/workdir docker run --rm -it \\ -p 8888 :8080 \\ -v /home/ ${ USER } /MLclass:/home/Mlclass \\ --workdir /home/ \\ gcr.io/kaggle-images/python \\ /bin/bash /run_jupyter.sh Note: this image is very large ! Options breakdown: * --rm remove the container when we stop it * -it run the container in interactive mode * -p forward port from host:container * other: options from the kaggle container You should now see a jupyter lab with mlclass accessible if you connect your browser (in your laptop) to port 8888 (localhost:8888) So basically, we mapped the ports local 8888 to vm 8888 and vm 8888 to docker 8080","title":"5.4 Mounting volumes and ports"},{"location":"1_3_docker_tp.html#6-bonus-using-google-cloud-tools-for-docker","text":"Using cloud shell you should be able to do the Hello World Dockerfile exercise except that instead of using docker build you use Google Cloud Build Tutorial: https://cloud.google.com/cloud-build/docs/quickstart-docker Example command : gcloud builds submit --tag eu.gcr.io/$PROJECT_ID/{image}:{tag} . Help to get your project id: PROJECT_ID=$(gcloud config get-value project 2> /dev/null) Example Try to build the hello world app","title":"6. Bonus - Using Google Cloud Tools for Docker"},{"location":"1_3_docker_tp.html#7-bonus-docker-compose","text":"https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6 https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md","title":"7. Bonus - Docker Compose"},{"location":"1_3_docker_tp.html#8-bonus-going-further","text":"https://container.training/","title":"8. Bonus - Going further"},{"location":"1_4_be.html","text":"Bureau d'\u00e9tudes Cloud & Docker \ud83d\udd17 Objectives of this BE \ud83d\udd17 This Bureau d'\u00e9tudes (BE, for short) will guide you through the essential notions to be able to manipulate with regard to cloud computer and docker, We will illustrate the following: Work in a remote environment (inside a VM, using google cloud shell) Creation and ssh connection to virtual machine instances Usage of managed storage capabilities Creating your own docker images Exchanging docker images through a Container Registry Pulling and running docker images created by your teammates In particular, this workflow: Warning Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions. How to run this BE \ud83d\udd17 There are two ways of interacting with google cloud platform, Locally with the google-cloud-sdk (eduroam, 4G network & gcloud installed) With the web-based Google Coud Shell We will be using the gcloud CLI in Google Cloud Shell or the gcloud sdk for the following: Create a GCE Virtual Machine Connect to SSH with port forwarding to said machine The rest will be done from the virtual machine except web preview that you can do from your browser Warning If you are on ISAE-EDU, using the google-cloud-sdk will not work, you will have to use your web browser exclusively For the rest of this walkthrough, if it is written \"from your local machine\", this will be either your web browser, or your laptop's terminal or google cloud shell If it is written \"inside the VM\", this means that you have to run the SSH tunnel first... \ud83d\ude4f\ud83c\udffb Use Google Chrome without any ad blockers for console.cloud.google.com if you have any issues Warning \u26a0\ufe0f ISAE-EDU is tricky and may prevent you from correctly connecting \u26a0\ufe0f eduroam works best with ssh connections \u26a0\ufe0f Remember that you may be often disconnected of cloud shell so using the cloud sdk locally is often easier :) Team composition \ud83d\udd17 You should be in team of 5, however this will work with a minimum of 2 people. Designate a \"project manager\" (the person who is the most comfortable with the google cloud platform UI). She or He will have the hard task of giving access to his/her GCP project to the other team members to enable collaboration. This means that the project of the \"team leader\" will be billed a little more for the duration of this BE, so please be kind with the project and apply good cloud hygiene :) Each team member picks a different cute mascot and remembers it: \ud83d\udc08 cat \ud83d\udc15 dog \ud83d\udc7d (baby) yoda \ud83e\udd89 owl \ud83d\udc3c panda Find a groupname, because you will need it for the next steps 1 - Get the necessary resources from Google Cloud Storage \ud83d\udd17 From your google cloud shell , The resources are located at the URI gs://fchouteau-isae-cloud/be/${MASCOT} , Your ${MASCOT} name is either: cat dog owl panda yoda I advise you to export MASCOT=.... to remember it :) ONLY DOWNLOAD your mascot resources (no cheating ! this will only cause confusion later) Download them to your instance using the gcloud cli (refer to your previous work for more information) Hint gsutil -m cp -r { source } { destination } Remember that google storage URIs always begin with gs:// Go to ( cd ) the folder where you downloaded your resources You should see a file structure like this fchouteau@be-cloud-mascot:~/be$ tree yoda -L 2 yoda \u251c\u2500\u2500 app.py \u251c\u2500\u2500 AUTHOR.txt \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 favicon.ico \u251c\u2500\u2500 imgs \u2502 \u251c\u2500\u2500 1.gif \u2502 \u251c\u2500\u2500 2.gif \u2502 \u251c\u2500\u2500 3.gif \u2502 \u251c\u2500\u2500 4.gif \u2502 \u2514\u2500\u2500 5.gif \u2514\u2500\u2500 template.html.jinja2 1 directory, 10 files 2 - Build your docker image \ud83d\udd17 Question Look at the Dockerfile ( cat Dockerfile ), what does it seem to do ? Look at app.py ( cat app.py ). What is Flask ? What does it seem to do ? Edit the file AUTHOR.txt to add your name instead of the placeholder Refer to your previous work to build the image Danger On which port is your flask app running ? ( cat Dockerfile ) Note it carefully ! You will need to communicate it to your teammate :) Hint To edit a file, it's easier if you are on jupyter lab... Otherwise you can still use nano or vim Here a VIM tutorial https://www.openvim.com/ , good luck with that Otherwise it's simple (or not): vim {file}, switch to edit mode (type :edit ), edit your line, ESCAPE then exit VIM like this :wq! Nano is easier ! https://linuxize.com/post/how-to-use-nano-text-editor/ When building the image, name it appropriately... like eu.gcr.io/${PROJECT_ID}/webapp-gif:${GROUPNAME}-${MASCOT}-1.0 ! Hint to get your project id: PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) now if you list your images you should see it ! REPOSITORY TAG IMAGE ID CREATED SIZE eu.gcr.io/{your project}/{your-app} 1.0 d1c5993848bf 2 minutes ago 62.1MB Question Describe concisely (on slack) to your past self (before the pandemic) what is a Docker Image 3 - Push your Docker image in the Container Registry \ud83d\udd17 Now push your image on the shared container registry Help your team mates so that everybody can build his/her Docker Image Question Describe succintly (on slack) to your past self (before the pandemic) what is a Container Registry In the end, things should look like this 4 - Create Google Compute Engine VM \ud83d\udd17 Each team member creates a separate machine on the same project , Here, you will create a Google Compute Engine instance, preconfigured with everything you need, If you use the google cloud CLI (either your local google cloud sdk or google cloud shell), you can use this First, set a variable with the name of your instance, export INSTANCE_NAME = \"be-cloud-mascot-{yourgroup}-{yourname}\" # Don't forget to replace values ! Then create your VM gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-1\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 75GB If you have an issue with quota, use any of europe-west4-{a,b,c,d} or europe-west1-{b,c,d} or europe-2-{a,b,c,d} as a zone If you use the web interface, follow this Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a Virtual Machine and what is Google Compute Engine 5 - Connect using SSH to the instance \ud83d\udd17 Option A with google cloud sdk installed locally \ud83d\udd17 If you are using the google cloud sdk in your computer, you can connect to ssh using the usual command (refer to the first hands-on ) with SSH Tunneling. FIRST CONNECT TO EDUROAM as eduroam allows connecting to ssh. Tunnel the following ports to your local machine: 8080: This is reserved for a jupyter lab session by default, it makes it easy to see & edit text 8081: You will neeed to run containers and expose them on a port Hint gcloud compute ssh { user } @ { instance } -- \\ -L { client-port } :localhost: { server-port } \\ -L { client-port-2 } :localhost: { server-port-2 } Go to your browser and connect to http://localhost:8080, you should be in a jupyter lab where you can access a terminal, a text editor etc... Option B with the web browser (& google cloud shell) \ud83d\udd17 If you can't SSH using the command line you will need to either 1 (not recommended) Use the in-browser SSH capabilities to connect to your instance (but that does not allow port forwarding so your life will be harder later on) 2 (recommended) Make a SSH tunnel between Google Cloud Shell and your VM with port forwarding AND use the web preview on port 8080 to access jupyter lab (and the web preview on port 8081 later on) Go to shell.cloud.google.com and do an SSH tunnel like described above, then use the web preview feature on port 8080 Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a SSH Tunnel and what is port forwarding I recommend option 2 6 - Pull Docker Images from your teammates \ud83d\udd17 You should be inside the your VM, Question How to check that you're inside your VM ? On your terminal you should see user@hostname at the beginning. Hostname should be the name of your VM Select another mascot and pull the corresponding docker image from the registry List the docker images you have. You should have at least 2 including yours 7 - Run Docker Containers from their Docker Images \ud83d\udd17 Run your container while mapping the correct port to your VM 8081 . Which port is it ? Well, ask the one who built the image. When running the container, setup the USER environment variable to your name ! Hint the port is not the same as yours if you don't set the username, it will show later ;) 9 - Display the results & share them \ud83d\udd17 You just launched a webapp on the port 8081 of your remote instance. If you have a ssh tunnel directly from your laptop, ensure that you made a tunnel for your port 8081 to any port of your machine then, go to http://localhost:(your port) inside your browser. The resulting webpage should appear If you are using google cloud shell, open web preview on port 8081 (you should have a tunnel running between your google cloud shell and your instance) The 8081 port has been opened to the internet, you can also connect to your machine's public ip address on the port 80801 (http//{ip}:80801) and you should see it. How to get your public IP ? Go to the GCP interface and you can find it, or run gcloud compute instances list | grep {your instance name} Success The webpage should show the mascot your chose to run The webpage should show the name of the author (not you) The webpage should show your name Bug If any of the three item above are missing, find the bug and solve it :) Example Try to refresh the webpage to make more gifs appear Share your result on slack 10. Cleanup the GCP project \ud83d\udd17 Remove your VMs (DELETE them) Remove images from the container registry Success \ud83c\udf89 you have successfully finished the BE. You know how to manipulate the basic notions around cloud computing and docker so that you won't be completely lost when someone will talk about it If you have time left on your hands, have a look at the next class, do the hands-on, and if you really have finished everything, read the kubernetes comic and do the Kubernetes hands-on !","title":"Google Cloud and Docker BE"},{"location":"1_4_be.html#bureau-detudes-cloud-docker","text":"","title":"Bureau d'\u00e9tudes Cloud &amp; Docker"},{"location":"1_4_be.html#objectives-of-this-be","text":"This Bureau d'\u00e9tudes (BE, for short) will guide you through the essential notions to be able to manipulate with regard to cloud computer and docker, We will illustrate the following: Work in a remote environment (inside a VM, using google cloud shell) Creation and ssh connection to virtual machine instances Usage of managed storage capabilities Creating your own docker images Exchanging docker images through a Container Registry Pulling and running docker images created by your teammates In particular, this workflow: Warning Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions.","title":"Objectives of this BE"},{"location":"1_4_be.html#how-to-run-this-be","text":"There are two ways of interacting with google cloud platform, Locally with the google-cloud-sdk (eduroam, 4G network & gcloud installed) With the web-based Google Coud Shell We will be using the gcloud CLI in Google Cloud Shell or the gcloud sdk for the following: Create a GCE Virtual Machine Connect to SSH with port forwarding to said machine The rest will be done from the virtual machine except web preview that you can do from your browser Warning If you are on ISAE-EDU, using the google-cloud-sdk will not work, you will have to use your web browser exclusively For the rest of this walkthrough, if it is written \"from your local machine\", this will be either your web browser, or your laptop's terminal or google cloud shell If it is written \"inside the VM\", this means that you have to run the SSH tunnel first... \ud83d\ude4f\ud83c\udffb Use Google Chrome without any ad blockers for console.cloud.google.com if you have any issues Warning \u26a0\ufe0f ISAE-EDU is tricky and may prevent you from correctly connecting \u26a0\ufe0f eduroam works best with ssh connections \u26a0\ufe0f Remember that you may be often disconnected of cloud shell so using the cloud sdk locally is often easier :)","title":"How to run this BE"},{"location":"1_4_be.html#team-composition","text":"You should be in team of 5, however this will work with a minimum of 2 people. Designate a \"project manager\" (the person who is the most comfortable with the google cloud platform UI). She or He will have the hard task of giving access to his/her GCP project to the other team members to enable collaboration. This means that the project of the \"team leader\" will be billed a little more for the duration of this BE, so please be kind with the project and apply good cloud hygiene :) Each team member picks a different cute mascot and remembers it: \ud83d\udc08 cat \ud83d\udc15 dog \ud83d\udc7d (baby) yoda \ud83e\udd89 owl \ud83d\udc3c panda Find a groupname, because you will need it for the next steps","title":"Team composition"},{"location":"1_4_be.html#1-get-the-necessary-resources-from-google-cloud-storage","text":"From your google cloud shell , The resources are located at the URI gs://fchouteau-isae-cloud/be/${MASCOT} , Your ${MASCOT} name is either: cat dog owl panda yoda I advise you to export MASCOT=.... to remember it :) ONLY DOWNLOAD your mascot resources (no cheating ! this will only cause confusion later) Download them to your instance using the gcloud cli (refer to your previous work for more information) Hint gsutil -m cp -r { source } { destination } Remember that google storage URIs always begin with gs:// Go to ( cd ) the folder where you downloaded your resources You should see a file structure like this fchouteau@be-cloud-mascot:~/be$ tree yoda -L 2 yoda \u251c\u2500\u2500 app.py \u251c\u2500\u2500 AUTHOR.txt \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 favicon.ico \u251c\u2500\u2500 imgs \u2502 \u251c\u2500\u2500 1.gif \u2502 \u251c\u2500\u2500 2.gif \u2502 \u251c\u2500\u2500 3.gif \u2502 \u251c\u2500\u2500 4.gif \u2502 \u2514\u2500\u2500 5.gif \u2514\u2500\u2500 template.html.jinja2 1 directory, 10 files","title":"1 - Get the necessary resources from Google Cloud Storage"},{"location":"1_4_be.html#2-build-your-docker-image","text":"Question Look at the Dockerfile ( cat Dockerfile ), what does it seem to do ? Look at app.py ( cat app.py ). What is Flask ? What does it seem to do ? Edit the file AUTHOR.txt to add your name instead of the placeholder Refer to your previous work to build the image Danger On which port is your flask app running ? ( cat Dockerfile ) Note it carefully ! You will need to communicate it to your teammate :) Hint To edit a file, it's easier if you are on jupyter lab... Otherwise you can still use nano or vim Here a VIM tutorial https://www.openvim.com/ , good luck with that Otherwise it's simple (or not): vim {file}, switch to edit mode (type :edit ), edit your line, ESCAPE then exit VIM like this :wq! Nano is easier ! https://linuxize.com/post/how-to-use-nano-text-editor/ When building the image, name it appropriately... like eu.gcr.io/${PROJECT_ID}/webapp-gif:${GROUPNAME}-${MASCOT}-1.0 ! Hint to get your project id: PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) now if you list your images you should see it ! REPOSITORY TAG IMAGE ID CREATED SIZE eu.gcr.io/{your project}/{your-app} 1.0 d1c5993848bf 2 minutes ago 62.1MB Question Describe concisely (on slack) to your past self (before the pandemic) what is a Docker Image","title":"2 - Build your docker image"},{"location":"1_4_be.html#3-push-your-docker-image-in-the-container-registry","text":"Now push your image on the shared container registry Help your team mates so that everybody can build his/her Docker Image Question Describe succintly (on slack) to your past self (before the pandemic) what is a Container Registry In the end, things should look like this","title":"3 - Push your Docker image in the Container Registry"},{"location":"1_4_be.html#4-create-google-compute-engine-vm","text":"Each team member creates a separate machine on the same project , Here, you will create a Google Compute Engine instance, preconfigured with everything you need, If you use the google cloud CLI (either your local google cloud sdk or google cloud shell), you can use this First, set a variable with the name of your instance, export INSTANCE_NAME = \"be-cloud-mascot-{yourgroup}-{yourname}\" # Don't forget to replace values ! Then create your VM gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-1\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 75GB If you have an issue with quota, use any of europe-west4-{a,b,c,d} or europe-west1-{b,c,d} or europe-2-{a,b,c,d} as a zone If you use the web interface, follow this Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a Virtual Machine and what is Google Compute Engine","title":"4 - Create Google Compute Engine VM"},{"location":"1_4_be.html#5-connect-using-ssh-to-the-instance","text":"","title":"5 - Connect using SSH to the instance"},{"location":"1_4_be.html#option-a-with-google-cloud-sdk-installed-locally","text":"If you are using the google cloud sdk in your computer, you can connect to ssh using the usual command (refer to the first hands-on ) with SSH Tunneling. FIRST CONNECT TO EDUROAM as eduroam allows connecting to ssh. Tunnel the following ports to your local machine: 8080: This is reserved for a jupyter lab session by default, it makes it easy to see & edit text 8081: You will neeed to run containers and expose them on a port Hint gcloud compute ssh { user } @ { instance } -- \\ -L { client-port } :localhost: { server-port } \\ -L { client-port-2 } :localhost: { server-port-2 } Go to your browser and connect to http://localhost:8080, you should be in a jupyter lab where you can access a terminal, a text editor etc...","title":"Option A with google cloud sdk installed locally"},{"location":"1_4_be.html#option-b-with-the-web-browser-google-cloud-shell","text":"If you can't SSH using the command line you will need to either 1 (not recommended) Use the in-browser SSH capabilities to connect to your instance (but that does not allow port forwarding so your life will be harder later on) 2 (recommended) Make a SSH tunnel between Google Cloud Shell and your VM with port forwarding AND use the web preview on port 8080 to access jupyter lab (and the web preview on port 8081 later on) Go to shell.cloud.google.com and do an SSH tunnel like described above, then use the web preview feature on port 8080 Your browser does not support the video tag. Question Describe concisely (on slack) to your past self (before the pandemic) what is a SSH Tunnel and what is port forwarding I recommend option 2","title":"Option B with the web browser (&amp; google cloud shell)"},{"location":"1_4_be.html#6-pull-docker-images-from-your-teammates","text":"You should be inside the your VM, Question How to check that you're inside your VM ? On your terminal you should see user@hostname at the beginning. Hostname should be the name of your VM Select another mascot and pull the corresponding docker image from the registry List the docker images you have. You should have at least 2 including yours","title":"6 - Pull Docker Images from your teammates"},{"location":"1_4_be.html#7-run-docker-containers-from-their-docker-images","text":"Run your container while mapping the correct port to your VM 8081 . Which port is it ? Well, ask the one who built the image. When running the container, setup the USER environment variable to your name ! Hint the port is not the same as yours if you don't set the username, it will show later ;)","title":"7 - Run Docker Containers from their Docker Images"},{"location":"1_4_be.html#9-display-the-results-share-them","text":"You just launched a webapp on the port 8081 of your remote instance. If you have a ssh tunnel directly from your laptop, ensure that you made a tunnel for your port 8081 to any port of your machine then, go to http://localhost:(your port) inside your browser. The resulting webpage should appear If you are using google cloud shell, open web preview on port 8081 (you should have a tunnel running between your google cloud shell and your instance) The 8081 port has been opened to the internet, you can also connect to your machine's public ip address on the port 80801 (http//{ip}:80801) and you should see it. How to get your public IP ? Go to the GCP interface and you can find it, or run gcloud compute instances list | grep {your instance name} Success The webpage should show the mascot your chose to run The webpage should show the name of the author (not you) The webpage should show your name Bug If any of the three item above are missing, find the bug and solve it :) Example Try to refresh the webpage to make more gifs appear Share your result on slack","title":"9 - Display the results &amp; share them"},{"location":"1_4_be.html#10-cleanup-the-gcp-project","text":"Remove your VMs (DELETE them) Remove images from the container registry Success \ud83c\udf89 you have successfully finished the BE. You know how to manipulate the basic notions around cloud computing and docker so that you won't be completely lost when someone will talk about it If you have time left on your hands, have a look at the next class, do the hands-on, and if you really have finished everything, read the kubernetes comic and do the Kubernetes hands-on !","title":"10. Cleanup the GCP project"},{"location":"1_5_deployment.html","text":"Deployment \ud83d\udd17 Link to slides Link to slides","title":"Lectures"},{"location":"1_5_deployment.html#deployment","text":"Link to slides Link to slides","title":"Deployment"},{"location":"1_5_deployment_tp.html","text":"Deployment : Deploy your ML model in production \ud83d\udd17 Objectives \ud83d\udd17 Your first ML model in production ! A model behind a Restful API, packaged in a docker A frontend using streamlit, packaged in a docker Deploy it on Google Cloud Platform using GCE & docker-compose Send it to your friends ! Regardons ce notebook Il effectue les op\u00e9rations suivantes: Chargement d'un mod\u00e8le Chargement d'une image D\u00e9tection des \"objets\" sur l'image Dessin des d\u00e9tections sur l'image Affichage L'objectif est de convertir ce notebook en deux applications : L'une qui \"sert\" les pr\u00e9dictions d'un mod\u00e8le (le serveur) L'une qui permet \u00e0 un utilisateur d'interagir facilement avec le mod\u00e8le en mettant en ligne sa propre image (le \"client\") Puis de les d\u00e9ployer sur une instance GCP Team Composition \ud83d\udd17 C'est mieux d'\u00eatre en bin\u00f4me pour s'entraider :) How to run this \ud83d\udd17 Commen\u00e7ons par cr\u00e9er une instance GCP bien configur\u00e9e depuis laquelle se connecter: export INSTANCE_NAME = \"tp-deployment-{yourgroup}-{yourname}\" # Don't forget to replace values ! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-1\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 75GB R\u00e9cuperez l'ip publique de la machine (via l'interface google cloud ou bien en faisant gcloud compute instances list | grep {votre instance} et notez l\u00e0 bien Depuis le google cloud shell - ou depuis votre machine si vous avez google cloud sdk d'install\u00e9 localement et que vous \u00eates sous eduroam, gcloud compute ssh { user } @ { instance } -- \\ -L 8080 :localhost:8080 \\ -L 8080 :localhost:8080 Vous pouvez ensuite aller sur localhost:8080 (install locale) ou faire un web preview depuis cloud shell sur le port 8080 de cloud shell, Vous devriez \u00eatre dans un jupyter lab Maintenant, depuis ce jupyter lab, ouvrez un terminal et r\u00e9cup\u00e9rez les fichiers suivants : gsutil cp -r gs://fchouteau-isae-cloud/deployment/* . 1 - Converting a prediction notebook into a webapplication \ud83d\udd17 Placez vous dans le dossier model nouvellement cr\u00e9\u00e9 Objectif \ud83d\udd17 Packager un mod\u00e8le de machine learning derri\u00e8re une webapplication pour pouvoir la d\u00e9ployer sur le web et servir des pr\u00e9dictions \u00e0 des utilisateurs Le mod\u00e8le: Un d\u00e9tecteur d'objets sur des photographies \"standard\" suppos\u00e9 marcher en temps r\u00e9el, qui sort des \"bounding boxes\" autour des objets d\u00e9tect\u00e9 dans des images Le papier vaut la lecture https://pjreddie.com/media/files/papers/YOLOv3.pdf On r\u00e9cup\u00e8re la version disponible sur torchhub https://pytorch.org/hub/ultralytics_yolov5/ qui correspond \u00e0 ceci https://github.com/ultralytics/yolov5 Voici une petite explication de l'historique https://medium.com/towards-artificial-intelligence/yolo-v5-is-here-custom-object-detection-tutorial-with-yolo-v5-12666ee1774e On se propose ici de wrapper 3 versions du mod\u00e8le (S,M,L) qui sont 3 versions +/- complexes du mod\u00e8le YOLO-V5, afin de pouvoir comparer les performances et les r\u00e9sultats D\u00e9roulement \ud83d\udd17 Transformer un notebook de pr\u00e9diction en \u201cWebApp\u201d en remplissant app.stub.py et en le renommant en app.py Packager l'application sous forme d'une image docker Tester son image docker localement Uploader le docker sur Google Container Registry D\u00e9veloppement de app.py \ud83d\udd17 Regardons le app.stub.py (que l'on renommera en app.py ) import base64 import io import time from typing import List , Dict import numpy as np import torch from PIL import Image from fastapi import FastAPI , HTTPException from pydantic import BaseModel class Input ( BaseModel ): model : str image : str class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str # !!!! FILL ME def parse_predictions ( prediction : np . ndarray , classes : [ str ]) -> List [ Detection ]: raise NotImplementedError # !!!! FILL ME def load_model ( model_name : str ): \"\"\"\"\"\" raise NotImplementedError MODEL_NAMES = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ] app = FastAPI ( title = \"NAME ME\" , description = \"\"\" DESCRIBE ME \"\"\" , version = \"1.0\" , ) # !!!! FILL ME # This is a dictionnary that must contains a model for each key (model names), fill load model # example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name) # You can also lazily load models only when they are called to avoid holding 3 models in memory MODELS = ... @app . get ( \"/\" , description = \"return the title\" , response_description = \"FILL ME\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/health\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = List [ str ]) def models () -> [ str ]: return MODEL_NAMES @app . post ( \"/predict\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = Result ) def predict ( inputs : Input ) -> Result : # get correct model model_name = inputs . model if model_name not in MODEL_NAMES : raise HTTPException ( status_code = 400 , detail = \"wrong model name, choose between {} \" . format ( MODEL_NAMES )) # Get the model from the list of available models model = MODELS . get ( model_name ) # Get & Decode image try : image = inputs . image . encode ( \"utf-8\" ) image = base64 . b64decode ( image ) image = Image . open ( io . BytesIO ( image )) except : raise HTTPException ( status_code = 400 , detail = \"File is not an image\" ) # Convert from RGBA to RGB *to avoid alpha channels* if image . mode == \"RGBA\" : image = image . convert ( \"RGB\" ) # Inference # RUN THE PREDICTION, TIME IT predictions = ... # Post processing classes = predictions . names predictions = predictions . xyxy [ 0 ] . numpy () # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method detections = ... result = Result ( detections =... , time =... , model =... ) return result Dans un premier temps, vous pouvez remplir la description des \"routes\" (i.e. des fonctions de l'application): @app . get ( \"/\" , description = \"return the title\" , response_description = \"FILL ME\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/health\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = List [ str ]) def models () -> [ str ]: return MODEL_NAMES Il y a deux fonctions \u00e0 compl\u00e9ter en s'inspirant du notebook inference.ipynb . Grace au typage de python, vous avez les types d'entr\u00e9e et de sortie des deux fonctions La premi\u00e8re prend un tableau de type (left, top, right, bottom, confidence, class_index) et une liste de noms de classes et cr\u00e9\u00e9e une liste d'objets Detection (voir le code pour la cr\u00e9ation des objets d\u00e9tection) La seconde fonction doit charger un mod\u00e8le via torchhub en fonction de son nom (voir le docker) # !!!! FILL ME def parse_predictions ( predictions : np . ndarray , classes : [ str ]) -> List [ Detection ]: raise NotImplementedError Hint def parse_prediction ( prediction : np . ndarray , classes : [ str ]) -> Detection : x0 , y0 , x1 , y1 , cnf , cls = prediction detection = Detection ( x_min = int ( x0 ), y_min = int ( y0 ), x_max = int ( x1 ), y_max = int ( y1 ), confidence = round ( float ( cnf ), 3 ), class_name = classes [ int ( cls )], ) return detection # !!!! FILL ME def load_model ( model_name : str ): \"\"\"\"\"\" raise NotImplementedError Hint def load_model ( model_name : str ) -> Dict : # Load model from torch model = torch . hub . load ( \"ultralytics/yolov5\" , model_name , pretrained = True ) # Evaluation mode + Non maximum threshold model = model . eval () return model Ensuite, vous pouvez executer les fonctions de chargement de mod\u00e8le, par exemple # !!!! FILL ME # This is a dictionnary that must contains a model for each key (model names), fill load model # example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name) # You can also lazily load models only when they are called to avoid holding 3 models in memory MODELS = {} for model_name in MODEL_NAMES : MODELS [ model_name ] = load_model ( model_name ) Enfin, il s'agit d'\u00e9crire un code qui effectue une pr\u00e9diction \u00e0 partir d'une image PIL et de mesurer le temps (indice: import time et t0 = time.time() ...) de pr\u00e9diction # RUN THE PREDICTION, TIME IT predictions = ... # Post processing classes = predictions . names predictions = predictions . xyxy [ 0 ] . numpy () Le r\u00e9sultat de predictions est un tableau numpy compos\u00e9 des colonnes left, top, right, bottom, confidence, class_index Il s'agit ensuite de transformer ces predictions en [Detection] # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method detections = parse_predictions ( predictions , classes ) Hint # Inference t0 = time . time () predictions = model ( image , size = 640 ) # includes NMS t1 = time . time () classes = predictions . names # Post processing predictions = predictions . xyxy [ 0 ] . numpy () detections = [ parse_prediction ( prediction = pred , classes = classes ) for pred in predictions ] result = Result ( detections = detections , time = round ( t1 - t0 , 3 ), model = model_name ) Correction \ud83d\udd17 app.py Hint import base64 import io import time from typing import List , Dict import numpy as np import torch from PIL import Image from fastapi import FastAPI , HTTPException from pydantic import BaseModel class Input ( BaseModel ): model : str image : str class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str def parse_prediction ( prediction : np . ndarray , classes : [ str ]) -> Detection : x0 , y0 , x1 , y1 , cnf , cls = prediction detection = Detection ( x_min = int ( x0 ), y_min = int ( y0 ), x_max = int ( x1 ), y_max = int ( y1 ), confidence = round ( float ( cnf ), 3 ), class_name = classes [ int ( cls )], ) return detection def load_model ( model_name : str ) -> Dict : # Load model from torch model = torch . hub . load ( \"ultralytics/yolov5\" , model_name , pretrained = True ) # Evaluation mode + Non maximum threshold model = model . eval () return model # %% app = FastAPI ( title = \"YOLO-V5 WebApp created with FastAPI\" , description = \"\"\" Wraps 3 different yolo-v5 models under the same RESTful API \"\"\" , version = \"1.1\" , ) # %% MODEL_NAMES = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ] MODELS = {} @app . get ( \"/\" , description = \"return the title\" , response_description = \"title\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"return the description\" , response_description = \"description\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/version\" , description = \"return the version\" , response_description = \"version\" , response_model = str ) def describe () -> str : return app . version @app . get ( \"/health\" , description = \"return whether it's alive\" , response_description = \"alive\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"Query the list of models\" , response_description = \"A list of available models\" , response_model = List [ str ], ) def models () -> [ str ]: return MODEL_NAMES @app . post ( \"/predict\" , description = \"Send a base64 encoded image + the model name, get detections\" , response_description = \"Detections + Processing time\" , response_model = Result , ) def predict ( inputs : Input ) -> Result : global MODELS # get correct model model_name = inputs . model if model_name not in MODEL_NAMES : raise HTTPException ( status_code = 400 , detail = \"wrong model name, choose between {} \" . format ( MODEL_NAMES )) # check load if MODELS . get ( model_name ) is None : MODELS [ model_name ] = load_model ( model_name ) model = MODELS . get ( model_name ) # Get Image # Decode image try : image = inputs . image . encode ( \"utf-8\" ) image = base64 . b64decode ( image ) image = Image . open ( io . BytesIO ( image )) except : raise HTTPException ( status_code = 400 , detail = \"File is not an image\" ) # Convert from RGBA to RGB *to avoid alpha channels* if image . mode == \"RGBA\" : image = image . convert ( \"RGB\" ) # Inference t0 = time . time () predictions = model ( image , size = 640 ) # includes NMS t1 = time . time () classes = predictions . names # Post processing predictions = predictions . xyxy [ 0 ] . numpy () detections = [ parse_prediction ( prediction = pred , classes = classes ) for pred in predictions ] result = Result ( detections = detections , time = round ( t1 - t0 , 3 ), model = model_name ) return result Construire le docker \ud83d\udd17 PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker build -t eu.gcr.io/ ${ PROJECT_ID } / { you rname }{ your app name } : { your version } -f Dockerfile . Tester le docker \ud83d\udd17 Vous pouvez lancer le docker localement et le tester avec le notebook PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker run --rm -p 8000 :8000 eu.gcr.io/ ${ PROJECT_ID } / { your-name } - { your app name } : { your version } Vous pouvez vous connecter \u00e0 votre appli via son ip publique sur le port 8000 depuis votre navigateur local http://{ip}:8000 Essayez quelques routes : /models /docs Pusher le docker sur google container registry \ud83d\udd17 gcloud auth configure-docker docker push eu.gcr.io/ ${ PROJECT_ID } / { your-name } - { your app name } : { your version } Si vous devez mettre \u00e0 jour le docker, il faut incr\u00e9menter la version pour le d\u00e9ploiement Liens Utiles \ud83d\udd17 https://fastapi.tiangolo.com/ https://requests.readthedocs.io/en/master/ https://testdriven.io/blog/fastapi-streamlit/ 2 - Making a companion application \ud83d\udd17 Allez dans le dossier streamlit Objectif \ud83d\udd17 Cr\u00e9er une application \"compagnon\" qui permet de faire des requ\u00eates \u00e0 un mod\u00e8le de fa\u00e7on ergonomique et de visualiser les r\u00e9sultats D\u00e9roulement \ud83d\udd17 Remplir app.stub.py , le renommer en app.py en remplissant les bons champs (s'aider des notebooks dans app/ ) et en cr\u00e9ant des jolies visualisations Packager l'application sous forme d'une image docker Tester son image docker localement Uploader le docker sur Google Container Registry Guide de d\u00e9veloppement \ud83d\udd17 Regardons le APP.md Remplissez le fichier avec la description de votre application Regardons le app.stub.py import requests import streamlit as st from PIL import Image import io import base64 from pydantic import BaseModel from typing import List import random # ---- Functions --- class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str @st . cache ( show_spinner = True ) def make_dummy_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" This simulates a fake answer for you to test your application without having access to any other input from other teams \"\"\" # We do a dummy encode and decode pass to check that the file is correct with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } # We do a dummy decode _image = data . get ( \"image\" ) _image = _image . encode ( \"utf-8\" ) _image = base64 . b64decode ( _image ) _image = Image . open ( io . BytesIO ( _image )) # type: Image if _image . mode == \"RGBA\" : _image = _image . convert ( \"RGB\" ) _model = data . get ( \"model\" ) # We generate a random prediction w , h = _image . size detections = [ Detection ( x_min = random . randint ( 0 , w // 2 - 1 ), y_min = random . randint ( 0 , h // 2 - 1 ), x_max = random . randint ( w // w , w - 1 ), y_max = random . randint ( h // 2 , h - 1 ), class_name = \"dummy\" , confidence = round ( random . random (), 3 ), ) for _ in range ( random . randint ( 1 , 10 )) ] # We return the result result = Result ( time = 0.1 , model = _model , detections = detections ) return result @st . cache ( show_spinner = True ) def make_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" Process our data and send a proper request \"\"\" with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } response = requests . post ( \" {} /predict\" . format ( model_url ), json = data ) if not response . status_code == 200 : raise ValueError ( \"Error in processing payload, {} \" . format ( response . text )) response = response . json () return Result . parse_obj ( response ) # ---- Streamlit App --- st . title ( \"NAME ME BECAUSE I AM AWESOME\" ) with open ( \"APP.md\" ) as f : st . markdown ( f . read ()) # --- Sidebar --- # defines an h1 header model_url = st . sidebar . text_input ( label = \"Cluster URL\" , value = \"http://localhost:8000\" ) _model_url = model_url . strip ( \"/\" ) if st . sidebar . button ( \"Send 'is alive' to IP\" ): try : response = requests . get ( \" {} /health\" . format ( _model_url )) if response . status_code == 200 : st . sidebar . success ( \"Webapp responding at {} \" . format ( _model_url )) else : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) except ConnectionError : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) test_mode_on = st . sidebar . checkbox ( label = \"Test Mode - Generate dummy answer\" , value = False ) # --- Main window st . markdown ( \"## Inputs\" ) st . markdown ( \"Describe something... You can also add things like confidence slider etc...\" ) # Here we should be able to choose between [\"yolov5s\", \"yolov5m\", \"yolov5l\"], perhaps a radio button with the three choices ? model_name = ... # Here we should be able to upload a file (our image) image_file = ... # Converting image, this is done for you :) if image_file is not None : image_file . seek ( 0 ) image = image_file . read () image = Image . open ( io . BytesIO ( image )) if st . button ( label = \"SEND PAYLOAD\" ): if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = ... # call the proper function else : result = ... # call the proper function st . balloons () st . markdown ( \"## Display\" ) st . markdown ( \"Make something pretty, draw polygons and confidence..., here's an ugly output\" ) st . image ( image , width = 512 , caption = \"Uploaded Image\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) for detection in result . detections : st . json ( detection . json ()) La majorit\u00e9 des fonctions de requ\u00eate sont d\u00e9j\u00e0 impl\u00e9ment\u00e9es, il reste \u00e0 faire les fonctions d'entr\u00e9es utilisateurs et la visualisation Entr\u00e9e: Utilisation de st.radio et st.file_uploader : https://docs.streamlit.io/en/stable/getting_started.html https://docs.streamlit.io/en/stable/api.html#streamlit.radio https://docs.streamlit.io/en/stable/api.html#streamlit.file_uploader st . markdown ( \"## Inputs\" ) st . markdown ( \"Select your model (Small, Medium or Large)\" ) model_name = st . radio ( label = \"Model Name\" , options = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ]) st . markdown ( \"Upload an image\" ) image_file = st . file_uploader ( label = \"Image File\" , type = [ \"png\" , \"jpg\" , \"tif\" ]) Visualisations Exemple de code qui imite le notebook de pr\u00e9diction pour dessiner sur une image PIL def draw_preds ( image : Image , detections : [ Detection ]): class_names = list ( set ([ detection . class_name for detection in detections ])) image_with_preds = image . copy () # Define colors colors = plt . cm . get_cmap ( \"viridis\" , len ( class_names )) . colors colors = ( colors [:, : 3 ] * 255.0 ) . astype ( np . uint8 ) # Define font font = list ( Path ( \"/usr/share/fonts\" ) . glob ( \"**/*.ttf\" ))[ 0 ] . name font = ImageFont . truetype ( font = font , size = np . floor ( 3e-2 * image_with_preds . size [ 1 ] + 0.5 ) . astype ( \"int32\" )) thickness = ( image_with_preds . size [ 0 ] + image_with_preds . size [ 1 ]) // 300 # Draw detections for detection in detections : left , top , right , bottom = detection . x_min , detection . y_min , detection . x_max , detection . y_max score = float ( detection . confidence ) predicted_class = detection . class_name class_idx = class_names . index ( predicted_class ) label = \" {} {:.2f} \" . format ( predicted_class , score ) draw = ImageDraw . Draw ( image_with_preds ) label_size = draw . textsize ( label , font ) top = max ( 0 , np . floor ( top + 0.5 ) . astype ( \"int32\" )) left = max ( 0 , np . floor ( left + 0.5 ) . astype ( \"int32\" )) bottom = min ( image_with_preds . size [ 1 ], np . floor ( bottom + 0.5 ) . astype ( \"int32\" )) right = min ( image_with_preds . size [ 0 ], np . floor ( right + 0.5 ) . astype ( \"int32\" )) if top - label_size [ 1 ] >= 0 : text_origin = np . array ([ left , top - label_size [ 1 ]]) else : text_origin = np . array ([ left , top + 1 ]) # My kingdom for a good redistributable image drawing library. for r in range ( thickness ): draw . rectangle ([ left + r , top + r , right - r , bottom - r ], outline = tuple ( colors [ class_idx ])) draw . rectangle ([ tuple ( text_origin ), tuple ( text_origin + label_size )], fill = tuple ( colors [ class_idx ])) if any ( colors [ class_idx ] > 128 ): fill = ( 0 , 0 , 0 ) else : fill = ( 255 , 255 , 255 ) draw . text ( text_origin , label , fill = fill , font = font ) del draw return image_with_preds Utilisation (exemple) if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = ... # call the proper function else : result = ... # call the proper function st . balloons () st . markdown ( \"## Display\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) image_with_preds = draw_preds ( image , result . detections ) st . image ( image_with_preds , width = 1024 , caption = \"Image with detections\" ) st . markdown ( \"### Detection dump\" ) for detection in result . detections : st . json ( detection . json ()) Corection app.py \ud83d\udd17 Hint import base64 import io import random from pathlib import Path from typing import List import matplotlib.pyplot as plt import numpy as np import requests import streamlit as st from PIL import Image from PIL import ImageDraw , ImageFont from pydantic import BaseModel # ---- Functions --- class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str @st . cache ( show_spinner = True ) def make_dummy_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" This simulates a fake answer for you to test your application without having access to any other input from other teams \"\"\" # We do a dummy encode and decode pass to check that the file is correct with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } # We do a dummy decode _image = data . get ( \"image\" ) _image = _image . encode ( \"utf-8\" ) _image = base64 . b64decode ( _image ) _image = Image . open ( io . BytesIO ( _image )) # type: Image if _image . mode == \"RGBA\" : _image = _image . convert ( \"RGB\" ) _model = data . get ( \"model\" ) # We generate a random prediction w , h = _image . size detections = [ Detection ( x_min = random . randint ( 0 , w // 2 - 1 ), y_min = random . randint ( 0 , h // 2 - 1 ), x_max = random . randint ( w // w , w - 1 ), y_max = random . randint ( h // 2 , h - 1 ), class_name = \"dummy\" , confidence = round ( random . random (), 3 ), ) for _ in range ( random . randint ( 1 , 10 )) ] # We return the result result = Result ( time = 0.1 , model = _model , detections = detections ) return result @st . cache ( show_spinner = True ) def make_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" Process our data and send a proper request \"\"\" with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } response = requests . post ( \" {} /predict\" . format ( model_url ), json = data ) if not response . status_code == 200 : raise ValueError ( \"Error in processing payload, {} \" . format ( response . text )) response = response . json () return Result . parse_obj ( response ) def draw_preds ( image : Image , detections : [ Detection ]): class_names = list ( set ([ detection . class_name for detection in detections ])) image_with_preds = image . copy () # Define colors colors = plt . cm . get_cmap ( \"viridis\" , len ( class_names )) . colors colors = ( colors [:, : 3 ] * 255.0 ) . astype ( np . uint8 ) # Define font font = list ( Path ( \"/usr/share/fonts\" ) . glob ( \"**/*.ttf\" ))[ 0 ] . name font = ImageFont . truetype ( font = font , size = np . floor ( 3e-2 * image_with_preds . size [ 1 ] + 0.5 ) . astype ( \"int32\" )) thickness = ( image_with_preds . size [ 0 ] + image_with_preds . size [ 1 ]) // 300 # Draw detections for detection in detections : left , top , right , bottom = detection . x_min , detection . y_min , detection . x_max , detection . y_max score = float ( detection . confidence ) predicted_class = detection . class_name class_idx = class_names . index ( predicted_class ) label = \" {} {:.2f} \" . format ( predicted_class , score ) draw = ImageDraw . Draw ( image_with_preds ) label_size = draw . textsize ( label , font ) top = max ( 0 , np . floor ( top + 0.5 ) . astype ( \"int32\" )) left = max ( 0 , np . floor ( left + 0.5 ) . astype ( \"int32\" )) bottom = min ( image_with_preds . size [ 1 ], np . floor ( bottom + 0.5 ) . astype ( \"int32\" )) right = min ( image_with_preds . size [ 0 ], np . floor ( right + 0.5 ) . astype ( \"int32\" )) if top - label_size [ 1 ] >= 0 : text_origin = np . array ([ left , top - label_size [ 1 ]]) else : text_origin = np . array ([ left , top + 1 ]) # My kingdom for a good redistributable image drawing library. for r in range ( thickness ): draw . rectangle ([ left + r , top + r , right - r , bottom - r ], outline = tuple ( colors [ class_idx ])) draw . rectangle ([ tuple ( text_origin ), tuple ( text_origin + label_size )], fill = tuple ( colors [ class_idx ])) if any ( colors [ class_idx ] > 128 ): fill = ( 0 , 0 , 0 ) else : fill = ( 255 , 255 , 255 ) draw . text ( text_origin , label , fill = fill , font = font ) del draw return image_with_preds # ---- Streamlit App --- st . title ( \"Yolo v5 Companion App\" ) st . markdown ( \"A super nice companion application to send requests and parse results \\n \" \"We wrap https://pytorch.org/hub/ultralytics_yolov5/\" ) # ---- Sidebar ---- test_mode_on = st . sidebar . checkbox ( label = \"Test Mode - Generate dummy answer\" , value = False ) st . sidebar . markdown ( \"Enter the cluster URL\" ) model_url = st . sidebar . text_input ( label = \"Cluster URL\" , value = \"http://localhost:8000\" ) _model_url = model_url . strip ( \"/\" ) if st . sidebar . button ( \"Send 'is alive' to IP\" ): try : health = requests . get ( \" {} /health\" . format ( _model_url )) title = requests . get ( \" {} /\" . format ( _model_url )) version = requests . get ( \" {} /version\" . format ( _model_url )) describe = requests . get ( \" {} /describe\" . format ( _model_url )) if health . status_code == 200 : st . sidebar . success ( \"Webapp responding at {} \" . format ( _model_url )) st . sidebar . json ({ \"title\" : title . text , \"version\" : version . text , \"description\" : describe . text }) else : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) except ConnectionError : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) # ---- Main window ---- st . markdown ( \"## Inputs\" ) st . markdown ( \"Select your model (Small, Medium or Large)\" ) # Data input model_name = st . radio ( label = \"Model Name\" , options = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ]) st . markdown ( \"Upload an image\" ) image_file = st . file_uploader ( label = \"Image File\" , type = [ \"png\" , \"jpg\" , \"tif\" ]) confidence_threshold = st . slider ( label = \"Confidence filter\" , min_value = 0.0 , max_value = 1.0 , value = 0.0 , step = 0.05 ) # UploadFile to PIL Image if image_file is not None : image_file . seek ( 0 ) image = image_file . read () image = Image . open ( io . BytesIO ( image )) st . markdown ( \"Send the payload to {} /predict\" . format ( _model_url )) # Send payload if st . button ( label = \"SEND PAYLOAD\" ): if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = make_dummy_request ( model_url = _model_url , model = model_name , image = image ) else : result = make_request ( model_url = _model_url , model = model_name , image = image ) st . balloons () # Display results st . markdown ( \"## Display\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) detections = [ detection for detection in result . detections if detection . confidence > confidence_threshold ] image_with_preds = draw_preds ( image , detections ) st . image ( image_with_preds , width = 1024 , caption = \"Image with detections\" ) st . markdown ( \"### Detection dump\" ) for detection in result . detections : st . json ( detection . json ()) !!! note: Le test mode servait pour un ancien BE. Si vous avez tout fait dans l'ordre vous ne devriez pas en avoir besoin Construire le docker \ud83d\udd17 PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker build -t eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version } -f Dockerfile . Tester le docker \ud83d\udd17 Au lieu de faire streamlit run app.py , vous pouvez lancer le docker localement et aller sur {ip}:8501 pour tester le docker PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker run --rm -p 8501 :8501 eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version } Vous pouvez vous rendre sur l'ip de la machine sur le port 8501 Indiquez l'ip de la machine port 8000 \u00e0 gauche Pousser le docker sur google container registry \ud83d\udd17 gcloud auth configure-docker docker push eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version } Liens Utiles \ud83d\udd17 Doc Streamlit 3 - Running two dockers in parallel using docker-compose \ud83d\udd17 On va utiliser docker compose pour lancer les deux applications en simultan\u00e9 de sorte \u00e0 ce qu'elles communiquent Plus d'infos sur docker compose Fermez tous les dockers etc. Cr\u00e9ez un fichier docker-compose.yml Ajoutez-y ce contenu: version : '3' services : yolo : image : \"test-yolo-v5:dummy\" ports : - \"8000:8000\" hostname : yolo streamlit : image : \"test-yolo-v5-streamlit:dummy\" ports : - \"8501:8501\" hostname : streamlit Modifiez les noms des images avec celles que vous avez utilis\u00e9es On constate qu'on d\u00e9clare 2 services: - 1 service \"yolo\" - 1 service \"streamlit\" On d\u00e9clare aussi les ports ouverts de chaque application Maintenant... comment lancer les deux applications ? docker-compose up dans le dossier o\u00f9 se trouve votre docker-compose.yml Si docker-compose ne fonctionne pas, sudo apt -y install docker-compose Normalement: - le service de mod\u00e8le est accessible sur le port 8000 de la machine - le service streamlit est accessible sur le port 8501 de la machine - vous devez indiquer l'hostname \"yolo\" pour communiquer entre streamlit et le mod\u00e8le. En effet, les services sont accessibles via un r\u00e9seau sp\u00e9cial \"local\" entre tous les containers lanc\u00e9s via docker-compose Conclusion \ud83d\udd17 \ud83c\udf89 Bravo ! \ud83c\udf89 Vous avez d\u00e9ploy\u00e9 votre premier mod\u00e8le en production !","title":"TP - Deployment"},{"location":"1_5_deployment_tp.html#deployment-deploy-your-ml-model-in-production","text":"","title":"Deployment : Deploy your ML model in production"},{"location":"1_5_deployment_tp.html#objectives","text":"Your first ML model in production ! A model behind a Restful API, packaged in a docker A frontend using streamlit, packaged in a docker Deploy it on Google Cloud Platform using GCE & docker-compose Send it to your friends ! Regardons ce notebook Il effectue les op\u00e9rations suivantes: Chargement d'un mod\u00e8le Chargement d'une image D\u00e9tection des \"objets\" sur l'image Dessin des d\u00e9tections sur l'image Affichage L'objectif est de convertir ce notebook en deux applications : L'une qui \"sert\" les pr\u00e9dictions d'un mod\u00e8le (le serveur) L'une qui permet \u00e0 un utilisateur d'interagir facilement avec le mod\u00e8le en mettant en ligne sa propre image (le \"client\") Puis de les d\u00e9ployer sur une instance GCP","title":"Objectives"},{"location":"1_5_deployment_tp.html#team-composition","text":"C'est mieux d'\u00eatre en bin\u00f4me pour s'entraider :)","title":"Team Composition"},{"location":"1_5_deployment_tp.html#how-to-run-this","text":"Commen\u00e7ons par cr\u00e9er une instance GCP bien configur\u00e9e depuis laquelle se connecter: export INSTANCE_NAME = \"tp-deployment-{yourgroup}-{yourname}\" # Don't forget to replace values ! gcloud compute instances create $INSTANCE_NAME \\ --zone = \"europe-west4-a\" \\ --machine-type = \"n1-standard-1\" \\ --image-family = \"common-cpu\" \\ --image-project = \"deeplearning-platform-release\" \\ --maintenance-policy = TERMINATE \\ --scopes = \"storage-rw\" \\ --boot-disk-size = 75GB R\u00e9cuperez l'ip publique de la machine (via l'interface google cloud ou bien en faisant gcloud compute instances list | grep {votre instance} et notez l\u00e0 bien Depuis le google cloud shell - ou depuis votre machine si vous avez google cloud sdk d'install\u00e9 localement et que vous \u00eates sous eduroam, gcloud compute ssh { user } @ { instance } -- \\ -L 8080 :localhost:8080 \\ -L 8080 :localhost:8080 Vous pouvez ensuite aller sur localhost:8080 (install locale) ou faire un web preview depuis cloud shell sur le port 8080 de cloud shell, Vous devriez \u00eatre dans un jupyter lab Maintenant, depuis ce jupyter lab, ouvrez un terminal et r\u00e9cup\u00e9rez les fichiers suivants : gsutil cp -r gs://fchouteau-isae-cloud/deployment/* .","title":"How to run this"},{"location":"1_5_deployment_tp.html#1-converting-a-prediction-notebook-into-a-webapplication","text":"Placez vous dans le dossier model nouvellement cr\u00e9\u00e9","title":"1 - Converting a prediction notebook into a webapplication"},{"location":"1_5_deployment_tp.html#objectif","text":"Packager un mod\u00e8le de machine learning derri\u00e8re une webapplication pour pouvoir la d\u00e9ployer sur le web et servir des pr\u00e9dictions \u00e0 des utilisateurs Le mod\u00e8le: Un d\u00e9tecteur d'objets sur des photographies \"standard\" suppos\u00e9 marcher en temps r\u00e9el, qui sort des \"bounding boxes\" autour des objets d\u00e9tect\u00e9 dans des images Le papier vaut la lecture https://pjreddie.com/media/files/papers/YOLOv3.pdf On r\u00e9cup\u00e8re la version disponible sur torchhub https://pytorch.org/hub/ultralytics_yolov5/ qui correspond \u00e0 ceci https://github.com/ultralytics/yolov5 Voici une petite explication de l'historique https://medium.com/towards-artificial-intelligence/yolo-v5-is-here-custom-object-detection-tutorial-with-yolo-v5-12666ee1774e On se propose ici de wrapper 3 versions du mod\u00e8le (S,M,L) qui sont 3 versions +/- complexes du mod\u00e8le YOLO-V5, afin de pouvoir comparer les performances et les r\u00e9sultats","title":"Objectif"},{"location":"1_5_deployment_tp.html#deroulement","text":"Transformer un notebook de pr\u00e9diction en \u201cWebApp\u201d en remplissant app.stub.py et en le renommant en app.py Packager l'application sous forme d'une image docker Tester son image docker localement Uploader le docker sur Google Container Registry","title":"D\u00e9roulement"},{"location":"1_5_deployment_tp.html#developpement-de-apppy","text":"Regardons le app.stub.py (que l'on renommera en app.py ) import base64 import io import time from typing import List , Dict import numpy as np import torch from PIL import Image from fastapi import FastAPI , HTTPException from pydantic import BaseModel class Input ( BaseModel ): model : str image : str class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str # !!!! FILL ME def parse_predictions ( prediction : np . ndarray , classes : [ str ]) -> List [ Detection ]: raise NotImplementedError # !!!! FILL ME def load_model ( model_name : str ): \"\"\"\"\"\" raise NotImplementedError MODEL_NAMES = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ] app = FastAPI ( title = \"NAME ME\" , description = \"\"\" DESCRIBE ME \"\"\" , version = \"1.0\" , ) # !!!! FILL ME # This is a dictionnary that must contains a model for each key (model names), fill load model # example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name) # You can also lazily load models only when they are called to avoid holding 3 models in memory MODELS = ... @app . get ( \"/\" , description = \"return the title\" , response_description = \"FILL ME\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/health\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = List [ str ]) def models () -> [ str ]: return MODEL_NAMES @app . post ( \"/predict\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = Result ) def predict ( inputs : Input ) -> Result : # get correct model model_name = inputs . model if model_name not in MODEL_NAMES : raise HTTPException ( status_code = 400 , detail = \"wrong model name, choose between {} \" . format ( MODEL_NAMES )) # Get the model from the list of available models model = MODELS . get ( model_name ) # Get & Decode image try : image = inputs . image . encode ( \"utf-8\" ) image = base64 . b64decode ( image ) image = Image . open ( io . BytesIO ( image )) except : raise HTTPException ( status_code = 400 , detail = \"File is not an image\" ) # Convert from RGBA to RGB *to avoid alpha channels* if image . mode == \"RGBA\" : image = image . convert ( \"RGB\" ) # Inference # RUN THE PREDICTION, TIME IT predictions = ... # Post processing classes = predictions . names predictions = predictions . xyxy [ 0 ] . numpy () # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method detections = ... result = Result ( detections =... , time =... , model =... ) return result Dans un premier temps, vous pouvez remplir la description des \"routes\" (i.e. des fonctions de l'application): @app . get ( \"/\" , description = \"return the title\" , response_description = \"FILL ME\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/health\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"FILL ME\" , response_description = \"FILL ME\" , response_model = List [ str ]) def models () -> [ str ]: return MODEL_NAMES Il y a deux fonctions \u00e0 compl\u00e9ter en s'inspirant du notebook inference.ipynb . Grace au typage de python, vous avez les types d'entr\u00e9e et de sortie des deux fonctions La premi\u00e8re prend un tableau de type (left, top, right, bottom, confidence, class_index) et une liste de noms de classes et cr\u00e9\u00e9e une liste d'objets Detection (voir le code pour la cr\u00e9ation des objets d\u00e9tection) La seconde fonction doit charger un mod\u00e8le via torchhub en fonction de son nom (voir le docker) # !!!! FILL ME def parse_predictions ( predictions : np . ndarray , classes : [ str ]) -> List [ Detection ]: raise NotImplementedError Hint def parse_prediction ( prediction : np . ndarray , classes : [ str ]) -> Detection : x0 , y0 , x1 , y1 , cnf , cls = prediction detection = Detection ( x_min = int ( x0 ), y_min = int ( y0 ), x_max = int ( x1 ), y_max = int ( y1 ), confidence = round ( float ( cnf ), 3 ), class_name = classes [ int ( cls )], ) return detection # !!!! FILL ME def load_model ( model_name : str ): \"\"\"\"\"\" raise NotImplementedError Hint def load_model ( model_name : str ) -> Dict : # Load model from torch model = torch . hub . load ( \"ultralytics/yolov5\" , model_name , pretrained = True ) # Evaluation mode + Non maximum threshold model = model . eval () return model Ensuite, vous pouvez executer les fonctions de chargement de mod\u00e8le, par exemple # !!!! FILL ME # This is a dictionnary that must contains a model for each key (model names), fill load model # example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name) # You can also lazily load models only when they are called to avoid holding 3 models in memory MODELS = {} for model_name in MODEL_NAMES : MODELS [ model_name ] = load_model ( model_name ) Enfin, il s'agit d'\u00e9crire un code qui effectue une pr\u00e9diction \u00e0 partir d'une image PIL et de mesurer le temps (indice: import time et t0 = time.time() ...) de pr\u00e9diction # RUN THE PREDICTION, TIME IT predictions = ... # Post processing classes = predictions . names predictions = predictions . xyxy [ 0 ] . numpy () Le r\u00e9sultat de predictions est un tableau numpy compos\u00e9 des colonnes left, top, right, bottom, confidence, class_index Il s'agit ensuite de transformer ces predictions en [Detection] # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method detections = parse_predictions ( predictions , classes ) Hint # Inference t0 = time . time () predictions = model ( image , size = 640 ) # includes NMS t1 = time . time () classes = predictions . names # Post processing predictions = predictions . xyxy [ 0 ] . numpy () detections = [ parse_prediction ( prediction = pred , classes = classes ) for pred in predictions ] result = Result ( detections = detections , time = round ( t1 - t0 , 3 ), model = model_name )","title":"D\u00e9veloppement de app.py"},{"location":"1_5_deployment_tp.html#correction","text":"app.py Hint import base64 import io import time from typing import List , Dict import numpy as np import torch from PIL import Image from fastapi import FastAPI , HTTPException from pydantic import BaseModel class Input ( BaseModel ): model : str image : str class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str def parse_prediction ( prediction : np . ndarray , classes : [ str ]) -> Detection : x0 , y0 , x1 , y1 , cnf , cls = prediction detection = Detection ( x_min = int ( x0 ), y_min = int ( y0 ), x_max = int ( x1 ), y_max = int ( y1 ), confidence = round ( float ( cnf ), 3 ), class_name = classes [ int ( cls )], ) return detection def load_model ( model_name : str ) -> Dict : # Load model from torch model = torch . hub . load ( \"ultralytics/yolov5\" , model_name , pretrained = True ) # Evaluation mode + Non maximum threshold model = model . eval () return model # %% app = FastAPI ( title = \"YOLO-V5 WebApp created with FastAPI\" , description = \"\"\" Wraps 3 different yolo-v5 models under the same RESTful API \"\"\" , version = \"1.1\" , ) # %% MODEL_NAMES = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ] MODELS = {} @app . get ( \"/\" , description = \"return the title\" , response_description = \"title\" , response_model = str ) def root () -> str : return app . title @app . get ( \"/describe\" , description = \"return the description\" , response_description = \"description\" , response_model = str ) def describe () -> str : return app . description @app . get ( \"/version\" , description = \"return the version\" , response_description = \"version\" , response_model = str ) def describe () -> str : return app . version @app . get ( \"/health\" , description = \"return whether it's alive\" , response_description = \"alive\" , response_model = str ) def health () -> str : return \"HEALTH OK\" @app . get ( \"/models\" , description = \"Query the list of models\" , response_description = \"A list of available models\" , response_model = List [ str ], ) def models () -> [ str ]: return MODEL_NAMES @app . post ( \"/predict\" , description = \"Send a base64 encoded image + the model name, get detections\" , response_description = \"Detections + Processing time\" , response_model = Result , ) def predict ( inputs : Input ) -> Result : global MODELS # get correct model model_name = inputs . model if model_name not in MODEL_NAMES : raise HTTPException ( status_code = 400 , detail = \"wrong model name, choose between {} \" . format ( MODEL_NAMES )) # check load if MODELS . get ( model_name ) is None : MODELS [ model_name ] = load_model ( model_name ) model = MODELS . get ( model_name ) # Get Image # Decode image try : image = inputs . image . encode ( \"utf-8\" ) image = base64 . b64decode ( image ) image = Image . open ( io . BytesIO ( image )) except : raise HTTPException ( status_code = 400 , detail = \"File is not an image\" ) # Convert from RGBA to RGB *to avoid alpha channels* if image . mode == \"RGBA\" : image = image . convert ( \"RGB\" ) # Inference t0 = time . time () predictions = model ( image , size = 640 ) # includes NMS t1 = time . time () classes = predictions . names # Post processing predictions = predictions . xyxy [ 0 ] . numpy () detections = [ parse_prediction ( prediction = pred , classes = classes ) for pred in predictions ] result = Result ( detections = detections , time = round ( t1 - t0 , 3 ), model = model_name ) return result","title":"Correction"},{"location":"1_5_deployment_tp.html#construire-le-docker","text":"PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker build -t eu.gcr.io/ ${ PROJECT_ID } / { you rname }{ your app name } : { your version } -f Dockerfile .","title":"Construire le docker"},{"location":"1_5_deployment_tp.html#tester-le-docker","text":"Vous pouvez lancer le docker localement et le tester avec le notebook PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker run --rm -p 8000 :8000 eu.gcr.io/ ${ PROJECT_ID } / { your-name } - { your app name } : { your version } Vous pouvez vous connecter \u00e0 votre appli via son ip publique sur le port 8000 depuis votre navigateur local http://{ip}:8000 Essayez quelques routes : /models /docs","title":"Tester le docker"},{"location":"1_5_deployment_tp.html#pusher-le-docker-sur-google-container-registry","text":"gcloud auth configure-docker docker push eu.gcr.io/ ${ PROJECT_ID } / { your-name } - { your app name } : { your version } Si vous devez mettre \u00e0 jour le docker, il faut incr\u00e9menter la version pour le d\u00e9ploiement","title":"Pusher le docker sur google container registry"},{"location":"1_5_deployment_tp.html#liens-utiles","text":"https://fastapi.tiangolo.com/ https://requests.readthedocs.io/en/master/ https://testdriven.io/blog/fastapi-streamlit/","title":"Liens Utiles"},{"location":"1_5_deployment_tp.html#2-making-a-companion-application","text":"Allez dans le dossier streamlit","title":"2 - Making a companion application"},{"location":"1_5_deployment_tp.html#objectif_1","text":"Cr\u00e9er une application \"compagnon\" qui permet de faire des requ\u00eates \u00e0 un mod\u00e8le de fa\u00e7on ergonomique et de visualiser les r\u00e9sultats","title":"Objectif"},{"location":"1_5_deployment_tp.html#deroulement_1","text":"Remplir app.stub.py , le renommer en app.py en remplissant les bons champs (s'aider des notebooks dans app/ ) et en cr\u00e9ant des jolies visualisations Packager l'application sous forme d'une image docker Tester son image docker localement Uploader le docker sur Google Container Registry","title":"D\u00e9roulement"},{"location":"1_5_deployment_tp.html#guide-de-developpement","text":"Regardons le APP.md Remplissez le fichier avec la description de votre application Regardons le app.stub.py import requests import streamlit as st from PIL import Image import io import base64 from pydantic import BaseModel from typing import List import random # ---- Functions --- class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str @st . cache ( show_spinner = True ) def make_dummy_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" This simulates a fake answer for you to test your application without having access to any other input from other teams \"\"\" # We do a dummy encode and decode pass to check that the file is correct with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } # We do a dummy decode _image = data . get ( \"image\" ) _image = _image . encode ( \"utf-8\" ) _image = base64 . b64decode ( _image ) _image = Image . open ( io . BytesIO ( _image )) # type: Image if _image . mode == \"RGBA\" : _image = _image . convert ( \"RGB\" ) _model = data . get ( \"model\" ) # We generate a random prediction w , h = _image . size detections = [ Detection ( x_min = random . randint ( 0 , w // 2 - 1 ), y_min = random . randint ( 0 , h // 2 - 1 ), x_max = random . randint ( w // w , w - 1 ), y_max = random . randint ( h // 2 , h - 1 ), class_name = \"dummy\" , confidence = round ( random . random (), 3 ), ) for _ in range ( random . randint ( 1 , 10 )) ] # We return the result result = Result ( time = 0.1 , model = _model , detections = detections ) return result @st . cache ( show_spinner = True ) def make_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" Process our data and send a proper request \"\"\" with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } response = requests . post ( \" {} /predict\" . format ( model_url ), json = data ) if not response . status_code == 200 : raise ValueError ( \"Error in processing payload, {} \" . format ( response . text )) response = response . json () return Result . parse_obj ( response ) # ---- Streamlit App --- st . title ( \"NAME ME BECAUSE I AM AWESOME\" ) with open ( \"APP.md\" ) as f : st . markdown ( f . read ()) # --- Sidebar --- # defines an h1 header model_url = st . sidebar . text_input ( label = \"Cluster URL\" , value = \"http://localhost:8000\" ) _model_url = model_url . strip ( \"/\" ) if st . sidebar . button ( \"Send 'is alive' to IP\" ): try : response = requests . get ( \" {} /health\" . format ( _model_url )) if response . status_code == 200 : st . sidebar . success ( \"Webapp responding at {} \" . format ( _model_url )) else : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) except ConnectionError : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) test_mode_on = st . sidebar . checkbox ( label = \"Test Mode - Generate dummy answer\" , value = False ) # --- Main window st . markdown ( \"## Inputs\" ) st . markdown ( \"Describe something... You can also add things like confidence slider etc...\" ) # Here we should be able to choose between [\"yolov5s\", \"yolov5m\", \"yolov5l\"], perhaps a radio button with the three choices ? model_name = ... # Here we should be able to upload a file (our image) image_file = ... # Converting image, this is done for you :) if image_file is not None : image_file . seek ( 0 ) image = image_file . read () image = Image . open ( io . BytesIO ( image )) if st . button ( label = \"SEND PAYLOAD\" ): if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = ... # call the proper function else : result = ... # call the proper function st . balloons () st . markdown ( \"## Display\" ) st . markdown ( \"Make something pretty, draw polygons and confidence..., here's an ugly output\" ) st . image ( image , width = 512 , caption = \"Uploaded Image\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) for detection in result . detections : st . json ( detection . json ()) La majorit\u00e9 des fonctions de requ\u00eate sont d\u00e9j\u00e0 impl\u00e9ment\u00e9es, il reste \u00e0 faire les fonctions d'entr\u00e9es utilisateurs et la visualisation Entr\u00e9e: Utilisation de st.radio et st.file_uploader : https://docs.streamlit.io/en/stable/getting_started.html https://docs.streamlit.io/en/stable/api.html#streamlit.radio https://docs.streamlit.io/en/stable/api.html#streamlit.file_uploader st . markdown ( \"## Inputs\" ) st . markdown ( \"Select your model (Small, Medium or Large)\" ) model_name = st . radio ( label = \"Model Name\" , options = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ]) st . markdown ( \"Upload an image\" ) image_file = st . file_uploader ( label = \"Image File\" , type = [ \"png\" , \"jpg\" , \"tif\" ]) Visualisations Exemple de code qui imite le notebook de pr\u00e9diction pour dessiner sur une image PIL def draw_preds ( image : Image , detections : [ Detection ]): class_names = list ( set ([ detection . class_name for detection in detections ])) image_with_preds = image . copy () # Define colors colors = plt . cm . get_cmap ( \"viridis\" , len ( class_names )) . colors colors = ( colors [:, : 3 ] * 255.0 ) . astype ( np . uint8 ) # Define font font = list ( Path ( \"/usr/share/fonts\" ) . glob ( \"**/*.ttf\" ))[ 0 ] . name font = ImageFont . truetype ( font = font , size = np . floor ( 3e-2 * image_with_preds . size [ 1 ] + 0.5 ) . astype ( \"int32\" )) thickness = ( image_with_preds . size [ 0 ] + image_with_preds . size [ 1 ]) // 300 # Draw detections for detection in detections : left , top , right , bottom = detection . x_min , detection . y_min , detection . x_max , detection . y_max score = float ( detection . confidence ) predicted_class = detection . class_name class_idx = class_names . index ( predicted_class ) label = \" {} {:.2f} \" . format ( predicted_class , score ) draw = ImageDraw . Draw ( image_with_preds ) label_size = draw . textsize ( label , font ) top = max ( 0 , np . floor ( top + 0.5 ) . astype ( \"int32\" )) left = max ( 0 , np . floor ( left + 0.5 ) . astype ( \"int32\" )) bottom = min ( image_with_preds . size [ 1 ], np . floor ( bottom + 0.5 ) . astype ( \"int32\" )) right = min ( image_with_preds . size [ 0 ], np . floor ( right + 0.5 ) . astype ( \"int32\" )) if top - label_size [ 1 ] >= 0 : text_origin = np . array ([ left , top - label_size [ 1 ]]) else : text_origin = np . array ([ left , top + 1 ]) # My kingdom for a good redistributable image drawing library. for r in range ( thickness ): draw . rectangle ([ left + r , top + r , right - r , bottom - r ], outline = tuple ( colors [ class_idx ])) draw . rectangle ([ tuple ( text_origin ), tuple ( text_origin + label_size )], fill = tuple ( colors [ class_idx ])) if any ( colors [ class_idx ] > 128 ): fill = ( 0 , 0 , 0 ) else : fill = ( 255 , 255 , 255 ) draw . text ( text_origin , label , fill = fill , font = font ) del draw return image_with_preds Utilisation (exemple) if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = ... # call the proper function else : result = ... # call the proper function st . balloons () st . markdown ( \"## Display\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) image_with_preds = draw_preds ( image , result . detections ) st . image ( image_with_preds , width = 1024 , caption = \"Image with detections\" ) st . markdown ( \"### Detection dump\" ) for detection in result . detections : st . json ( detection . json ())","title":"Guide de d\u00e9veloppement"},{"location":"1_5_deployment_tp.html#corection-apppy","text":"Hint import base64 import io import random from pathlib import Path from typing import List import matplotlib.pyplot as plt import numpy as np import requests import streamlit as st from PIL import Image from PIL import ImageDraw , ImageFont from pydantic import BaseModel # ---- Functions --- class Detection ( BaseModel ): x_min : int y_min : int x_max : int y_max : int class_name : str confidence : float class Result ( BaseModel ): detections : List [ Detection ] = [] time : float = 0.0 model : str @st . cache ( show_spinner = True ) def make_dummy_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" This simulates a fake answer for you to test your application without having access to any other input from other teams \"\"\" # We do a dummy encode and decode pass to check that the file is correct with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } # We do a dummy decode _image = data . get ( \"image\" ) _image = _image . encode ( \"utf-8\" ) _image = base64 . b64decode ( _image ) _image = Image . open ( io . BytesIO ( _image )) # type: Image if _image . mode == \"RGBA\" : _image = _image . convert ( \"RGB\" ) _model = data . get ( \"model\" ) # We generate a random prediction w , h = _image . size detections = [ Detection ( x_min = random . randint ( 0 , w // 2 - 1 ), y_min = random . randint ( 0 , h // 2 - 1 ), x_max = random . randint ( w // w , w - 1 ), y_max = random . randint ( h // 2 , h - 1 ), class_name = \"dummy\" , confidence = round ( random . random (), 3 ), ) for _ in range ( random . randint ( 1 , 10 )) ] # We return the result result = Result ( time = 0.1 , model = _model , detections = detections ) return result @st . cache ( show_spinner = True ) def make_request ( model_url : str , model : str , image : Image ) -> Result : \"\"\" Process our data and send a proper request \"\"\" with io . BytesIO () as buffer : image . save ( buffer , format = \"PNG\" ) buffer : str = base64 . b64encode ( buffer . getvalue ()) . decode ( \"utf-8\" ) data = { \"model\" : model , \"image\" : buffer } response = requests . post ( \" {} /predict\" . format ( model_url ), json = data ) if not response . status_code == 200 : raise ValueError ( \"Error in processing payload, {} \" . format ( response . text )) response = response . json () return Result . parse_obj ( response ) def draw_preds ( image : Image , detections : [ Detection ]): class_names = list ( set ([ detection . class_name for detection in detections ])) image_with_preds = image . copy () # Define colors colors = plt . cm . get_cmap ( \"viridis\" , len ( class_names )) . colors colors = ( colors [:, : 3 ] * 255.0 ) . astype ( np . uint8 ) # Define font font = list ( Path ( \"/usr/share/fonts\" ) . glob ( \"**/*.ttf\" ))[ 0 ] . name font = ImageFont . truetype ( font = font , size = np . floor ( 3e-2 * image_with_preds . size [ 1 ] + 0.5 ) . astype ( \"int32\" )) thickness = ( image_with_preds . size [ 0 ] + image_with_preds . size [ 1 ]) // 300 # Draw detections for detection in detections : left , top , right , bottom = detection . x_min , detection . y_min , detection . x_max , detection . y_max score = float ( detection . confidence ) predicted_class = detection . class_name class_idx = class_names . index ( predicted_class ) label = \" {} {:.2f} \" . format ( predicted_class , score ) draw = ImageDraw . Draw ( image_with_preds ) label_size = draw . textsize ( label , font ) top = max ( 0 , np . floor ( top + 0.5 ) . astype ( \"int32\" )) left = max ( 0 , np . floor ( left + 0.5 ) . astype ( \"int32\" )) bottom = min ( image_with_preds . size [ 1 ], np . floor ( bottom + 0.5 ) . astype ( \"int32\" )) right = min ( image_with_preds . size [ 0 ], np . floor ( right + 0.5 ) . astype ( \"int32\" )) if top - label_size [ 1 ] >= 0 : text_origin = np . array ([ left , top - label_size [ 1 ]]) else : text_origin = np . array ([ left , top + 1 ]) # My kingdom for a good redistributable image drawing library. for r in range ( thickness ): draw . rectangle ([ left + r , top + r , right - r , bottom - r ], outline = tuple ( colors [ class_idx ])) draw . rectangle ([ tuple ( text_origin ), tuple ( text_origin + label_size )], fill = tuple ( colors [ class_idx ])) if any ( colors [ class_idx ] > 128 ): fill = ( 0 , 0 , 0 ) else : fill = ( 255 , 255 , 255 ) draw . text ( text_origin , label , fill = fill , font = font ) del draw return image_with_preds # ---- Streamlit App --- st . title ( \"Yolo v5 Companion App\" ) st . markdown ( \"A super nice companion application to send requests and parse results \\n \" \"We wrap https://pytorch.org/hub/ultralytics_yolov5/\" ) # ---- Sidebar ---- test_mode_on = st . sidebar . checkbox ( label = \"Test Mode - Generate dummy answer\" , value = False ) st . sidebar . markdown ( \"Enter the cluster URL\" ) model_url = st . sidebar . text_input ( label = \"Cluster URL\" , value = \"http://localhost:8000\" ) _model_url = model_url . strip ( \"/\" ) if st . sidebar . button ( \"Send 'is alive' to IP\" ): try : health = requests . get ( \" {} /health\" . format ( _model_url )) title = requests . get ( \" {} /\" . format ( _model_url )) version = requests . get ( \" {} /version\" . format ( _model_url )) describe = requests . get ( \" {} /describe\" . format ( _model_url )) if health . status_code == 200 : st . sidebar . success ( \"Webapp responding at {} \" . format ( _model_url )) st . sidebar . json ({ \"title\" : title . text , \"version\" : version . text , \"description\" : describe . text }) else : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) except ConnectionError : st . sidebar . error ( \"Webapp not respond at {} , check url\" . format ( _model_url )) # ---- Main window ---- st . markdown ( \"## Inputs\" ) st . markdown ( \"Select your model (Small, Medium or Large)\" ) # Data input model_name = st . radio ( label = \"Model Name\" , options = [ \"yolov5s\" , \"yolov5m\" , \"yolov5l\" ]) st . markdown ( \"Upload an image\" ) image_file = st . file_uploader ( label = \"Image File\" , type = [ \"png\" , \"jpg\" , \"tif\" ]) confidence_threshold = st . slider ( label = \"Confidence filter\" , min_value = 0.0 , max_value = 1.0 , value = 0.0 , step = 0.05 ) # UploadFile to PIL Image if image_file is not None : image_file . seek ( 0 ) image = image_file . read () image = Image . open ( io . BytesIO ( image )) st . markdown ( \"Send the payload to {} /predict\" . format ( _model_url )) # Send payload if st . button ( label = \"SEND PAYLOAD\" ): if test_mode_on : st . warning ( \"Simulating a dummy request to {} \" . format ( model_url )) result = make_dummy_request ( model_url = _model_url , model = model_name , image = image ) else : result = make_request ( model_url = _model_url , model = model_name , image = image ) st . balloons () # Display results st . markdown ( \"## Display\" ) st . text ( \"Model : {} \" . format ( result . model )) st . text ( \"Processing time : {} s\" . format ( result . time )) detections = [ detection for detection in result . detections if detection . confidence > confidence_threshold ] image_with_preds = draw_preds ( image , detections ) st . image ( image_with_preds , width = 1024 , caption = \"Image with detections\" ) st . markdown ( \"### Detection dump\" ) for detection in result . detections : st . json ( detection . json ()) !!! note: Le test mode servait pour un ancien BE. Si vous avez tout fait dans l'ordre vous ne devriez pas en avoir besoin","title":"Corection app.py"},{"location":"1_5_deployment_tp.html#construire-le-docker_1","text":"PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker build -t eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version } -f Dockerfile .","title":"Construire le docker"},{"location":"1_5_deployment_tp.html#tester-le-docker_1","text":"Au lieu de faire streamlit run app.py , vous pouvez lancer le docker localement et aller sur {ip}:8501 pour tester le docker PROJECT_ID = $( gcloud config get-value project 2 > /dev/null ) docker run --rm -p 8501 :8501 eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version } Vous pouvez vous rendre sur l'ip de la machine sur le port 8501 Indiquez l'ip de la machine port 8000 \u00e0 gauche","title":"Tester le docker"},{"location":"1_5_deployment_tp.html#pousser-le-docker-sur-google-container-registry","text":"gcloud auth configure-docker docker push eu.gcr.io/ ${ PROJECT_ID } / { your app name } : { your version }","title":"Pousser le docker sur google container registry"},{"location":"1_5_deployment_tp.html#liens-utiles_1","text":"Doc Streamlit","title":"Liens Utiles"},{"location":"1_5_deployment_tp.html#3-running-two-dockers-in-parallel-using-docker-compose","text":"On va utiliser docker compose pour lancer les deux applications en simultan\u00e9 de sorte \u00e0 ce qu'elles communiquent Plus d'infos sur docker compose Fermez tous les dockers etc. Cr\u00e9ez un fichier docker-compose.yml Ajoutez-y ce contenu: version : '3' services : yolo : image : \"test-yolo-v5:dummy\" ports : - \"8000:8000\" hostname : yolo streamlit : image : \"test-yolo-v5-streamlit:dummy\" ports : - \"8501:8501\" hostname : streamlit Modifiez les noms des images avec celles que vous avez utilis\u00e9es On constate qu'on d\u00e9clare 2 services: - 1 service \"yolo\" - 1 service \"streamlit\" On d\u00e9clare aussi les ports ouverts de chaque application Maintenant... comment lancer les deux applications ? docker-compose up dans le dossier o\u00f9 se trouve votre docker-compose.yml Si docker-compose ne fonctionne pas, sudo apt -y install docker-compose Normalement: - le service de mod\u00e8le est accessible sur le port 8000 de la machine - le service streamlit est accessible sur le port 8501 de la machine - vous devez indiquer l'hostname \"yolo\" pour communiquer entre streamlit et le mod\u00e8le. En effet, les services sont accessibles via un r\u00e9seau sp\u00e9cial \"local\" entre tous les containers lanc\u00e9s via docker-compose","title":"3 - Running two dockers in parallel using docker-compose"},{"location":"1_5_deployment_tp.html#conclusion","text":"\ud83c\udf89 Bravo ! \ud83c\udf89 Vous avez d\u00e9ploy\u00e9 votre premier mod\u00e8le en production !","title":"Conclusion"},{"location":"1_5_kub_intro.html","text":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine \ud83d\udd17 What is JupyterHub \ud83d\udd17 JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators. JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more. Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub). Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users. Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware. The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration. Zero to Jupyterhub using Kubernetes \ud83d\udd17 JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team). This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider. Instructions \ud83d\udd17 Go here and follow the instructions Use Google Kubernetes Engine to setup your cluster Info You will use the same method later in the year to setup a Dask Kubernetes cluster using helm Give some people the public IP of your cluster so that they can connect to it... try to make it scale !","title":"TP (at home) - Kubenertes Intro"},{"location":"1_5_kub_intro.html#kubernetes-zero-to-jupyterhub-using-google-kubernetes-engine","text":"","title":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine"},{"location":"1_5_kub_intro.html#what-is-jupyterhub","text":"JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators. JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more. Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub). Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users. Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware. The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration.","title":"What is JupyterHub"},{"location":"1_5_kub_intro.html#zero-to-jupyterhub-using-kubernetes","text":"JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team). This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider.","title":"Zero to Jupyterhub using Kubernetes"},{"location":"1_5_kub_intro.html#instructions","text":"Go here and follow the instructions Use Google Kubernetes Engine to setup your cluster Info You will use the same method later in the year to setup a Dask Kubernetes cluster using helm Give some people the public IP of your cluster so that they can connect to it... try to make it scale !","title":"Instructions"},{"location":"1_6_conclusion.html","text":"Recap' & Conclusion \ud83d\udd17 Link to slides","title":"Conclusion"},{"location":"1_6_conclusion.html#recap-conclusion","text":"Link to slides","title":"Recap' &amp; Conclusion"},{"location":"1_7_readings.html","text":"Readings \ud83d\udd17 About Cloud Computing \ud83d\udd17 Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... & Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade . ACM computing surveys (CSUR), 51(5), 1-38. On sustainable data centers and energy use (intro) The NIST Definitions of Cloud Computing Open Data: Open Sentinel 2 archive on AWS Environmental Impact of Cloud vs On Premise Environmental Impact of cloud vs on-premise medium blog post Paper from Natural Resources Defense Council on Cloud vs On-Premise Anecdotes about Cloud Computing About Containers \ud83d\udd17 Docker whitepaper: Docker and the way of the Devops What exactly is Docker ? Simple explanation from a medium blog post About Orchestration \ud83d\udd17 Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., & Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg . In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17). Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic","title":"Reading List"},{"location":"1_7_readings.html#readings","text":"","title":"Readings"},{"location":"1_7_readings.html#about-cloud-computing","text":"Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... & Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade . ACM computing surveys (CSUR), 51(5), 1-38. On sustainable data centers and energy use (intro) The NIST Definitions of Cloud Computing Open Data: Open Sentinel 2 archive on AWS Environmental Impact of Cloud vs On Premise Environmental Impact of cloud vs on-premise medium blog post Paper from Natural Resources Defense Council on Cloud vs On-Premise Anecdotes about Cloud Computing","title":"About Cloud Computing"},{"location":"1_7_readings.html#about-containers","text":"Docker whitepaper: Docker and the way of the Devops What exactly is Docker ? Simple explanation from a medium blog post","title":"About Containers"},{"location":"1_7_readings.html#about-orchestration","text":"Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., & Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg . In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17). Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic","title":"About Orchestration"},{"location":"2_1_overview.html","text":"Introduction to Data Distribution \ud83d\udd17 Course Introduction Course Overview \ud83d\udd17 Data Distribution & Big Data Processing Harnessing the complexity of large amounts of data is a challenge in itself. But Big Data processing is more than that: originally characterized by the 3 Vs of Volume, Velocity and Variety, the concepts popularized by Hadoop and Google requires dedicated computing solutions (both software and infrastructure), which will be explored in this module. Objectives \ud83d\udd17 By the end of this module, participants will be able to: Understand the differences and usage between main distributed computing architectures (HPC, Big Data, Cloud, CPU vs GPGPU) Implement the distribution of simple operations via the Map/Reduce principle in PySpark Understand the principle of Kubernetes Deploy a Big Data Processing Platform on the Cloud Implement the distribution of data wrangling/cleaning and training machine learning algorithms using PyData stack, Jupyter notebooks and Dask Quizz \ud83d\udd17 I'll try to propose some quizz to be sure you're following! Program \ud83d\udd17 Big Data & Distributed Computing (3h) \ud83d\udd17 Introduction to Big Data and its ecosystem (1h) What is Big Data? Legacy \u201cBig Data\u201d ecosystem Big Data use cases Big Data to Machine Learning Big Data platforms, Hadoop & Beyond (2h) Hadoop, HDFS and MapReduce, Datalakes, Data Pipelines From HPC to Big Data to Cloud and High Performance Data Analytics BI vs Big Data Hadoop legacy: Spark, Dask, Object Storage ... Spark (3.5h) \ud83d\udd17 Spark Introduction (1h) Play with MapReduce through Spark (Notebook on small datasets) (2.5h) Kubernetes & Dask (3.5h) \ud83d\udd17 Containers Orchestration (1h) Kubernetes & CaaS & PaaS (Databricks, Coiled) Play with Kubernetes (if we have time) Dask Presentation (1h) Deploy a Data processing platform on the Cloud based on Kubernetes and Dask (1.5h) Exercise: DaskHub or Dask Kubernetes or Pangeo Evaluation (7h) \ud83d\udd17 Prerequisite: Pangeo platform deployed before Clean big amounts of data using Dask in the cloud (3h) Train machine learning models in parallel (hyper parameter search) (3h) Notebook with cell codes to fill or answers to give Evaluation introduction slides","title":"Introduction"},{"location":"2_1_overview.html#introduction-to-data-distribution","text":"Course Introduction","title":"Introduction to Data Distribution"},{"location":"2_1_overview.html#course-overview","text":"Data Distribution & Big Data Processing Harnessing the complexity of large amounts of data is a challenge in itself. But Big Data processing is more than that: originally characterized by the 3 Vs of Volume, Velocity and Variety, the concepts popularized by Hadoop and Google requires dedicated computing solutions (both software and infrastructure), which will be explored in this module.","title":"Course Overview"},{"location":"2_1_overview.html#objectives","text":"By the end of this module, participants will be able to: Understand the differences and usage between main distributed computing architectures (HPC, Big Data, Cloud, CPU vs GPGPU) Implement the distribution of simple operations via the Map/Reduce principle in PySpark Understand the principle of Kubernetes Deploy a Big Data Processing Platform on the Cloud Implement the distribution of data wrangling/cleaning and training machine learning algorithms using PyData stack, Jupyter notebooks and Dask","title":"Objectives"},{"location":"2_1_overview.html#quizz","text":"I'll try to propose some quizz to be sure you're following!","title":"Quizz"},{"location":"2_1_overview.html#program","text":"","title":"Program"},{"location":"2_1_overview.html#big-data-distributed-computing-3h","text":"Introduction to Big Data and its ecosystem (1h) What is Big Data? Legacy \u201cBig Data\u201d ecosystem Big Data use cases Big Data to Machine Learning Big Data platforms, Hadoop & Beyond (2h) Hadoop, HDFS and MapReduce, Datalakes, Data Pipelines From HPC to Big Data to Cloud and High Performance Data Analytics BI vs Big Data Hadoop legacy: Spark, Dask, Object Storage ...","title":"Big Data &amp; Distributed Computing (3h)"},{"location":"2_1_overview.html#spark-35h","text":"Spark Introduction (1h) Play with MapReduce through Spark (Notebook on small datasets) (2.5h)","title":"Spark (3.5h)"},{"location":"2_1_overview.html#kubernetes-dask-35h","text":"Containers Orchestration (1h) Kubernetes & CaaS & PaaS (Databricks, Coiled) Play with Kubernetes (if we have time) Dask Presentation (1h) Deploy a Data processing platform on the Cloud based on Kubernetes and Dask (1.5h) Exercise: DaskHub or Dask Kubernetes or Pangeo","title":"Kubernetes &amp; Dask (3.5h)"},{"location":"2_1_overview.html#evaluation-7h","text":"Prerequisite: Pangeo platform deployed before Clean big amounts of data using Dask in the cloud (3h) Train machine learning models in parallel (hyper parameter search) (3h) Notebook with cell codes to fill or answers to give Evaluation introduction slides","title":"Evaluation (7h)"},{"location":"2_2_functional.html","text":"Functional Programming \ud83d\udd17 This section of the course is not given this year . Functional Programming for Distributed Data \ud83d\udd17 Link to slides Introduction to Julia \ud83d\udd17 As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation: Julia download Julia kernel for Jupyter Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance. Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook. Github Colab Functional Programming in Julia \ud83d\udd17 Julia documentation explaining: Functions , showing that they are first-class the map function which is a higher-order function distributed computing allowing for transfer of functions between threads or workers Distributed Data in Julia \ud83d\udd17 Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: DistributedArrays - A general Array type which can be distributed over multiple workers. JuliaDB - A data structuring package which automatically handles distributed data storage and computation Spark.jl - A Julia interface to Apache Spark. Related blog post . Map Reduce Exercise \ud83d\udd17 The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example. MapReduce notebook MapReduce notebook on Colab (requires adding Julia kernel installation)","title":"Functional Programming"},{"location":"2_2_functional.html#functional-programming","text":"This section of the course is not given this year .","title":"Functional Programming"},{"location":"2_2_functional.html#functional-programming-for-distributed-data","text":"Link to slides","title":"Functional Programming for Distributed Data"},{"location":"2_2_functional.html#introduction-to-julia","text":"As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation: Julia download Julia kernel for Jupyter Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance. Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook. Github Colab","title":"Introduction to Julia"},{"location":"2_2_functional.html#functional-programming-in-julia","text":"Julia documentation explaining: Functions , showing that they are first-class the map function which is a higher-order function distributed computing allowing for transfer of functions between threads or workers","title":"Functional Programming in Julia"},{"location":"2_2_functional.html#distributed-data-in-julia","text":"Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: DistributedArrays - A general Array type which can be distributed over multiple workers. JuliaDB - A data structuring package which automatically handles distributed data storage and computation Spark.jl - A Julia interface to Apache Spark. Related blog post .","title":"Distributed Data in Julia"},{"location":"2_2_functional.html#map-reduce-exercise","text":"The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example. MapReduce notebook MapReduce notebook on Colab (requires adding Julia kernel installation)","title":"Map Reduce Exercise"},{"location":"2_3_mapreduce.html","text":"Hadoop and MapReduce \ud83d\udd17 In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example. Introduction to Big Data and its ecosystem (1h) What is Big Data? Legacy \u201cBig Data\u201d ecosystem Big Data use cases Big Data to Machine Learning Big Data platforms, Hadoop & Beyond (2h) Hadoop, HDFS and MapReduce, Datalakes, Data Pipelines From HPC to Big Data to Cloud and High Performance Data Analytics BI vs Big Data Hadoop legacy: Spark, Dask, Object Storage ... It contains also a short interactive exercise using Python Map Reduce.","title":"Hadoop and MapReduce"},{"location":"2_3_mapreduce.html#hadoop-and-mapreduce","text":"In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example. Introduction to Big Data and its ecosystem (1h) What is Big Data? Legacy \u201cBig Data\u201d ecosystem Big Data use cases Big Data to Machine Learning Big Data platforms, Hadoop & Beyond (2h) Hadoop, HDFS and MapReduce, Datalakes, Data Pipelines From HPC to Big Data to Cloud and High Performance Data Analytics BI vs Big Data Hadoop legacy: Spark, Dask, Object Storage ... It contains also a short interactive exercise using Python Map Reduce.","title":"Hadoop and MapReduce"},{"location":"2_4_spark.html","text":"Spark \ud83d\udd17 In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. We use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means. Spark introduction Spark notebook Spark notebook on Colab Spark notebook on mybinder","title":"Spark"},{"location":"2_4_spark.html#spark","text":"In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. We use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means. Spark introduction Spark notebook Spark notebook on Colab Spark notebook on mybinder","title":"Spark"},{"location":"2_5_dask.html","text":"Dask on Kubernetes \ud83d\udd17 In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project . Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn. Orchestration and Kubernetes Kubernetes Dask processing framework Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits. Deploying a Dask Hub slides Additional resources can be found in the dask documentation . This class builds on the orchestration class, going into further detail on K8S specifics.","title":"Dask on Kubernetes"},{"location":"2_5_dask.html#dask-on-kubernetes","text":"In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project . Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn. Orchestration and Kubernetes Kubernetes Dask processing framework Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits. Deploying a Dask Hub slides Additional resources can be found in the dask documentation . This class builds on the orchestration class, going into further detail on K8S specifics.","title":"Dask on Kubernetes"},{"location":"2_6_project.html","text":"Project - Dask \ud83d\udd17 The evaluation for this class is a Dask notebook . You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class with instructions in this notebook . You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS . You may work with a partner, in this case make sure to specify in your notebook that you worked together. The notebook is due on February 18, 2021, by midnight. Finish deploying a Dask Hub Dask tutorial, if needed Evaluation notebook Just git clone the OBD project in your running Jupyterlab to begin!","title":"Dask project"},{"location":"2_6_project.html#project-dask","text":"The evaluation for this class is a Dask notebook . You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class with instructions in this notebook . You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS . You may work with a partner, in this case make sure to specify in your notebook that you worked together. The notebook is due on February 18, 2021, by midnight. Finish deploying a Dask Hub Dask tutorial, if needed Evaluation notebook Just git clone the OBD project in your running Jupyterlab to begin!","title":"Project - Dask"}]}